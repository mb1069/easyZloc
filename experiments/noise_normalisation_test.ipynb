{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f2a973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyotf.otf import HanserPSF, apply_aberration, apply_named_aberration\n",
    "import matplotlib.pyplot as plt\n",
    "from data.visualise import grid_psfs\n",
    "import numpy as np\n",
    "\n",
    "kwargs = dict(\n",
    "    wl=647,\n",
    "    na=1.3, \n",
    "    ni=1.51,\n",
    "    res=106,\n",
    "    zres=10,\n",
    "    size=32,\n",
    "    zsize=200,\n",
    "    vec_corr=\"none\",\n",
    "    condition=\"none\",\n",
    ")\n",
    "psf = HanserPSF(**kwargs)\n",
    "psf = apply_aberration(psf, np.array([0, 0, 0, 0, 0]), np.array([0, 0, 0, 0, 1]))\n",
    "\n",
    "blank_psf = psf.PSFi\n",
    "plt.imshow(grid_psfs(blank_psf).T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e93ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.visualise import show_psf_axial\n",
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "show_psf_axial(blank_psf, '', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272119d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_psf *= (2**16)\n",
    "assert blank_psf.max() < np.iinfo(np.uint16).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6db369e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.noise.noise_psf import generate_noisy_psf, EMCCD\n",
    "\n",
    "def noise_psf(psf, nsr, baseline):\n",
    "    background_noise = psf.max() * nsr\n",
    "    if background_noise<0:\n",
    "        background_noise = 0\n",
    "    config = {\n",
    "        'noise_background': background_noise,\n",
    "        'quantum_efficiency': np.random.normal(0.9, 0.1),\n",
    "        'read_noise': np.random.normal(74.4, 1),\n",
    "        'spurious_charge': np.random.normal(0.0002, 1e-5),\n",
    "        'em_gain': np.random.normal(300.0, 5), \n",
    "        'baseline': np.random.normal(baseline, 5), \n",
    "        'e_per_adu': 45.0\n",
    "    }\n",
    "    noise_config = EMCCD(**config)\n",
    "    noisy_psf = noise_config.add_noise(psf)\n",
    "    noisy_psf[noisy_psf<0] = 0\n",
    "    return noisy_psf\n",
    "\n",
    "n_test_datasets = 100\n",
    "test_dataset = np.concatenate([noise_psf(blank_psf, np.random.normal(0.4, 0.3), 250)/2 for _ in range(n_test_datasets)])\n",
    "coords = np.concatenate([np.zeros((200, 2)) for _ in range(n_test_datasets)])\n",
    "zs = np.concatenate([np.arange(-1000, 1000, 10) for _ in range(n_test_datasets)])\n",
    "\n",
    "\n",
    "n_train_datasets = 1000\n",
    "train_dataset = np.concatenate([noise_psf(blank_psf, np.random.normal(0.4, 0.5), np.random.normal(100, 5)) for _ in range(n_train_datasets)])\n",
    "train_coords = np.concatenate([np.zeros((200, 2)) for _ in range(n_train_datasets)])\n",
    "train_zs = np.concatenate([np.arange(-1000, 1000, 10) for _ in range(n_train_datasets)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc08da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = {}\n",
    "for i in [-500, 0, 500]:\n",
    "    idx = np.argwhere(np.abs(zs-i) < 100).squeeze()\n",
    "    sub_test_dataset = test_dataset[idx]\n",
    "    sub_test_coords = coords[idx]\n",
    "    sub_test_zs = zs[idx]\n",
    "    dataset[f'test_{i}nm'] = [[sub_test_dataset, sub_test_coords], sub_test_zs]\n",
    "\n",
    "psf_train, psf_other, coords_train, coords_other, zs_train, zs_other = train_test_split(train_dataset, train_coords, train_zs, train_size=0.8, random_state=42)\n",
    "\n",
    "dataset['test'] = [[test_dataset, coords], zs]\n",
    "dataset['train'] = [[psf_train, coords_train], zs_train]\n",
    "dataset['val'] = [[psf_other, coords_other], zs_other]\n",
    "\n",
    "for i in [-500, 0, 500]:\n",
    "    idx = np.argwhere(np.abs(zs_train-i) < 100).squeeze()\n",
    "    train_reduced_psf = psf_train[idx]\n",
    "    train_reduced_coords = coords_train[idx]\n",
    "    train_reduced_zs = zs_train[idx]\n",
    "    dataset[f'train_{i}nm'] = [[train_reduced_psf, train_reduced_coords], train_reduced_zs] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cca36f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k, v in dataset.items():\n",
    "    print(k)\n",
    "    print('     Imgs')\n",
    "    print('\\t', v[0][0].shape)\n",
    "    print('\\t', v[0][0].min(), v[0][0].max())\n",
    "    print('     Coords')\n",
    "    print('\\t', v[0][1].shape)\n",
    "    print('     Zs')\n",
    "    print('\\t', v[1].shape)\n",
    "    print('\\t', v[1].min(), v[1].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f386ebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in dataset.items():\n",
    "    print(k)\n",
    "    imgs = dataset[k][0][0][0:10]\n",
    "    plt.rcParams['figure.figsize'] = [15, 5]\n",
    "    plt.imshow(grid_psfs(imgs))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d02d51",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def plot_hists(dataset, query):\n",
    "    for k, v in dataset.items():\n",
    "        if re.match(query, k):\n",
    "            plt.hist(v[0][0].flatten(), label=k, bins=50, histtype=u'step', density=True)\n",
    "    plt.legend()\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.rcParams['figure.figsize'] = [15, 5]\n",
    "    plt.xlabel('Pixel value')\n",
    "    plt.show()\n",
    "\n",
    "def plot_all_hists():\n",
    "    plot_hists(dataset, r'^[^_]+$')\n",
    "    plot_hists(dataset, r'.+_-500nm')\n",
    "    plot_hists(dataset, r'.+_500nm')\n",
    "    plot_hists(dataset, r'.+_0nm')\n",
    "\n",
    "plot_all_hists()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2dc77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_max_norm\n",
    "def norm_zero_one(img):\n",
    "    img_max = img.max()\n",
    "    img_min = img.min()\n",
    "    return (img - img_min) / (img_max - img_min)\n",
    "\n",
    "def norm_one_one(img):\n",
    "    return (2 * norm_zero_one(img)) - 1\n",
    "\n",
    "for k in dataset.keys():\n",
    "    dataset[k][0][0] = np.stack([norm_zero_one(img) for img in dataset[k][0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e68217",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_hists()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0501c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "for k in dataset.keys():\n",
    "    dataset[k][0][0] = dataset[k][0][0][:, :, :, np.newaxis]\n",
    "    dataset[k][0][0] = np.stack([tf.image.grayscale_to_rgb(tf.convert_to_tensor(img)).numpy() for img in dataset[k][0][0]])\n",
    "    print(k, dataset[k][0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "876c78ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-cd53e421645c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrayscale_to_rgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from skimage.transform import resize\n",
    "import tensorflow as tf\n",
    "for k in train_dataset.data.keys():\n",
    "    print(k, train_dataset.data[k][0][0].shape)\n",
    "    train_dataset.data[k][0][0] = np.stack([tf.image.grayscale_to_rgb(tf.convert_to_tensor(img)).numpy() for img in train_dataset.data[k][0][0]])\n",
    "    train_dataset.data[k][0][0] = np.stack([resize(img, (32, 32, 1), anti_aliasing=True) for img in train_dataset.data[k][0][0]])\n",
    "    print(k, train_dataset.data[k][0][0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb1c058",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "dataset2 = dataset.copy()\n",
    "from model.model import load_new_model, train_model, get_resnet_101\n",
    "\n",
    "model = get_resnet_101(32, 0.01)\n",
    "train_model(model, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb1d8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no norm\n",
    "# Mean error 334.7929205786267\n",
    "# std error 311.9182186726581\n",
    "\n",
    "# sample-wise [0,1]\n",
    "# Mean error 18.256286726335162\n",
    "# std error 13.497500530378288\n",
    "\n",
    "# sample wise [-1, 1]\n",
    "# Mean error 20.571977236271884\n",
    "# std error 15.336221691580258"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5bf2cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "test_x, test_y = dataset['test']\n",
    "\n",
    "pred_y = model.predict(test_x).squeeze()\n",
    "error = abs(test_y-pred_y)\n",
    "\n",
    "print(f'Mean error {np.mean(error)}')\n",
    "print(f'std error {np.std(error)}')\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "sns.histplot(error)\n",
    "plt.xlabel('Error (nm)')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "idx = np.argsort(error)[::-1][0:1000]\n",
    "test_imgs = test_x[0].squeeze()[idx]\n",
    "plt.rcParams['figure.figsize'] = [30, 50]\n",
    "plt.imshow(grid_psfs(test_imgs.mean(axis=-1)))\n",
    "plt.show()\n",
    "\n",
    "def snr(img):\n",
    "    return img.max() / np.median(img)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [3, 5]\n",
    "plt.scatter(test_y, pred_y)\n",
    "plt.show()\n",
    "error = abs(pred_y-test_y)\n",
    "plt.boxplot(error)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac47b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_z = model.predict(dataset['train'][0]).squeeze()\n",
    "plt.plot(dataset['train'][1], pred_z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5344a3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_zs = dataset['train'][1]\n",
    "idx = np.argwhere(train_zs==0).squeeze()\n",
    "img = dataset['train'][0][0][idx][0:10]\n",
    "print(img.shape)\n",
    "# model.predict((img, np.zeros((10, 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d5c2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_z = model.predict((dataset['val'][0][0], dataset['val'][0][1])).squeeze()\n",
    "error = abs(dataset['val'][1]-pred_z)\n",
    "\n",
    "best_img_idx = np.argmin(error)\n",
    "img = dataset['val'][0][0][best_img_idx:best_img_idx+1]\n",
    "coord = dataset['val'][0][1][best_img_idx:best_img_idx+1]\n",
    "z = dataset['val'][1][best_img_idx:best_img_idx+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f242e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_circular_mask(h, w, center=None, radius=None):\n",
    "\n",
    "    if center is None: # use the middle of the image\n",
    "        center = (int(w/2), int(h/2))\n",
    "    if radius is None: # use the smallest distance between the center and image walls\n",
    "        radius = min(center[0], center[1], w-center[0], h-center[1])\n",
    "\n",
    "    Y, X = np.ogrid[:h, :w]\n",
    "    dist_from_center = np.sqrt((X - center[0])**2 + (Y-center[1])**2).astype(int)\n",
    "\n",
    "    mask = dist_from_center <= radius\n",
    "    mask = mask[:, :, np.newaxis]\n",
    "    mask = np.repeat(mask, 3, axis=-1)\n",
    "    return mask\n",
    "mask = create_circular_mask(32, 32, radius=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d8293c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img = img[0:1]\n",
    "for _ in range(10):\n",
    "    nsr = 0.01\n",
    "    noise = np.random.normal(loc=nsr, scale=0.01, size=img.shape[1:])\n",
    "    print(noise.shape)\n",
    "    print(mask.shape)\n",
    "#     noise[~mask] = 0\n",
    "    psf = img + noise\n",
    "    psf = norm_zero_one(psf)\n",
    "    print(psf.min(), psf.max())\n",
    "    print(model.predict((psf, np.zeros((1, 2)))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab64c31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
