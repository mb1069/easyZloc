{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caa75045",
   "metadata": {},
   "source": [
    "# 1. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3c826d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug enabled in datasets.py\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Loaded spots...\n",
      "Converted coords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/546 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligning (547, 81, 31, 31) psfs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [03:38<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coords 0.001033217853055148 1.0436054516632136\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b8c350f950d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mz_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'paired_bead_stacks'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainingPicassoDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_configs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/uni/phd/smlm_z/data/datasets.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, z_range, raw_data_only, lazy)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0mpsfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m             \u001b[0;31m# psfs, coords, zs = self.trim_stack(psfs, coords, zs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mpsfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/uni/phd/smlm_z/data/datasets.py\u001b[0m in \u001b[0;36mprepare_training_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Coords'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Prepared stacks...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from data.datasets import TrainingPicassoDataset\n",
    "from config.datasets import dataset_configs\n",
    "\n",
    "z_range = 1000\n",
    "dataset = 'paired_bead_stacks'\n",
    "train_dataset = TrainingPicassoDataset(dataset_configs[dataset]['training'], z_range)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55374256",
   "metadata": {},
   "source": [
    "3. Estimate 'ground truth' 0 position (focus point) of each bead using peak brightness. This is inaccurate due to high SNR ratio in image, so refine these positions by assuming dots of light sit on a plane, model new Z positions as least squares of plane and exclude poorly located points\n",
    "4. Normalise each image to range [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8179860b",
   "metadata": {},
   "source": [
    "# 2. Training the model\n",
    "\n",
    "Currently, I'm using 2 'calibration' datasets to see if my model can adapt from one to the other.\n",
    "All of the training dataset is used for training, and the 2nd dataset is used as validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc25ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [3, 5]\n",
    "def gather_stats(dataset, name, dataset_type='training'):\n",
    "    records = [] \n",
    "    \n",
    "    if dataset_type == 'training':\n",
    "        (imgs, coords), z = dataset\n",
    "    else:\n",
    "        imgs, coords = dataset\n",
    "        z = np.zeros((imgs.shape[0]))\n",
    "    \n",
    "    for img, coord, z in zip(imgs, coords, z):\n",
    "        results = {\n",
    "            'name': name,\n",
    "            'peak_brightness': img.max(),\n",
    "            'min_brightness': img.min(),\n",
    "            'mean_brightness': img.mean(),\n",
    "            'median_brightness': np.median(img),\n",
    "            'rho': coord[0],\n",
    "            'theta': coord[1],\n",
    "            'z': z,\n",
    "        }\n",
    "        records.append(results)\n",
    "        \n",
    "    return pd.DataFrame.from_records(records)\n",
    "\n",
    "if 'all' not in train_dataset.data:\n",
    "    train = train_dataset.data['train']\n",
    "    val = train_dataset.data['val']\n",
    "    test = train_dataset.data['test']\n",
    "\n",
    "    \n",
    "    records = []\n",
    "    records.append(gather_stats(train, 'train'))\n",
    "    records.append(gather_stats(val, 'val'))\n",
    "    records.append(gather_stats(test, 'test'))\n",
    "\n",
    "    \n",
    "    df = pd.concat(records)\n",
    "    for col in list(df):\n",
    "        if col != 'name':\n",
    "            df.boxplot(by='name', column=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9206b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect training dataset\n",
    "from data.visualise import grid_psfs\n",
    "import numpy as np\n",
    "plt.rcParams['figure.figsize'] = [50, 50]\n",
    "\n",
    "samples = min(1000, train_dataset.data['train'][1].shape[0])\n",
    "sub_idx = np.random.choice(np.arange(0, train_dataset.data['train'][1].shape[0]), samples, replace=False)\n",
    "print(sub_idx.shape)\n",
    "sub_zs = train_dataset.data['train'][1][sub_idx]\n",
    "sub_imgs = train_dataset.data['train'][0][0][sub_idx]\n",
    "idx = np.argsort(sub_zs)\n",
    "sub_imgs = sub_imgs[idx]\n",
    "\n",
    "plt.imshow(grid_psfs(sub_imgs.squeeze()))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f451dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_NEW_MODEL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d46522",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from model.model import load_new_model\n",
    "\n",
    "from wandb.keras import WandbCallback\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "import wandb\n",
    "import os\n",
    "import logging\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "bound = train_dataset.data['train'][0][0].shape[1]\n",
    "\n",
    "BATCH_SIZE = 2**10\n",
    "LEARNING_RATE = 1e-3\n",
    "wandb.init(project=\"smlm_z\",\n",
    "           name=dataset,\n",
    "           config={\n",
    "               \"batch_size\": BATCH_SIZE,\n",
    "               \"learning_rate\": LEARNING_RATE,\n",
    "               \"dataset\": dataset,\n",
    "               \"bound\": bound,\n",
    "           })\n",
    "\n",
    "save_path = os.path.join(os.getcwd(), 'wandb')\n",
    "import glob\n",
    "wandb.save(f'{save_path}/*.py', base_path=save_path)\n",
    "\n",
    "def train_model(dataset, val_dataset=None, pretrained_model=None):\n",
    "    if not val_dataset:\n",
    "        val_dataset = dataset['val']\n",
    "#     for k in dataset:\n",
    "#         imgs = dataset[k][0][0]\n",
    "#         norm_imgs = imgs / imgs.max(axis=(1, 2))[:, np.newaxis, np.newaxis]\n",
    "#         dataset[k][0][0] = norm_imgs\n",
    "\n",
    "    model = pretrained_model or load_new_model(bound, LEARNING_RATE)\n",
    "\n",
    "    callbacks = [\n",
    "        ReduceLROnPlateau(\n",
    "        monitor='loss', factor=0.1, patience=50, verbose=True,\n",
    "        mode='min', min_delta=1, cooldown=50, min_lr=1e-7,),\n",
    "#         ReduceLROnPlateau(\n",
    "#         monitor='val_mean_absolute_error', factor=0.1, patience=50, verbose=True,\n",
    "#         mode='min', min_delta=1, cooldown=50, min_lr=1e-10,),\n",
    "        EarlyStopping(monitor='val_mean_absolute_error', patience=500, verbose=False, min_delta=1, restore_best_weights=True),\n",
    "#         WandbCallback(save_model=False),\n",
    "        TqdmCallback(verbose=1),\n",
    "    ]\n",
    "\n",
    "    history = model.fit(*dataset['train'], epochs=5000, verbose=False, batch_size=BATCH_SIZE, validation_data=(*val_dataset,), callbacks=callbacks)\n",
    "    \n",
    "    plt.rcParams['figure.figsize'] = [10, 5]\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(history.history['mean_absolute_error'], label='mse')\n",
    "    ax1.plot(history.history['val_mean_absolute_error'], label='val_mse')\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.legend(loc=1)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(history.history['lr'], label='lr')\n",
    "    ax2.legend(loc=0)\n",
    "\n",
    "    plt.show()\n",
    "    return model\n",
    "\n",
    "if TRAIN_NEW_MODEL == True:\n",
    "    model = train_model(train_dataset.data)\n",
    "#     save_model(model)\n",
    "else:\n",
    "    model = load_model(load_regression_model(LEARNING_RATE))\n",
    "\n",
    "tmp_x = train_dataset.data['train'][0][0][0:2], train_dataset.data['train'][0][1][0:2]\n",
    "model(tmp_x)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881c9887",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "test_x, test_y = train_dataset.data['test']\n",
    "\n",
    "# from scipy.ndimage import median_filter\n",
    "# test_x[0] = np.stack([median_filter(d, size=2) for d in test_x[0].copy()])\n",
    "\n",
    "# from final_project.smlm_3d.data.datasets import mask_img_stack\n",
    "# test_x[0] = mask_img_stack(test_x[0].copy(), radius=12)\n",
    "\n",
    "\n",
    "pred_y = model.predict(test_x).squeeze()\n",
    "error = abs(test_y-pred_y)\n",
    "\n",
    "print(f'Mean error {np.mean(error)}')\n",
    "print(f'std error {np.std(error)}')\n",
    "\n",
    "sns.histplot(error)\n",
    "plt.xlabel('Error (nm)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 20220425_Miguel/training_20nm\n",
    "# Cubic spline model\n",
    "# Mean error 34.24343542454768\n",
    "# std error 36.64187932722746\n",
    "\n",
    "# Spline super-sampling model w/o plane fit\n",
    "# Mean error 28.941918452166664\n",
    "# std error 26.241176376179656\n",
    "\n",
    "\n",
    "# Spline fit super-sampling model w/ improved image normalisation\n",
    "# Mean error 28.089797139687477\n",
    "# std error 24.976300570311047\n",
    "\n",
    "# Latest results - spline fit super-sampling w/ improved image normalisation + plane-fit\n",
    "# Mean error 27.546041584423875\n",
    "# std error 26.033701351832462\n",
    "\n",
    "\n",
    "\n",
    "# latest\n",
    "# Mean error 72.83350251990716\n",
    "# std error 130.39968131300466\n",
    "\n",
    "\n",
    "idx = np.argsort(error)[::-1][0:1000]\n",
    "test_imgs = test_x[0].squeeze()[idx]\n",
    "plt.rcParams['figure.figsize'] = [30, 50]\n",
    "plt.imshow(grid_psfs(test_imgs))\n",
    "plt.show()\n",
    "\n",
    "def snr(img):\n",
    "    return img.max() / np.median(img)\n",
    "plt.rcParams['figure.figsize'] = [3, 5]\n",
    "plt.scatter(test_y, pred_y)\n",
    "plt.show()\n",
    "error = abs(pred_y-test_y)\n",
    "plt.boxplot(error)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter([snr(img) for img in test_imgs], error, marker='.', alpha=0.2)\n",
    "plt.xlabel('snr')\n",
    "plt.ylabel('error [nm]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d85ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
