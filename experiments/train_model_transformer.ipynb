{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caa75045",
   "metadata": {},
   "source": [
    "# 1. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3c826d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded spots...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/93 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligning (94, 41, 31, 31) psfs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [00:18<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared stacks...\n",
      "masking\n",
      "Adding noise... 30\n",
      "0.0 2317.5955918324116\n",
      "195 2110\n",
      "195 1648\n",
      "Standardising using\n",
      " \tmean: 0.003320395030787893\n",
      " \tstd 0.0007166993002643155\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from data.datasets import TrainingPicassoDataset\n",
    "from config.datasets import dataset_configs\n",
    "\n",
    "z_range = 1000\n",
    "dataset = 'picasso_test'\n",
    "train_dataset = TrainingPicassoDataset(dataset_configs[dataset]['training'], z_range)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf7e9507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77345, 64, 64, 1)\n",
      "(713, 64, 64, 1)\n",
      "(357, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "\n",
    "for k in train_dataset.data.keys():\n",
    "#     train_dataset.data[k][0][0] = np.repeat(train_dataset.data[k][0][0], 3, axis=-1)\n",
    "    train_dataset.data[k][0][0] = np.stack([resize(img, (64, 64, 1), anti_aliasing=True) for img in train_dataset.data[k][0][0]])\n",
    "    print(train_dataset.data[k][0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "732b8200",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train_dataset.data['train'][0][0], train_dataset.data['train'][1]\n",
    "x_test, y_test = train_dataset.data['test'][0][0], train_dataset.data['test'][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80d0c98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x_train = tf.convert_to_tensor(x_train)\n",
    "y_train = tf.convert_to_tensor(y_train)\n",
    "x_test = tf.convert_to_tensor(x_test)\n",
    "y_test = tf.convert_to_tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f0e0176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f4dbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 64 X 64\n",
      "Patch size: 6 X 6\n",
      "Patches per image: 100\n",
      "Elements per patch: 36\n",
      "Epoch 1/5000\n",
      "272/272 - 25s - loss: 147354.9844 - RMSE: 383.8685 - val_loss: 148320.1719 - val_RMSE: 385.1236 - lr: 0.0100 - 25s/epoch - 94ms/step\n",
      "Epoch 2/5000\n",
      "272/272 - 19s - loss: 132656.5156 - RMSE: 364.2204 - val_loss: 98483.2344 - val_RMSE: 313.8204 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 3/5000\n",
      "272/272 - 19s - loss: 129624.8359 - RMSE: 360.0345 - val_loss: 95570.6484 - val_RMSE: 309.1450 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 4/5000\n",
      "272/272 - 19s - loss: 130118.5078 - RMSE: 360.7194 - val_loss: 120856.3516 - val_RMSE: 347.6440 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 5/5000\n",
      "272/272 - 19s - loss: 132404.1719 - RMSE: 363.8738 - val_loss: 85589.1797 - val_RMSE: 292.5563 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 6/5000\n",
      "272/272 - 19s - loss: 171906.3750 - RMSE: 414.6159 - val_loss: 111564.2109 - val_RMSE: 334.0123 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 7/5000\n",
      "272/272 - 19s - loss: 146594.7188 - RMSE: 382.8769 - val_loss: 102575.3594 - val_RMSE: 320.2739 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 8/5000\n",
      "272/272 - 19s - loss: 138877.1719 - RMSE: 372.6623 - val_loss: 99949.6172 - val_RMSE: 316.1481 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 9/5000\n",
      "272/272 - 19s - loss: 121574.8125 - RMSE: 348.6758 - val_loss: 72244.6641 - val_RMSE: 268.7837 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 10/5000\n",
      "272/272 - 19s - loss: 141942.8906 - RMSE: 376.7531 - val_loss: 95869.4766 - val_RMSE: 309.6280 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 11/5000\n",
      "272/272 - 19s - loss: 183132.2500 - RMSE: 427.9395 - val_loss: 246734.9844 - val_RMSE: 496.7242 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 12/5000\n",
      "272/272 - 19s - loss: 216097.7656 - RMSE: 464.8632 - val_loss: 98422.1562 - val_RMSE: 313.7231 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 13/5000\n",
      "272/272 - 19s - loss: 135541.4844 - RMSE: 368.1596 - val_loss: 91871.4141 - val_RMSE: 303.1030 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 14/5000\n",
      "272/272 - 19s - loss: 131668.2500 - RMSE: 362.8612 - val_loss: 92541.5547 - val_RMSE: 304.2064 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 15/5000\n",
      "272/272 - 19s - loss: 129389.0234 - RMSE: 359.7068 - val_loss: 96444.1016 - val_RMSE: 310.5545 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 16/5000\n",
      "272/272 - 19s - loss: 139430.0938 - RMSE: 373.4034 - val_loss: 90774.5234 - val_RMSE: 301.2881 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 17/5000\n",
      "272/272 - 19s - loss: 151704.5156 - RMSE: 389.4926 - val_loss: 133947.3750 - val_RMSE: 365.9882 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 18/5000\n",
      "272/272 - 19s - loss: 147639.6250 - RMSE: 384.2390 - val_loss: 96346.4688 - val_RMSE: 310.3973 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 19/5000\n",
      "272/272 - 19s - loss: 151951.8906 - RMSE: 389.8101 - val_loss: 90254.8438 - val_RMSE: 300.4244 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 20/5000\n",
      "272/272 - 19s - loss: 154990.1250 - RMSE: 393.6879 - val_loss: 116166.5000 - val_RMSE: 340.8321 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 21/5000\n",
      "272/272 - 19s - loss: 161417.6875 - RMSE: 401.7682 - val_loss: 128268.0078 - val_RMSE: 358.1452 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 22/5000\n",
      "272/272 - 19s - loss: 148525.2812 - RMSE: 385.3898 - val_loss: 90014.4062 - val_RMSE: 300.0240 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 23/5000\n",
      "272/272 - 19s - loss: 126628.1875 - RMSE: 355.8485 - val_loss: 92785.7031 - val_RMSE: 304.6075 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 24/5000\n",
      "272/272 - 19s - loss: 138408.7812 - RMSE: 372.0333 - val_loss: 99279.2500 - val_RMSE: 315.0861 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 25/5000\n",
      "272/272 - 19s - loss: 136976.3750 - RMSE: 370.1032 - val_loss: 98021.1172 - val_RMSE: 313.0833 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 26/5000\n",
      "272/272 - 19s - loss: 141093.3281 - RMSE: 375.6239 - val_loss: 87347.3750 - val_RMSE: 295.5459 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 27/5000\n",
      "272/272 - 19s - loss: 138692.7344 - RMSE: 372.4147 - val_loss: 105813.9453 - val_RMSE: 325.2906 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 28/5000\n",
      "272/272 - 19s - loss: 136465.6094 - RMSE: 369.4125 - val_loss: 106304.1094 - val_RMSE: 326.0431 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 29/5000\n",
      "272/272 - 19s - loss: 137893.4375 - RMSE: 371.3401 - val_loss: 102428.2734 - val_RMSE: 320.0442 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 30/5000\n",
      "272/272 - 19s - loss: 135658.4062 - RMSE: 368.3183 - val_loss: 99144.7188 - val_RMSE: 314.8725 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 31/5000\n",
      "272/272 - 19s - loss: 131627.0625 - RMSE: 362.8044 - val_loss: 97189.5469 - val_RMSE: 311.7524 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 32/5000\n",
      "272/272 - 19s - loss: 128195.5000 - RMSE: 358.0440 - val_loss: 96866.6406 - val_RMSE: 311.2341 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 33/5000\n",
      "272/272 - 19s - loss: 124322.8828 - RMSE: 352.5945 - val_loss: 93237.8984 - val_RMSE: 305.3488 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 34/5000\n",
      "\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "272/272 - 19s - loss: 121678.7812 - RMSE: 348.8249 - val_loss: 90227.2031 - val_RMSE: 300.3784 - lr: 0.0100 - 19s/epoch - 71ms/step\n",
      "Epoch 35/5000\n",
      "272/272 - 19s - loss: 110139.1641 - RMSE: 331.8722 - val_loss: 81533.6094 - val_RMSE: 285.5409 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 36/5000\n",
      "272/272 - 19s - loss: 107103.8906 - RMSE: 327.2673 - val_loss: 79765.0469 - val_RMSE: 282.4271 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 37/5000\n",
      "272/272 - 19s - loss: 103754.8359 - RMSE: 322.1100 - val_loss: 77390.4531 - val_RMSE: 278.1914 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 38/5000\n",
      "272/272 - 19s - loss: 101090.8203 - RMSE: 317.9478 - val_loss: 73831.7500 - val_RMSE: 271.7200 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 39/5000\n",
      "272/272 - 19s - loss: 98338.4297 - RMSE: 313.5896 - val_loss: 70948.8906 - val_RMSE: 266.3623 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 40/5000\n",
      "272/272 - 19s - loss: 94936.7031 - RMSE: 308.1180 - val_loss: 65459.9141 - val_RMSE: 255.8513 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 41/5000\n",
      "272/272 - 19s - loss: 92737.2891 - RMSE: 304.5280 - val_loss: 63070.4297 - val_RMSE: 251.1383 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 42/5000\n",
      "272/272 - 19s - loss: 89924.1953 - RMSE: 299.8736 - val_loss: 58700.8750 - val_RMSE: 242.2826 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 43/5000\n",
      "272/272 - 19s - loss: 86860.2422 - RMSE: 294.7206 - val_loss: 55682.6211 - val_RMSE: 235.9716 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 44/5000\n",
      "272/272 - 19s - loss: 84027.4688 - RMSE: 289.8749 - val_loss: 53881.8516 - val_RMSE: 232.1246 - lr: 1.0000e-03 - 19s/epoch - 72ms/step\n",
      "Epoch 45/5000\n",
      "272/272 - 19s - loss: 80453.5078 - RMSE: 283.6433 - val_loss: 48803.1797 - val_RMSE: 220.9144 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 46/5000\n",
      "272/272 - 19s - loss: 77064.3047 - RMSE: 277.6046 - val_loss: 45899.4414 - val_RMSE: 214.2415 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 47/5000\n",
      "272/272 - 19s - loss: 74875.3047 - RMSE: 273.6335 - val_loss: 43250.5508 - val_RMSE: 207.9677 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 48/5000\n",
      "272/272 - 19s - loss: 71048.3750 - RMSE: 266.5490 - val_loss: 43058.0625 - val_RMSE: 207.5044 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 49/5000\n",
      "272/272 - 19s - loss: 69215.7969 - RMSE: 263.0890 - val_loss: 39347.1289 - val_RMSE: 198.3611 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 50/5000\n",
      "272/272 - 19s - loss: 66195.7031 - RMSE: 257.2852 - val_loss: 37329.4570 - val_RMSE: 193.2083 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 51/5000\n",
      "272/272 - 19s - loss: 65463.1328 - RMSE: 255.8576 - val_loss: 36430.1719 - val_RMSE: 190.8669 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 52/5000\n",
      "272/272 - 19s - loss: 63620.3281 - RMSE: 252.2307 - val_loss: 36175.5898 - val_RMSE: 190.1988 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 53/5000\n",
      "272/272 - 19s - loss: 61425.6641 - RMSE: 247.8420 - val_loss: 34219.1562 - val_RMSE: 184.9842 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 54/5000\n",
      "272/272 - 19s - loss: 58748.5742 - RMSE: 242.3811 - val_loss: 31595.4492 - val_RMSE: 177.7511 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 55/5000\n",
      "272/272 - 19s - loss: 56855.5586 - RMSE: 238.4440 - val_loss: 32058.6328 - val_RMSE: 179.0492 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/5000\n",
      "272/272 - 19s - loss: 54509.5742 - RMSE: 233.4729 - val_loss: 30655.9336 - val_RMSE: 175.0884 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 57/5000\n",
      "272/272 - 19s - loss: 51876.8125 - RMSE: 227.7648 - val_loss: 31380.2441 - val_RMSE: 177.1447 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 58/5000\n",
      "272/272 - 19s - loss: 49013.8125 - RMSE: 221.3906 - val_loss: 29506.3145 - val_RMSE: 171.7740 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 59/5000\n",
      "272/272 - 19s - loss: 47988.8828 - RMSE: 219.0636 - val_loss: 32053.9941 - val_RMSE: 179.0363 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 60/5000\n",
      "272/272 - 19s - loss: 46026.2148 - RMSE: 214.5372 - val_loss: 28476.0645 - val_RMSE: 168.7485 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 61/5000\n",
      "272/272 - 19s - loss: 42535.9336 - RMSE: 206.2424 - val_loss: 28897.3105 - val_RMSE: 169.9921 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 62/5000\n",
      "272/272 - 19s - loss: 40554.2734 - RMSE: 201.3809 - val_loss: 30510.2383 - val_RMSE: 174.6718 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 63/5000\n",
      "272/272 - 19s - loss: 38975.8750 - RMSE: 197.4231 - val_loss: 27796.7246 - val_RMSE: 166.7235 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 64/5000\n",
      "272/272 - 19s - loss: 36878.1641 - RMSE: 192.0369 - val_loss: 27121.5332 - val_RMSE: 164.6862 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 65/5000\n",
      "272/272 - 19s - loss: 34437.7031 - RMSE: 185.5740 - val_loss: 27880.7305 - val_RMSE: 166.9752 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 66/5000\n",
      "272/272 - 19s - loss: 32748.3398 - RMSE: 180.9650 - val_loss: 29397.9277 - val_RMSE: 171.4582 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 67/5000\n",
      "272/272 - 19s - loss: 31373.3867 - RMSE: 177.1253 - val_loss: 29398.2422 - val_RMSE: 171.4592 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 68/5000\n",
      "272/272 - 19s - loss: 29036.0215 - RMSE: 170.3996 - val_loss: 26576.9473 - val_RMSE: 163.0244 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 69/5000\n",
      "272/272 - 19s - loss: 28018.8359 - RMSE: 167.3883 - val_loss: 26428.9238 - val_RMSE: 162.5697 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 70/5000\n",
      "272/272 - 19s - loss: 25993.8652 - RMSE: 161.2261 - val_loss: 26603.9941 - val_RMSE: 163.1073 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 71/5000\n",
      "272/272 - 19s - loss: 24909.5723 - RMSE: 157.8277 - val_loss: 26451.5625 - val_RMSE: 162.6394 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 72/5000\n",
      "272/272 - 19s - loss: 24250.1445 - RMSE: 155.7246 - val_loss: 29235.0098 - val_RMSE: 170.9825 - lr: 1.0000e-03 - 19s/epoch - 72ms/step\n",
      "Epoch 73/5000\n",
      "272/272 - 19s - loss: 22214.7793 - RMSE: 149.0462 - val_loss: 27889.5410 - val_RMSE: 167.0016 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 74/5000\n",
      "272/272 - 19s - loss: 21815.1543 - RMSE: 147.6995 - val_loss: 25566.4473 - val_RMSE: 159.8951 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 75/5000\n",
      "272/272 - 19s - loss: 20666.7031 - RMSE: 143.7592 - val_loss: 25551.0352 - val_RMSE: 159.8469 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 76/5000\n",
      "272/272 - 19s - loss: 19920.5332 - RMSE: 141.1401 - val_loss: 24795.8281 - val_RMSE: 157.4669 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 77/5000\n",
      "272/272 - 19s - loss: 19138.4160 - RMSE: 138.3417 - val_loss: 25475.2188 - val_RMSE: 159.6096 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 78/5000\n",
      "272/272 - 19s - loss: 18549.4727 - RMSE: 136.1964 - val_loss: 24815.9844 - val_RMSE: 157.5309 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 79/5000\n",
      "272/272 - 19s - loss: 17889.8418 - RMSE: 133.7529 - val_loss: 24403.8555 - val_RMSE: 156.2173 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 80/5000\n",
      "272/272 - 19s - loss: 17190.4980 - RMSE: 131.1125 - val_loss: 24446.5215 - val_RMSE: 156.3538 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 81/5000\n",
      "272/272 - 19s - loss: 16925.9395 - RMSE: 130.0997 - val_loss: 25294.8086 - val_RMSE: 159.0434 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 82/5000\n",
      "272/272 - 19s - loss: 16005.1484 - RMSE: 126.5115 - val_loss: 23896.7891 - val_RMSE: 154.5859 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 83/5000\n",
      "272/272 - 19s - loss: 15846.6553 - RMSE: 125.8835 - val_loss: 24333.5176 - val_RMSE: 155.9920 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 84/5000\n",
      "272/272 - 19s - loss: 15593.6855 - RMSE: 124.8747 - val_loss: 23941.4863 - val_RMSE: 154.7304 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 85/5000\n",
      "272/272 - 19s - loss: 14897.4033 - RMSE: 122.0549 - val_loss: 23873.6426 - val_RMSE: 154.5110 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 86/5000\n",
      "272/272 - 19s - loss: 14856.4551 - RMSE: 121.8871 - val_loss: 22466.3594 - val_RMSE: 149.8878 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 87/5000\n",
      "272/272 - 19s - loss: 14380.4111 - RMSE: 119.9184 - val_loss: 23173.6855 - val_RMSE: 152.2290 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 88/5000\n",
      "272/272 - 19s - loss: 14214.0576 - RMSE: 119.2227 - val_loss: 24417.1094 - val_RMSE: 156.2598 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 89/5000\n",
      "272/272 - 19s - loss: 13469.5703 - RMSE: 116.0585 - val_loss: 23307.0020 - val_RMSE: 152.6663 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 90/5000\n",
      "272/272 - 19s - loss: 13056.2930 - RMSE: 114.2641 - val_loss: 22779.4023 - val_RMSE: 150.9285 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 91/5000\n",
      "272/272 - 19s - loss: 12658.7900 - RMSE: 112.5113 - val_loss: 22279.1855 - val_RMSE: 149.2621 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 92/5000\n",
      "272/272 - 19s - loss: 12769.6553 - RMSE: 113.0029 - val_loss: 22608.3086 - val_RMSE: 150.3606 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 93/5000\n",
      "272/272 - 19s - loss: 12163.1562 - RMSE: 110.2867 - val_loss: 22356.7930 - val_RMSE: 149.5219 - lr: 1.0000e-03 - 19s/epoch - 72ms/step\n",
      "Epoch 94/5000\n",
      "272/272 - 19s - loss: 11827.7598 - RMSE: 108.7555 - val_loss: 22129.4746 - val_RMSE: 148.7598 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 95/5000\n",
      "272/272 - 19s - loss: 11667.9785 - RMSE: 108.0184 - val_loss: 22638.3320 - val_RMSE: 150.4604 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 96/5000\n",
      "272/272 - 19s - loss: 11514.6865 - RMSE: 107.3065 - val_loss: 22776.2539 - val_RMSE: 150.9180 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 97/5000\n",
      "272/272 - 19s - loss: 11084.7090 - RMSE: 105.2839 - val_loss: 22195.5293 - val_RMSE: 148.9816 - lr: 1.0000e-03 - 19s/epoch - 72ms/step\n",
      "Epoch 98/5000\n",
      "272/272 - 19s - loss: 10858.2822 - RMSE: 104.2031 - val_loss: 23028.0898 - val_RMSE: 151.7501 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 99/5000\n",
      "272/272 - 19s - loss: 10704.3584 - RMSE: 103.4619 - val_loss: 21549.9824 - val_RMSE: 146.7991 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 100/5000\n",
      "272/272 - 19s - loss: 10616.0557 - RMSE: 103.0342 - val_loss: 22084.7480 - val_RMSE: 148.6094 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 101/5000\n",
      "272/272 - 19s - loss: 10301.0479 - RMSE: 101.4941 - val_loss: 22232.6777 - val_RMSE: 149.1063 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 102/5000\n",
      "272/272 - 19s - loss: 10156.3574 - RMSE: 100.7788 - val_loss: 21237.8184 - val_RMSE: 145.7320 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 103/5000\n",
      "272/272 - 19s - loss: 10056.6621 - RMSE: 100.2829 - val_loss: 22277.2148 - val_RMSE: 149.2555 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 104/5000\n",
      "272/272 - 19s - loss: 9837.6230 - RMSE: 99.1848 - val_loss: 22209.9707 - val_RMSE: 149.0301 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 105/5000\n",
      "272/272 - 19s - loss: 9585.5166 - RMSE: 97.9057 - val_loss: 21557.5781 - val_RMSE: 146.8250 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 106/5000\n",
      "272/272 - 19s - loss: 9351.4414 - RMSE: 96.7029 - val_loss: 20684.3594 - val_RMSE: 143.8206 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 107/5000\n",
      "272/272 - 19s - loss: 9216.3076 - RMSE: 96.0016 - val_loss: 20698.9238 - val_RMSE: 143.8712 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 108/5000\n",
      "272/272 - 19s - loss: 9281.5068 - RMSE: 96.3406 - val_loss: 21464.2246 - val_RMSE: 146.5067 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 109/5000\n",
      "272/272 - 19s - loss: 8916.5049 - RMSE: 94.4272 - val_loss: 21162.7578 - val_RMSE: 145.4742 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 110/5000\n",
      "272/272 - 19s - loss: 9043.8477 - RMSE: 95.0991 - val_loss: 21202.9629 - val_RMSE: 145.6124 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 111/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 - 19s - loss: 8698.0693 - RMSE: 93.2634 - val_loss: 22012.1328 - val_RMSE: 148.3649 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 112/5000\n",
      "272/272 - 19s - loss: 8575.6182 - RMSE: 92.6046 - val_loss: 21176.7227 - val_RMSE: 145.5222 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 113/5000\n",
      "272/272 - 19s - loss: 8458.0254 - RMSE: 91.9675 - val_loss: 21242.3613 - val_RMSE: 145.7476 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 114/5000\n",
      "272/272 - 19s - loss: 8244.6953 - RMSE: 90.8003 - val_loss: 21685.8555 - val_RMSE: 147.2612 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 115/5000\n",
      "272/272 - 19s - loss: 8186.5654 - RMSE: 90.4796 - val_loss: 21153.3008 - val_RMSE: 145.4417 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 116/5000\n",
      "272/272 - 19s - loss: 8248.0586 - RMSE: 90.8188 - val_loss: 21184.4102 - val_RMSE: 145.5486 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 117/5000\n",
      "272/272 - 19s - loss: 8302.5547 - RMSE: 91.1184 - val_loss: 22323.7070 - val_RMSE: 149.4112 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 118/5000\n",
      "272/272 - 19s - loss: 8036.3164 - RMSE: 89.6455 - val_loss: 22140.0312 - val_RMSE: 148.7953 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 119/5000\n",
      "272/272 - 19s - loss: 7966.8408 - RMSE: 89.2572 - val_loss: 20516.3848 - val_RMSE: 143.2354 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 120/5000\n",
      "272/272 - 19s - loss: 7763.3994 - RMSE: 88.1102 - val_loss: 21044.7188 - val_RMSE: 145.0680 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 121/5000\n",
      "272/272 - 19s - loss: 7693.3516 - RMSE: 87.7118 - val_loss: 20703.8945 - val_RMSE: 143.8885 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 122/5000\n",
      "272/272 - 19s - loss: 7663.5801 - RMSE: 87.5419 - val_loss: 20724.4590 - val_RMSE: 143.9599 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 123/5000\n",
      "272/272 - 19s - loss: 7496.6997 - RMSE: 86.5835 - val_loss: 20968.5625 - val_RMSE: 144.8053 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 124/5000\n",
      "272/272 - 19s - loss: 7832.9307 - RMSE: 88.5038 - val_loss: 20494.8945 - val_RMSE: 143.1604 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 125/5000\n",
      "272/272 - 19s - loss: 7565.9863 - RMSE: 86.9827 - val_loss: 20621.5293 - val_RMSE: 143.6020 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 126/5000\n",
      "272/272 - 19s - loss: 7442.4971 - RMSE: 86.2699 - val_loss: 20352.8027 - val_RMSE: 142.6633 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 127/5000\n",
      "272/272 - 19s - loss: 7492.7847 - RMSE: 86.5609 - val_loss: 20607.1875 - val_RMSE: 143.5520 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 128/5000\n",
      "272/272 - 19s - loss: 7197.9839 - RMSE: 84.8409 - val_loss: 20445.6797 - val_RMSE: 142.9884 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 129/5000\n",
      "272/272 - 19s - loss: 7176.3218 - RMSE: 84.7132 - val_loss: 20481.5840 - val_RMSE: 143.1139 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 130/5000\n",
      "272/272 - 19s - loss: 6919.6689 - RMSE: 83.1845 - val_loss: 20577.2559 - val_RMSE: 143.4477 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 131/5000\n",
      "272/272 - 19s - loss: 7099.1846 - RMSE: 84.2567 - val_loss: 20314.9004 - val_RMSE: 142.5303 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 132/5000\n",
      "272/272 - 19s - loss: 7038.7554 - RMSE: 83.8973 - val_loss: 20678.1934 - val_RMSE: 143.7991 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 133/5000\n",
      "272/272 - 19s - loss: 7028.8101 - RMSE: 83.8380 - val_loss: 20302.8496 - val_RMSE: 142.4881 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 134/5000\n",
      "272/272 - 19s - loss: 6939.1558 - RMSE: 83.3016 - val_loss: 20147.1094 - val_RMSE: 141.9405 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 135/5000\n",
      "272/272 - 19s - loss: 6852.1992 - RMSE: 82.7780 - val_loss: 20438.7422 - val_RMSE: 142.9641 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 136/5000\n",
      "272/272 - 19s - loss: 6822.7124 - RMSE: 82.5997 - val_loss: 20234.2930 - val_RMSE: 142.2473 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 137/5000\n",
      "272/272 - 19s - loss: 6736.9907 - RMSE: 82.0792 - val_loss: 20135.4570 - val_RMSE: 141.8995 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 138/5000\n",
      "272/272 - 19s - loss: 6799.5835 - RMSE: 82.4596 - val_loss: 20592.4766 - val_RMSE: 143.5008 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 139/5000\n",
      "272/272 - 19s - loss: 6810.2363 - RMSE: 82.5242 - val_loss: 20607.9395 - val_RMSE: 143.5547 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 140/5000\n",
      "272/272 - 19s - loss: 6576.2700 - RMSE: 81.0942 - val_loss: 20120.6934 - val_RMSE: 141.8474 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 141/5000\n",
      "272/272 - 19s - loss: 6695.8794 - RMSE: 81.8284 - val_loss: 20337.6543 - val_RMSE: 142.6102 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 142/5000\n",
      "272/272 - 19s - loss: 6382.1406 - RMSE: 79.8883 - val_loss: 20583.3262 - val_RMSE: 143.4689 - lr: 1.0000e-03 - 19s/epoch - 72ms/step\n",
      "Epoch 143/5000\n",
      "272/272 - 19s - loss: 6468.0679 - RMSE: 80.4243 - val_loss: 19924.8105 - val_RMSE: 141.1553 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 144/5000\n",
      "272/272 - 19s - loss: 6375.8105 - RMSE: 79.8487 - val_loss: 20662.4180 - val_RMSE: 143.7443 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 145/5000\n",
      "272/272 - 19s - loss: 6299.8262 - RMSE: 79.3714 - val_loss: 20552.9258 - val_RMSE: 143.3629 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 146/5000\n",
      "272/272 - 19s - loss: 6391.7798 - RMSE: 79.9486 - val_loss: 19550.2090 - val_RMSE: 139.8221 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 147/5000\n",
      "272/272 - 19s - loss: 6261.7842 - RMSE: 79.1314 - val_loss: 20651.8594 - val_RMSE: 143.7076 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 148/5000\n",
      "272/272 - 19s - loss: 6329.8467 - RMSE: 79.5603 - val_loss: 20849.7363 - val_RMSE: 144.3944 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 149/5000\n",
      "272/272 - 19s - loss: 6293.1870 - RMSE: 79.3296 - val_loss: 20381.9492 - val_RMSE: 142.7654 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 150/5000\n",
      "272/272 - 19s - loss: 6355.4429 - RMSE: 79.7210 - val_loss: 20736.8125 - val_RMSE: 144.0028 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 151/5000\n",
      "272/272 - 19s - loss: 6221.7446 - RMSE: 78.8780 - val_loss: 20164.3633 - val_RMSE: 142.0013 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 152/5000\n",
      "272/272 - 19s - loss: 6005.9219 - RMSE: 77.4979 - val_loss: 19223.5410 - val_RMSE: 138.6490 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 153/5000\n",
      "272/272 - 19s - loss: 6069.7690 - RMSE: 77.9087 - val_loss: 21083.6152 - val_RMSE: 145.2020 - lr: 1.0000e-03 - 19s/epoch - 72ms/step\n",
      "Epoch 154/5000\n",
      "272/272 - 19s - loss: 5986.6094 - RMSE: 77.3732 - val_loss: 20000.6035 - val_RMSE: 141.4235 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 155/5000\n",
      "272/272 - 19s - loss: 6054.9033 - RMSE: 77.8133 - val_loss: 20260.2305 - val_RMSE: 142.3384 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 156/5000\n",
      "272/272 - 19s - loss: 6090.3862 - RMSE: 78.0409 - val_loss: 19574.5176 - val_RMSE: 139.9090 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 157/5000\n",
      "272/272 - 19s - loss: 6098.3677 - RMSE: 78.0920 - val_loss: 20204.3887 - val_RMSE: 142.1421 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 158/5000\n",
      "272/272 - 19s - loss: 5827.1289 - RMSE: 76.3356 - val_loss: 19983.8535 - val_RMSE: 141.3643 - lr: 1.0000e-03 - 19s/epoch - 72ms/step\n",
      "Epoch 159/5000\n",
      "272/272 - 19s - loss: 5918.5059 - RMSE: 76.9318 - val_loss: 19123.9609 - val_RMSE: 138.2894 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 160/5000\n",
      "272/272 - 19s - loss: 5921.1943 - RMSE: 76.9493 - val_loss: 18858.1504 - val_RMSE: 137.3250 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 161/5000\n",
      "272/272 - 19s - loss: 5959.9888 - RMSE: 77.2010 - val_loss: 20132.0977 - val_RMSE: 141.8876 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 162/5000\n",
      "272/272 - 19s - loss: 5948.1094 - RMSE: 77.1240 - val_loss: 19796.1309 - val_RMSE: 140.6987 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 163/5000\n",
      "272/272 - 19s - loss: 5848.0493 - RMSE: 76.4725 - val_loss: 20670.8535 - val_RMSE: 143.7736 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 164/5000\n",
      "272/272 - 19s - loss: 5915.5820 - RMSE: 76.9128 - val_loss: 19769.6484 - val_RMSE: 140.6046 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 165/5000\n",
      "272/272 - 19s - loss: 5686.9648 - RMSE: 75.4120 - val_loss: 20713.8535 - val_RMSE: 143.9231 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 166/5000\n",
      "272/272 - 19s - loss: 5599.0127 - RMSE: 74.8265 - val_loss: 19784.4844 - val_RMSE: 140.6573 - lr: 1.0000e-03 - 19s/epoch - 72ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/5000\n",
      "272/272 - 19s - loss: 5637.9551 - RMSE: 75.0863 - val_loss: 20976.5703 - val_RMSE: 144.8329 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 168/5000\n",
      "272/272 - 19s - loss: 5780.0483 - RMSE: 76.0266 - val_loss: 20882.2188 - val_RMSE: 144.5068 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 169/5000\n",
      "272/272 - 19s - loss: 5726.9795 - RMSE: 75.6768 - val_loss: 21005.8398 - val_RMSE: 144.9339 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 170/5000\n",
      "272/272 - 19s - loss: 5684.0571 - RMSE: 75.3927 - val_loss: 20567.3496 - val_RMSE: 143.4132 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 171/5000\n",
      "272/272 - 19s - loss: 5647.8013 - RMSE: 75.1519 - val_loss: 20568.6406 - val_RMSE: 143.4177 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 172/5000\n",
      "272/272 - 19s - loss: 5464.6733 - RMSE: 73.9234 - val_loss: 19024.6309 - val_RMSE: 137.9298 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 173/5000\n",
      "272/272 - 19s - loss: 5430.0708 - RMSE: 73.6890 - val_loss: 21667.2109 - val_RMSE: 147.1979 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 174/5000\n",
      "272/272 - 19s - loss: 5613.9004 - RMSE: 74.9260 - val_loss: 20846.1055 - val_RMSE: 144.3818 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 175/5000\n",
      "272/272 - 19s - loss: 5703.1284 - RMSE: 75.5191 - val_loss: 19992.0176 - val_RMSE: 141.3931 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 176/5000\n",
      "272/272 - 19s - loss: 5490.0518 - RMSE: 74.0949 - val_loss: 19254.0254 - val_RMSE: 138.7589 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 177/5000\n",
      "272/272 - 19s - loss: 5633.6089 - RMSE: 75.0574 - val_loss: 20277.3691 - val_RMSE: 142.3986 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 178/5000\n",
      "272/272 - 19s - loss: 5512.4604 - RMSE: 74.2459 - val_loss: 19785.9922 - val_RMSE: 140.6627 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 179/5000\n",
      "272/272 - 19s - loss: 5471.8960 - RMSE: 73.9723 - val_loss: 20093.5430 - val_RMSE: 141.7517 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 180/5000\n",
      "272/272 - 19s - loss: 5387.3599 - RMSE: 73.3986 - val_loss: 19667.3965 - val_RMSE: 140.2405 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 181/5000\n",
      "272/272 - 19s - loss: 5513.9399 - RMSE: 74.2559 - val_loss: 20782.6211 - val_RMSE: 144.1618 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 182/5000\n",
      "272/272 - 19s - loss: 5352.4868 - RMSE: 73.1607 - val_loss: 20362.4883 - val_RMSE: 142.6972 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 183/5000\n",
      "272/272 - 19s - loss: 5211.5127 - RMSE: 72.1908 - val_loss: 20102.9766 - val_RMSE: 141.7850 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 184/5000\n",
      "272/272 - 19s - loss: 5365.5225 - RMSE: 73.2497 - val_loss: 20395.0000 - val_RMSE: 142.8111 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 185/5000\n",
      "272/272 - 19s - loss: 5433.4644 - RMSE: 73.7120 - val_loss: 20126.5684 - val_RMSE: 141.8681 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 186/5000\n",
      "272/272 - 19s - loss: 5417.5557 - RMSE: 73.6040 - val_loss: 22061.4082 - val_RMSE: 148.5308 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 187/5000\n",
      "272/272 - 19s - loss: 5256.5898 - RMSE: 72.5023 - val_loss: 20985.1191 - val_RMSE: 144.8624 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 188/5000\n",
      "272/272 - 19s - loss: 5339.7080 - RMSE: 73.0733 - val_loss: 20827.6484 - val_RMSE: 144.3179 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 189/5000\n",
      "272/272 - 19s - loss: 5305.6245 - RMSE: 72.8397 - val_loss: 20164.5195 - val_RMSE: 142.0018 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 190/5000\n",
      "272/272 - 19s - loss: 5163.2231 - RMSE: 71.8556 - val_loss: 20494.5723 - val_RMSE: 143.1593 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 191/5000\n",
      "272/272 - 19s - loss: 5209.6865 - RMSE: 72.1782 - val_loss: 19386.3398 - val_RMSE: 139.2348 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 192/5000\n",
      "272/272 - 19s - loss: 5327.4795 - RMSE: 72.9896 - val_loss: 20526.8789 - val_RMSE: 143.2720 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 193/5000\n",
      "272/272 - 19s - loss: 5348.9395 - RMSE: 73.1364 - val_loss: 21752.1309 - val_RMSE: 147.4860 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 194/5000\n",
      "272/272 - 19s - loss: 5271.2271 - RMSE: 72.6032 - val_loss: 19988.2441 - val_RMSE: 141.3798 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 195/5000\n",
      "272/272 - 19s - loss: 5312.6982 - RMSE: 72.8883 - val_loss: 20377.6992 - val_RMSE: 142.7505 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 196/5000\n",
      "272/272 - 19s - loss: 5188.9722 - RMSE: 72.0345 - val_loss: 20980.1191 - val_RMSE: 144.8452 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 197/5000\n",
      "272/272 - 19s - loss: 5014.0059 - RMSE: 70.8096 - val_loss: 21042.3340 - val_RMSE: 145.0598 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 198/5000\n",
      "272/272 - 19s - loss: 5142.0220 - RMSE: 71.7079 - val_loss: 21190.3320 - val_RMSE: 145.5690 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 199/5000\n",
      "272/272 - 19s - loss: 5308.8730 - RMSE: 72.8620 - val_loss: 20114.9395 - val_RMSE: 141.8271 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 200/5000\n",
      "272/272 - 19s - loss: 5378.5376 - RMSE: 73.3385 - val_loss: 19565.1426 - val_RMSE: 139.8755 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 201/5000\n",
      "272/272 - 19s - loss: 5198.0649 - RMSE: 72.0976 - val_loss: 20430.8008 - val_RMSE: 142.9364 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 202/5000\n",
      "272/272 - 19s - loss: 5305.8569 - RMSE: 72.8413 - val_loss: 20367.7266 - val_RMSE: 142.7155 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 203/5000\n",
      "272/272 - 19s - loss: 5128.1870 - RMSE: 71.6114 - val_loss: 19733.5547 - val_RMSE: 140.4762 - lr: 1.0000e-03 - 19s/epoch - 72ms/step\n",
      "Epoch 204/5000\n",
      "272/272 - 19s - loss: 5220.1304 - RMSE: 72.2505 - val_loss: 20696.2031 - val_RMSE: 143.8618 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 205/5000\n",
      "272/272 - 19s - loss: 5204.2749 - RMSE: 72.1407 - val_loss: 19761.5469 - val_RMSE: 140.5758 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 206/5000\n",
      "272/272 - 19s - loss: 5165.6836 - RMSE: 71.8727 - val_loss: 20056.0996 - val_RMSE: 141.6196 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 207/5000\n",
      "272/272 - 19s - loss: 5149.4761 - RMSE: 71.7598 - val_loss: 20211.3145 - val_RMSE: 142.1665 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 208/5000\n",
      "272/272 - 19s - loss: 5353.6133 - RMSE: 73.1684 - val_loss: 19237.8438 - val_RMSE: 138.7005 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 209/5000\n",
      "272/272 - 19s - loss: 5218.0293 - RMSE: 72.2359 - val_loss: 20434.5898 - val_RMSE: 142.9496 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 210/5000\n",
      "272/272 - 19s - loss: 4984.2090 - RMSE: 70.5989 - val_loss: 18396.8457 - val_RMSE: 135.6350 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 211/5000\n",
      "272/272 - 19s - loss: 4976.1958 - RMSE: 70.5422 - val_loss: 20676.1875 - val_RMSE: 143.7922 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 212/5000\n",
      "272/272 - 19s - loss: 5100.4917 - RMSE: 71.4177 - val_loss: 20536.2656 - val_RMSE: 143.3048 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 213/5000\n",
      "272/272 - 19s - loss: 5163.1655 - RMSE: 71.8552 - val_loss: 19957.8965 - val_RMSE: 141.2724 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 214/5000\n",
      "272/272 - 19s - loss: 5232.7822 - RMSE: 72.3380 - val_loss: 19984.1191 - val_RMSE: 141.3652 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 215/5000\n",
      "272/272 - 19s - loss: 5119.4482 - RMSE: 71.5503 - val_loss: 19959.6895 - val_RMSE: 141.2788 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 216/5000\n",
      "272/272 - 19s - loss: 5088.2461 - RMSE: 71.3319 - val_loss: 19359.9102 - val_RMSE: 139.1399 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 217/5000\n",
      "272/272 - 19s - loss: 5073.1274 - RMSE: 71.2259 - val_loss: 22886.9434 - val_RMSE: 151.2843 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 218/5000\n",
      "272/272 - 19s - loss: 4918.3940 - RMSE: 70.1313 - val_loss: 20175.5605 - val_RMSE: 142.0407 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 219/5000\n",
      "272/272 - 19s - loss: 4912.2744 - RMSE: 70.0876 - val_loss: 20594.6797 - val_RMSE: 143.5085 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 220/5000\n",
      "272/272 - 19s - loss: 4949.0747 - RMSE: 70.3497 - val_loss: 19875.0410 - val_RMSE: 140.9789 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 221/5000\n",
      "272/272 - 19s - loss: 5084.2139 - RMSE: 71.3037 - val_loss: 19838.6074 - val_RMSE: 140.8496 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 222/5000\n",
      "272/272 - 19s - loss: 5109.2983 - RMSE: 71.4794 - val_loss: 20119.8672 - val_RMSE: 141.8445 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223/5000\n",
      "272/272 - 19s - loss: 5079.3418 - RMSE: 71.2695 - val_loss: 20028.1797 - val_RMSE: 141.5210 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 224/5000\n",
      "272/272 - 19s - loss: 5049.5547 - RMSE: 71.0602 - val_loss: 21084.7441 - val_RMSE: 145.2059 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 225/5000\n",
      "272/272 - 19s - loss: 5062.9370 - RMSE: 71.1543 - val_loss: 19764.4492 - val_RMSE: 140.5861 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 226/5000\n",
      "272/272 - 19s - loss: 5070.6694 - RMSE: 71.2086 - val_loss: 19239.6836 - val_RMSE: 138.7072 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 227/5000\n",
      "272/272 - 19s - loss: 4902.0137 - RMSE: 70.0144 - val_loss: 19513.0938 - val_RMSE: 139.6893 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 228/5000\n",
      "272/272 - 19s - loss: 5003.3286 - RMSE: 70.7342 - val_loss: 19286.7324 - val_RMSE: 138.8767 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 229/5000\n",
      "272/272 - 19s - loss: 4900.1025 - RMSE: 70.0007 - val_loss: 21407.8145 - val_RMSE: 146.3141 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 230/5000\n",
      "272/272 - 19s - loss: 4937.8394 - RMSE: 70.2698 - val_loss: 19246.1523 - val_RMSE: 138.7305 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 231/5000\n",
      "272/272 - 19s - loss: 4876.3311 - RMSE: 69.8307 - val_loss: 20316.7734 - val_RMSE: 142.5369 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 232/5000\n",
      "272/272 - 19s - loss: 4875.2192 - RMSE: 69.8228 - val_loss: 20149.7676 - val_RMSE: 141.9499 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 233/5000\n",
      "272/272 - 19s - loss: 5029.3813 - RMSE: 70.9181 - val_loss: 21321.1484 - val_RMSE: 146.0176 - lr: 1.0000e-03 - 19s/epoch - 72ms/step\n",
      "Epoch 234/5000\n",
      "272/272 - 19s - loss: 4914.6802 - RMSE: 70.1048 - val_loss: 21012.2773 - val_RMSE: 144.9561 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 235/5000\n",
      "272/272 - 19s - loss: 4961.1387 - RMSE: 70.4353 - val_loss: 19374.2891 - val_RMSE: 139.1916 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 236/5000\n",
      "272/272 - 19s - loss: 4953.8472 - RMSE: 70.3836 - val_loss: 20092.5625 - val_RMSE: 141.7482 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 237/5000\n",
      "272/272 - 19s - loss: 4804.4517 - RMSE: 69.3141 - val_loss: 20100.9258 - val_RMSE: 141.7777 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 238/5000\n",
      "272/272 - 19s - loss: 4935.9575 - RMSE: 70.2564 - val_loss: 21297.6895 - val_RMSE: 145.9373 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 239/5000\n",
      "272/272 - 19s - loss: 4981.8877 - RMSE: 70.5825 - val_loss: 20080.7402 - val_RMSE: 141.7065 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 240/5000\n",
      "272/272 - 19s - loss: 5033.4883 - RMSE: 70.9471 - val_loss: 20590.4355 - val_RMSE: 143.4937 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 241/5000\n",
      "272/272 - 19s - loss: 4970.1694 - RMSE: 70.4994 - val_loss: 20555.6680 - val_RMSE: 143.3725 - lr: 1.0000e-03 - 19s/epoch - 72ms/step\n",
      "Epoch 242/5000\n",
      "272/272 - 19s - loss: 4920.1030 - RMSE: 70.1434 - val_loss: 22469.6992 - val_RMSE: 149.8990 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 243/5000\n",
      "272/272 - 19s - loss: 5045.1118 - RMSE: 71.0289 - val_loss: 19719.0781 - val_RMSE: 140.4246 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 244/5000\n",
      "272/272 - 19s - loss: 4879.1699 - RMSE: 69.8511 - val_loss: 20785.8242 - val_RMSE: 144.1729 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 245/5000\n",
      "272/272 - 19s - loss: 4979.0566 - RMSE: 70.5624 - val_loss: 20932.6426 - val_RMSE: 144.6812 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 246/5000\n",
      "272/272 - 19s - loss: 4949.6025 - RMSE: 70.3534 - val_loss: 20125.5703 - val_RMSE: 141.8646 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 247/5000\n",
      "272/272 - 19s - loss: 4804.8003 - RMSE: 69.3167 - val_loss: 19910.0078 - val_RMSE: 141.1028 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 248/5000\n",
      "272/272 - 19s - loss: 4747.6270 - RMSE: 68.9030 - val_loss: 18919.2012 - val_RMSE: 137.5471 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 249/5000\n",
      "272/272 - 19s - loss: 4722.4727 - RMSE: 68.7202 - val_loss: 20687.3750 - val_RMSE: 143.8311 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 250/5000\n",
      "272/272 - 19s - loss: 4821.9839 - RMSE: 69.4405 - val_loss: 20039.3105 - val_RMSE: 141.5603 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 251/5000\n",
      "272/272 - 19s - loss: 4962.7393 - RMSE: 70.4467 - val_loss: 18303.2520 - val_RMSE: 135.2895 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 252/5000\n",
      "272/272 - 19s - loss: 4891.8701 - RMSE: 69.9419 - val_loss: 19843.0723 - val_RMSE: 140.8654 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 253/5000\n",
      "272/272 - 19s - loss: 4844.6523 - RMSE: 69.6035 - val_loss: 21298.4492 - val_RMSE: 145.9399 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 254/5000\n",
      "272/272 - 19s - loss: 4796.3013 - RMSE: 69.2553 - val_loss: 20115.2227 - val_RMSE: 141.8281 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 255/5000\n",
      "272/272 - 19s - loss: 4889.3726 - RMSE: 69.9240 - val_loss: 19923.9492 - val_RMSE: 141.1522 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 256/5000\n",
      "272/272 - 19s - loss: 4951.5039 - RMSE: 70.3669 - val_loss: 21245.3398 - val_RMSE: 145.7578 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 257/5000\n",
      "272/272 - 19s - loss: 4859.5684 - RMSE: 69.7106 - val_loss: 21069.8418 - val_RMSE: 145.1545 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 258/5000\n",
      "272/272 - 19s - loss: 4805.8960 - RMSE: 69.3246 - val_loss: 20957.7578 - val_RMSE: 144.7679 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 259/5000\n",
      "272/272 - 19s - loss: 4843.8354 - RMSE: 69.5977 - val_loss: 20129.8535 - val_RMSE: 141.8797 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 260/5000\n",
      "272/272 - 19s - loss: 4849.6895 - RMSE: 69.6397 - val_loss: 21691.7344 - val_RMSE: 147.2811 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 261/5000\n",
      "272/272 - 19s - loss: 4894.9761 - RMSE: 69.9641 - val_loss: 21371.2090 - val_RMSE: 146.1889 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 262/5000\n",
      "272/272 - 19s - loss: 4662.9531 - RMSE: 68.2858 - val_loss: 21474.9961 - val_RMSE: 146.5435 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 263/5000\n",
      "272/272 - 19s - loss: 4786.0122 - RMSE: 69.1810 - val_loss: 22805.4121 - val_RMSE: 151.0146 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 264/5000\n",
      "272/272 - 19s - loss: 4714.8833 - RMSE: 68.6650 - val_loss: 19726.5000 - val_RMSE: 140.4510 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 265/5000\n",
      "272/272 - 19s - loss: 4640.3867 - RMSE: 68.1204 - val_loss: 19513.4082 - val_RMSE: 139.6904 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 266/5000\n",
      "272/272 - 19s - loss: 4815.5659 - RMSE: 69.3943 - val_loss: 19314.2246 - val_RMSE: 138.9756 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 267/5000\n",
      "272/272 - 19s - loss: 4882.4497 - RMSE: 69.8745 - val_loss: 20559.4551 - val_RMSE: 143.3857 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 268/5000\n",
      "272/272 - 19s - loss: 4950.8628 - RMSE: 70.3624 - val_loss: 19944.8379 - val_RMSE: 141.2262 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 269/5000\n",
      "272/272 - 19s - loss: 4909.8735 - RMSE: 70.0705 - val_loss: 21074.1680 - val_RMSE: 145.1694 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 270/5000\n",
      "272/272 - 19s - loss: 4792.7544 - RMSE: 69.2297 - val_loss: 20576.9219 - val_RMSE: 143.4466 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 271/5000\n",
      "272/272 - 19s - loss: 4888.1875 - RMSE: 69.9156 - val_loss: 19658.0684 - val_RMSE: 140.2072 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 272/5000\n",
      "272/272 - 19s - loss: 4729.2568 - RMSE: 68.7696 - val_loss: 19667.0098 - val_RMSE: 140.2391 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 273/5000\n",
      "272/272 - 19s - loss: 4972.2808 - RMSE: 70.5144 - val_loss: 21239.9883 - val_RMSE: 145.7395 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 274/5000\n",
      "272/272 - 19s - loss: 4935.0830 - RMSE: 70.2501 - val_loss: 20762.4141 - val_RMSE: 144.0917 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 275/5000\n",
      "272/272 - 19s - loss: 4716.7188 - RMSE: 68.6784 - val_loss: 20615.9238 - val_RMSE: 143.5825 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 276/5000\n",
      "272/272 - 19s - loss: 4607.5259 - RMSE: 67.8788 - val_loss: 20777.8613 - val_RMSE: 144.1453 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 277/5000\n",
      "272/272 - 19s - loss: 4752.4209 - RMSE: 68.9378 - val_loss: 19731.6875 - val_RMSE: 140.4695 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 278/5000\n",
      "272/272 - 19s - loss: 4769.3105 - RMSE: 69.0602 - val_loss: 20924.3125 - val_RMSE: 144.6524 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 279/5000\n",
      "272/272 - 19s - loss: 4710.1733 - RMSE: 68.6307 - val_loss: 19589.7480 - val_RMSE: 139.9634 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 280/5000\n",
      "272/272 - 19s - loss: 4804.6431 - RMSE: 69.3155 - val_loss: 21101.8027 - val_RMSE: 145.2646 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 281/5000\n",
      "272/272 - 19s - loss: 4670.8989 - RMSE: 68.3440 - val_loss: 20813.7734 - val_RMSE: 144.2698 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 282/5000\n",
      "272/272 - 19s - loss: 4852.9189 - RMSE: 69.6629 - val_loss: 20423.4238 - val_RMSE: 142.9106 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 283/5000\n",
      "272/272 - 19s - loss: 4837.6323 - RMSE: 69.5531 - val_loss: 21348.1426 - val_RMSE: 146.1100 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 284/5000\n",
      "272/272 - 19s - loss: 4738.8677 - RMSE: 68.8394 - val_loss: 20825.6543 - val_RMSE: 144.3110 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 285/5000\n",
      "272/272 - 19s - loss: 4652.7520 - RMSE: 68.2111 - val_loss: 22267.2832 - val_RMSE: 149.2223 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 286/5000\n",
      "272/272 - 19s - loss: 4713.2148 - RMSE: 68.6529 - val_loss: 21809.3613 - val_RMSE: 147.6799 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 287/5000\n",
      "272/272 - 19s - loss: 4672.1343 - RMSE: 68.3530 - val_loss: 20912.3867 - val_RMSE: 144.6112 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 288/5000\n",
      "272/272 - 19s - loss: 4711.2446 - RMSE: 68.6385 - val_loss: 21043.1211 - val_RMSE: 145.0625 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 289/5000\n",
      "272/272 - 19s - loss: 4750.9653 - RMSE: 68.9272 - val_loss: 20646.1816 - val_RMSE: 143.6878 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 290/5000\n",
      "272/272 - 19s - loss: 4929.4092 - RMSE: 70.2097 - val_loss: 20230.0762 - val_RMSE: 142.2325 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 291/5000\n",
      "272/272 - 19s - loss: 4835.2402 - RMSE: 69.5359 - val_loss: 20027.3340 - val_RMSE: 141.5180 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 292/5000\n",
      "272/272 - 19s - loss: 4751.5513 - RMSE: 68.9315 - val_loss: 19591.5078 - val_RMSE: 139.9697 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 293/5000\n",
      "272/272 - 19s - loss: 4605.3169 - RMSE: 67.8625 - val_loss: 20435.7656 - val_RMSE: 142.9537 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 294/5000\n",
      "272/272 - 19s - loss: 4753.9023 - RMSE: 68.9485 - val_loss: 20760.1562 - val_RMSE: 144.0838 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 295/5000\n",
      "272/272 - 19s - loss: 4565.1357 - RMSE: 67.5658 - val_loss: 20269.7344 - val_RMSE: 142.3718 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 296/5000\n",
      "272/272 - 19s - loss: 4712.0513 - RMSE: 68.6444 - val_loss: 20754.7305 - val_RMSE: 144.0650 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 297/5000\n",
      "272/272 - 19s - loss: 4582.0405 - RMSE: 67.6908 - val_loss: 19877.7441 - val_RMSE: 140.9884 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 298/5000\n",
      "272/272 - 19s - loss: 4675.8096 - RMSE: 68.3799 - val_loss: 19830.3984 - val_RMSE: 140.8204 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 299/5000\n",
      "272/272 - 19s - loss: 4763.1558 - RMSE: 69.0156 - val_loss: 20492.5879 - val_RMSE: 143.1523 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 300/5000\n",
      "272/272 - 19s - loss: 4736.4277 - RMSE: 68.8217 - val_loss: 20168.8145 - val_RMSE: 142.0170 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 301/5000\n",
      "272/272 - 19s - loss: 4776.6904 - RMSE: 69.1136 - val_loss: 20291.9141 - val_RMSE: 142.4497 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 302/5000\n",
      "272/272 - 19s - loss: 4783.6118 - RMSE: 69.1637 - val_loss: 20210.0059 - val_RMSE: 142.1619 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 303/5000\n",
      "272/272 - 19s - loss: 4822.2812 - RMSE: 69.4426 - val_loss: 20438.8008 - val_RMSE: 142.9643 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 304/5000\n",
      "272/272 - 19s - loss: 4678.7188 - RMSE: 68.4012 - val_loss: 22054.1660 - val_RMSE: 148.5064 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 305/5000\n",
      "272/272 - 19s - loss: 4686.4341 - RMSE: 68.4575 - val_loss: 20362.3965 - val_RMSE: 142.6969 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 306/5000\n",
      "272/272 - 19s - loss: 4738.2910 - RMSE: 68.8352 - val_loss: 20761.0332 - val_RMSE: 144.0869 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 307/5000\n",
      "272/272 - 19s - loss: 4690.8979 - RMSE: 68.4901 - val_loss: 20419.6172 - val_RMSE: 142.8972 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 308/5000\n",
      "272/272 - 19s - loss: 4649.5034 - RMSE: 68.1873 - val_loss: 20524.1445 - val_RMSE: 143.2625 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 309/5000\n",
      "272/272 - 19s - loss: 4608.4927 - RMSE: 67.8859 - val_loss: 20980.4941 - val_RMSE: 144.8465 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 310/5000\n",
      "272/272 - 19s - loss: 4522.1274 - RMSE: 67.2468 - val_loss: 20770.2383 - val_RMSE: 144.1188 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 311/5000\n",
      "272/272 - 19s - loss: 4541.9219 - RMSE: 67.3938 - val_loss: 19416.4922 - val_RMSE: 139.3431 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 312/5000\n",
      "272/272 - 19s - loss: 4597.4888 - RMSE: 67.8048 - val_loss: 19758.1406 - val_RMSE: 140.5636 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 313/5000\n",
      "272/272 - 19s - loss: 4529.2627 - RMSE: 67.2998 - val_loss: 20782.7246 - val_RMSE: 144.1621 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 314/5000\n",
      "272/272 - 19s - loss: 4641.4316 - RMSE: 68.1281 - val_loss: 20761.2520 - val_RMSE: 144.0876 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 315/5000\n",
      "272/272 - 19s - loss: 4704.5410 - RMSE: 68.5897 - val_loss: 20200.7383 - val_RMSE: 142.1293 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 316/5000\n",
      "272/272 - 19s - loss: 4777.7266 - RMSE: 69.1211 - val_loss: 20819.5996 - val_RMSE: 144.2900 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 317/5000\n",
      "272/272 - 19s - loss: 4815.8589 - RMSE: 69.3964 - val_loss: 20552.9570 - val_RMSE: 143.3630 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 318/5000\n",
      "272/272 - 19s - loss: 4807.5449 - RMSE: 69.3365 - val_loss: 19708.6387 - val_RMSE: 140.3875 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 319/5000\n",
      "272/272 - 19s - loss: 4692.5264 - RMSE: 68.5020 - val_loss: 19418.2148 - val_RMSE: 139.3493 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 320/5000\n",
      "272/272 - 19s - loss: 4688.6484 - RMSE: 68.4737 - val_loss: 20332.6348 - val_RMSE: 142.5925 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 321/5000\n",
      "272/272 - 19s - loss: 4511.1255 - RMSE: 67.1649 - val_loss: 18902.5938 - val_RMSE: 137.4867 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 322/5000\n",
      "272/272 - 19s - loss: 4631.1484 - RMSE: 68.0525 - val_loss: 19653.3281 - val_RMSE: 140.1903 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 323/5000\n",
      "272/272 - 19s - loss: 4685.5420 - RMSE: 68.4510 - val_loss: 19292.8145 - val_RMSE: 138.8986 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 324/5000\n",
      "272/272 - 19s - loss: 4569.2515 - RMSE: 67.5962 - val_loss: 18713.2500 - val_RMSE: 136.7964 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 325/5000\n",
      "272/272 - 19s - loss: 4794.5337 - RMSE: 69.2426 - val_loss: 19664.7969 - val_RMSE: 140.2312 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 326/5000\n",
      "272/272 - 19s - loss: 4600.0781 - RMSE: 67.8239 - val_loss: 18841.8027 - val_RMSE: 137.2654 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 327/5000\n",
      "272/272 - 19s - loss: 4503.0537 - RMSE: 67.1048 - val_loss: 20002.0430 - val_RMSE: 141.4286 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 328/5000\n",
      "272/272 - 19s - loss: 4471.0205 - RMSE: 66.8657 - val_loss: 19206.2617 - val_RMSE: 138.5867 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 329/5000\n",
      "272/272 - 19s - loss: 4511.8447 - RMSE: 67.1703 - val_loss: 20508.2500 - val_RMSE: 143.2070 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 330/5000\n",
      "272/272 - 19s - loss: 4532.4136 - RMSE: 67.3232 - val_loss: 20119.4824 - val_RMSE: 141.8432 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 331/5000\n",
      "272/272 - 19s - loss: 4625.9678 - RMSE: 68.0145 - val_loss: 19030.1504 - val_RMSE: 137.9498 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 332/5000\n",
      "272/272 - 19s - loss: 4818.1182 - RMSE: 69.4127 - val_loss: 20565.0742 - val_RMSE: 143.4053 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 333/5000\n",
      "272/272 - 19s - loss: 4736.1157 - RMSE: 68.8194 - val_loss: 19373.2637 - val_RMSE: 139.1879 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 334/5000\n",
      "272/272 - 19s - loss: 4607.0601 - RMSE: 67.8753 - val_loss: 21479.6660 - val_RMSE: 146.5594 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 335/5000\n",
      "272/272 - 19s - loss: 4747.6758 - RMSE: 68.9034 - val_loss: 20963.7461 - val_RMSE: 144.7886 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 336/5000\n",
      "272/272 - 19s - loss: 4612.7314 - RMSE: 67.9171 - val_loss: 21005.4805 - val_RMSE: 144.9327 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 337/5000\n",
      "272/272 - 19s - loss: 4685.7524 - RMSE: 68.4526 - val_loss: 20232.7168 - val_RMSE: 142.2417 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 338/5000\n",
      "272/272 - 19s - loss: 4610.1968 - RMSE: 67.8984 - val_loss: 19824.7637 - val_RMSE: 140.8004 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 339/5000\n",
      "272/272 - 19s - loss: 4523.2095 - RMSE: 67.2548 - val_loss: 18153.6934 - val_RMSE: 134.7356 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 340/5000\n",
      "272/272 - 19s - loss: 4623.4644 - RMSE: 67.9961 - val_loss: 20079.4375 - val_RMSE: 141.7019 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 341/5000\n",
      "272/272 - 19s - loss: 4673.3564 - RMSE: 68.3619 - val_loss: 20058.7324 - val_RMSE: 141.6288 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 342/5000\n",
      "272/272 - 19s - loss: 4730.7603 - RMSE: 68.7805 - val_loss: 19977.2891 - val_RMSE: 141.3410 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 343/5000\n",
      "272/272 - 19s - loss: 4614.7832 - RMSE: 67.9322 - val_loss: 19991.4258 - val_RMSE: 141.3910 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 344/5000\n",
      "272/272 - 19s - loss: 4694.5708 - RMSE: 68.5169 - val_loss: 20804.2559 - val_RMSE: 144.2368 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 345/5000\n",
      "272/272 - 19s - loss: 4536.1660 - RMSE: 67.3511 - val_loss: 19133.1230 - val_RMSE: 138.3225 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 346/5000\n",
      "272/272 - 19s - loss: 4589.5210 - RMSE: 67.7460 - val_loss: 20458.8945 - val_RMSE: 143.0346 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 347/5000\n",
      "272/272 - 19s - loss: 4662.2505 - RMSE: 68.2807 - val_loss: 20666.4375 - val_RMSE: 143.7583 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 348/5000\n",
      "272/272 - 19s - loss: 4593.5684 - RMSE: 67.7759 - val_loss: 20074.9199 - val_RMSE: 141.6860 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 349/5000\n",
      "272/272 - 19s - loss: 4617.7661 - RMSE: 67.9541 - val_loss: 19760.3574 - val_RMSE: 140.5715 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 350/5000\n",
      "272/272 - 19s - loss: 4631.8965 - RMSE: 68.0580 - val_loss: 20617.0840 - val_RMSE: 143.5865 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 351/5000\n",
      "272/272 - 19s - loss: 4504.5405 - RMSE: 67.1159 - val_loss: 19770.5840 - val_RMSE: 140.6079 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 352/5000\n",
      "272/272 - 19s - loss: 4490.0698 - RMSE: 67.0080 - val_loss: 18882.1230 - val_RMSE: 137.4122 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 353/5000\n",
      "\n",
      "Epoch 353: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "272/272 - 19s - loss: 4701.9893 - RMSE: 68.5711 - val_loss: 20507.9355 - val_RMSE: 143.2059 - lr: 1.0000e-03 - 19s/epoch - 71ms/step\n",
      "Epoch 354/5000\n",
      "272/272 - 19s - loss: 4039.4333 - RMSE: 63.5565 - val_loss: 19608.9414 - val_RMSE: 140.0319 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 355/5000\n",
      "272/272 - 19s - loss: 3866.2510 - RMSE: 62.1792 - val_loss: 19839.6211 - val_RMSE: 140.8532 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 356/5000\n",
      "272/272 - 19s - loss: 3799.8606 - RMSE: 61.6430 - val_loss: 19738.1641 - val_RMSE: 140.4926 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 357/5000\n",
      "272/272 - 19s - loss: 3740.9170 - RMSE: 61.1630 - val_loss: 19878.8555 - val_RMSE: 140.9924 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 358/5000\n",
      "272/272 - 19s - loss: 3712.5454 - RMSE: 60.9307 - val_loss: 19844.6680 - val_RMSE: 140.8711 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 359/5000\n",
      "272/272 - 19s - loss: 3677.3071 - RMSE: 60.6408 - val_loss: 19924.3340 - val_RMSE: 141.1536 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 360/5000\n",
      "272/272 - 19s - loss: 3691.9558 - RMSE: 60.7615 - val_loss: 19913.3438 - val_RMSE: 141.1146 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 361/5000\n",
      "272/272 - 19s - loss: 3652.9866 - RMSE: 60.4399 - val_loss: 20150.6387 - val_RMSE: 141.9529 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 362/5000\n",
      "272/272 - 19s - loss: 3673.2041 - RMSE: 60.6070 - val_loss: 20350.3750 - val_RMSE: 142.6547 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 363/5000\n",
      "272/272 - 19s - loss: 3743.4106 - RMSE: 61.1834 - val_loss: 20102.4902 - val_RMSE: 141.7832 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 364/5000\n",
      "272/272 - 19s - loss: 3693.5952 - RMSE: 60.7750 - val_loss: 20128.3867 - val_RMSE: 141.8745 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 365/5000\n",
      "272/272 - 19s - loss: 3728.0342 - RMSE: 61.0576 - val_loss: 19941.3145 - val_RMSE: 141.2137 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 366/5000\n",
      "272/272 - 19s - loss: 3700.8955 - RMSE: 60.8350 - val_loss: 20002.6406 - val_RMSE: 141.4307 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 367/5000\n",
      "272/272 - 19s - loss: 3750.6018 - RMSE: 61.2422 - val_loss: 20378.4902 - val_RMSE: 142.7533 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 368/5000\n",
      "272/272 - 19s - loss: 3689.1697 - RMSE: 60.7385 - val_loss: 20297.5176 - val_RMSE: 142.4694 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 369/5000\n",
      "272/272 - 19s - loss: 3708.6926 - RMSE: 60.8990 - val_loss: 20337.1953 - val_RMSE: 142.6085 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 370/5000\n",
      "272/272 - 19s - loss: 3680.8416 - RMSE: 60.6699 - val_loss: 20025.2539 - val_RMSE: 141.5106 - lr: 1.0000e-04 - 19s/epoch - 72ms/step\n",
      "Epoch 371/5000\n",
      "272/272 - 19s - loss: 3711.1548 - RMSE: 60.9192 - val_loss: 20276.7773 - val_RMSE: 142.3965 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 372/5000\n",
      "272/272 - 19s - loss: 3766.8340 - RMSE: 61.3745 - val_loss: 20217.3242 - val_RMSE: 142.1876 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 373/5000\n",
      "272/272 - 19s - loss: 3750.6941 - RMSE: 61.2429 - val_loss: 20245.2930 - val_RMSE: 142.2859 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 374/5000\n",
      "272/272 - 19s - loss: 3763.6218 - RMSE: 61.3484 - val_loss: 20064.4648 - val_RMSE: 141.6491 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 375/5000\n",
      "272/272 - 19s - loss: 3749.9988 - RMSE: 61.2372 - val_loss: 20208.0625 - val_RMSE: 142.1551 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 376/5000\n",
      "272/272 - 19s - loss: 3782.8826 - RMSE: 61.5051 - val_loss: 20553.3105 - val_RMSE: 143.3643 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 377/5000\n",
      "272/272 - 19s - loss: 3800.0688 - RMSE: 61.6447 - val_loss: 20294.8555 - val_RMSE: 142.4600 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 378/5000\n",
      "272/272 - 19s - loss: 3844.3369 - RMSE: 62.0027 - val_loss: 20097.7148 - val_RMSE: 141.7664 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 379/5000\n",
      "272/272 - 19s - loss: 3814.1794 - RMSE: 61.7590 - val_loss: 20462.3613 - val_RMSE: 143.0467 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 380/5000\n",
      "272/272 - 19s - loss: 3828.4468 - RMSE: 61.8744 - val_loss: 20314.5781 - val_RMSE: 142.5292 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 381/5000\n",
      "272/272 - 19s - loss: 3872.6108 - RMSE: 62.2303 - val_loss: 20365.6152 - val_RMSE: 142.7081 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 382/5000\n",
      "272/272 - 19s - loss: 3875.3462 - RMSE: 62.2523 - val_loss: 19884.3691 - val_RMSE: 141.0119 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 383/5000\n",
      "272/272 - 19s - loss: 3839.1292 - RMSE: 61.9607 - val_loss: 20352.4551 - val_RMSE: 142.6620 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 384/5000\n",
      "272/272 - 19s - loss: 3863.5469 - RMSE: 62.1574 - val_loss: 20369.2012 - val_RMSE: 142.7207 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 385/5000\n",
      "272/272 - 19s - loss: 3842.7024 - RMSE: 61.9895 - val_loss: 20567.4023 - val_RMSE: 143.4134 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 386/5000\n",
      "272/272 - 19s - loss: 3953.3203 - RMSE: 62.8754 - val_loss: 19646.6230 - val_RMSE: 140.1664 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 387/5000\n",
      "272/272 - 19s - loss: 3940.1272 - RMSE: 62.7704 - val_loss: 20209.6484 - val_RMSE: 142.1606 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 388/5000\n",
      "272/272 - 19s - loss: 3900.1846 - RMSE: 62.4515 - val_loss: 20423.5742 - val_RMSE: 142.9111 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 389/5000\n",
      "272/272 - 19s - loss: 3853.3726 - RMSE: 62.0755 - val_loss: 20216.0137 - val_RMSE: 142.1830 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 390/5000\n",
      "272/272 - 19s - loss: 3907.3872 - RMSE: 62.5091 - val_loss: 20422.9297 - val_RMSE: 142.9088 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 391/5000\n",
      "272/272 - 19s - loss: 3899.5327 - RMSE: 62.4462 - val_loss: 20492.2539 - val_RMSE: 143.1512 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 392/5000\n",
      "272/272 - 19s - loss: 3973.3274 - RMSE: 63.0343 - val_loss: 21012.8613 - val_RMSE: 144.9581 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 393/5000\n",
      "272/272 - 19s - loss: 3947.2747 - RMSE: 62.8273 - val_loss: 20453.5527 - val_RMSE: 143.0159 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 394/5000\n",
      "272/272 - 19s - loss: 4014.4604 - RMSE: 63.3598 - val_loss: 20160.1465 - val_RMSE: 141.9864 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 395/5000\n",
      "272/272 - 19s - loss: 3943.4390 - RMSE: 62.7968 - val_loss: 20533.2129 - val_RMSE: 143.2941 - lr: 1.0000e-04 - 19s/epoch - 71ms/step\n",
      "Epoch 396/5000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Configure the hyperparameters\n",
    "\"\"\"\n",
    "\n",
    "learning_rate = 0.01\n",
    "weight_decay = 0.0001\n",
    "batch_size = 2**8\n",
    "num_epochs = 5000\n",
    "image_size = 64  # We'll resize input images to this size\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n",
    "num_classes = 1\n",
    "\n",
    "\"\"\"\n",
    "## Use data augmentation\n",
    "\"\"\"\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Implement multilayer perceptron (MLP)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Implement patch creation as a layer\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Let's display patches for a sample image\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
    "plt.imshow(image.numpy().astype(\"uint8\"))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "resized_image = tf.image.resize(\n",
    "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
    ")\n",
    "patches = Patches(patch_size)(resized_image)\n",
    "print(f\"Image size: {image_size} X {image_size}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "print(f\"Patches per image: {patches.shape[1]}\")\n",
    "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
    "\n",
    "n = int(np.sqrt(patches.shape[1]))\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = tf.reshape(patch, (patch_size, patch_size, 1))\n",
    "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "\"\"\"\n",
    "## Implement the patch encoding layer\n",
    "The `PatchEncoder` layer will linearly transform a patch by projecting it into a\n",
    "vector of size `projection_dim`. In addition, it adds a learnable position\n",
    "embedding to the projected vector.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Build the ViT model\n",
    "The ViT model consists of multiple Transformer blocks,\n",
    "which use the `layers.MultiHeadAttention` layer as a self-attention mechanism\n",
    "applied to the sequence of patches. The Transformer blocks produce a\n",
    "`[batch_size, num_patches, projection_dim]` tensor, which is processed via an\n",
    "classifier head with softmax to produce the final class probabilities output.\n",
    "Unlike the technique described in the [paper](https://arxiv.org/abs/2010.11929),\n",
    "which prepends a learnable embedding to the sequence of encoded patches to serve\n",
    "as the image representation, all the outputs of the final Transformer block are\n",
    "reshaped with `layers.Flatten()` and used as the image\n",
    "representation input to the classifier head.\n",
    "Note that the `layers.GlobalAveragePooling1D` layer\n",
    "could also be used instead to aggregate the outputs of the Transformer block,\n",
    "especially when the number of patches and the projection dimensions are large.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=((image_size, image_size, 1)))\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "## Compile, train, and evaluate the mode\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def run_experiment(model):\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.MeanSquaredError(),\n",
    "        metrics=[\n",
    "            keras.metrics.RootMeanSquaredError(name=\"RMSE\"),\n",
    "\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        ReduceLROnPlateau(\n",
    "        monitor='loss', factor=0.1, patience=25, verbose=True,\n",
    "        mode='min', min_delta=1, cooldown=25, min_lr=1e-7,),\n",
    "        EarlyStopping(monitor='RMSE', patience=100, verbose=False, min_delta=1, restore_best_weights=True),\n",
    "    ]\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=callbacks,\n",
    "        verbose=2,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "vit_classifier = create_vit_classifier()\n",
    "history = run_experiment(vit_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41673c79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
