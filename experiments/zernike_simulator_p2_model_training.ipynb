{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d552de5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "from tifffile import imread\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('/home/miguel/Projects/uni/phd/smlm_z/smlm-z/src/smlm_z/pipelines')\n",
    "try:\n",
    "    from preprocessing.nodes import extract_training_stacks\n",
    "except ImportError:\n",
    "    from preprocessing.nodes import extract_training_stacks\n",
    "\n",
    "bead_stacks = [\n",
    "    '/home/miguel/Projects/uni/data/smlm_3d/20230331_beads/3um_stack_20nm_step_20nm_bead_5/3um_stack_20nm_step_20nm_bead_5_MMStack_Default.ome.tif',\n",
    "#     '/home/miguel/Projects/uni/data/smlm_3d/20230331_beads/3um_stack_20nm_step_20nm_bead_3/3um_stack_20nm_step_20nm_bead_3_MMStack_Default.ome.tif',\n",
    "#     '/home/miguel/Projects/uni/data/smlm_3d/20230331_beads/3um_stack_20nm_step_20nm_bead_9/3um_stack_20nm_step_20nm_bead_9_MMStack_Default.ome.tif',\n",
    "#     '/home/miguel/Projects/uni/data/smlm_3d/20230331_beads/3um_stack_20nm_step_20nm_bead_7/3um_stack_20nm_step_20nm_bead_7_MMStack_Default.ome.tif',\n",
    "#     '/home/miguel/Projects/uni/data/smlm_3d/20230331_beads/3um_stack_20nm_step_20nm_bead_6/3um_stack_20nm_step_20nm_bead_6_MMStack_Default.ome.tif',\n",
    "#     '/home/miguel/Projects/uni/data/smlm_3d/20230331_beads/3um_stack_20nm_step_20nm_bead_2/3um_stack_20nm_step_20nm_bead_2_MMStack_Default.ome.tif',\n",
    "#     '/home/miguel/Projects/uni/data/smlm_3d/20230331_beads/3um_stack_20nm_step_20nm_bead_4/3um_stack_20nm_step_20nm_bead_4_MMStack_Default.ome.tif',\n",
    "#     '/home/miguel/Projects/uni/data/smlm_3d/20230331_beads/3um_stack_20nm_step_20nm_bead_8/3um_stack_20nm_step_20nm_bead_8_MMStack_Default.ome.tif',\n",
    "]\n",
    "\n",
    "all_spots = []\n",
    "all_locs = []\n",
    "all_stacks = []\n",
    "for bead_stack_path in bead_stacks:\n",
    "    bead_stack = imread(bead_stack_path)\n",
    "    slice_path = bead_stack_path.replace('.ome', '_slice.ome')\n",
    "    spots_path = slice_path.replace('.tif', '_spots.hdf5')\n",
    "    with h5py.File(spots_path) as f:\n",
    "        spots = np.array(f['spots'])\n",
    "\n",
    "    locs_path = spots_path.replace('_spots', '_locs')\n",
    "    locs = pd.read_hdf(locs_path, key='locs')\n",
    "\n",
    "    from skimage.feature import match_template\n",
    "\n",
    "    parameters = {\n",
    "        'picasso': {\n",
    "            'spot_size': 31,\n",
    "            'localised_frame': bead_stack.shape[0]//2\n",
    "        },\n",
    "        'DEBUG': False\n",
    "    }\n",
    "    stacks = extract_training_stacks(spots, bead_stack, parameters)\n",
    "\n",
    "    all_spots.append(spots)\n",
    "    all_locs.append(locs)\n",
    "    all_stacks.append(stacks)\n",
    "\n",
    "min_stack_length = min(list(map(lambda s: s.shape[1], all_stacks)))\n",
    "all_stacks = [s[:, :min_stack_length] for s in all_stacks]\n",
    "\n",
    "spots = np.concatenate(all_spots)\n",
    "locs = pd.concat(all_locs)\n",
    "stacks = np.concatenate(all_stacks)\n",
    "\n",
    "print(spots.shape)\n",
    "print(locs.shape)\n",
    "print(stacks.shape)\n",
    "max_pixel_val = np.max(stacks, axis=(1,2,3))\n",
    "mean_pixel_val = np.median(stacks, axis=(1,2,3))\n",
    "snr = max_pixel_val / mean_pixel_val\n",
    "locs['snr'] = snr\n",
    "\n",
    "locs = locs.reset_index(drop=True)\n",
    "\n",
    "# for i in np.argsort(snr):\n",
    "#     if snr[i] > 3:\n",
    "#         show_psf_axial(stacks[i], snr[i])\n",
    "    \n",
    "plt.rcParams[\"figure.figsize\"] = (4, 4)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.scatterplot(data=locs[locs['snr'] > 3], x='x', y='y', hue='snr')\n",
    "plt.xlim(locs['x'].min(), locs['x'].max())\n",
    "plt.ylim(locs['y'].min(), locs['y'].max())\n",
    "plt.show()\n",
    "\n",
    "\n",
    "idx = locs['snr'] > 3\n",
    "locs = locs[idx]\n",
    "spots = spots[idx]\n",
    "stacks = stacks[idx, :150]\n",
    "stacks = np.stack([mask_psf(s, 1, 12) for s in stacks])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ee200",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from smlm_z.pipelines.preprocessing.nodes import resize_stacks, norm_stacks\n",
    "except ImportError:\n",
    "    from smlm_z.pipelines.preprocessing.nodes import resize_stacks, norm_stacks\n",
    "stacks_min, stacks_max, stacks_dtype = stacks.min(), stacks.max(), stacks.dtype\n",
    "# stacks = resize_stacks(stacks, target_shape=(stacks.shape[1], 32, 32))\n",
    "stacks = norm_stacks(stacks)\n",
    "print(stacks.min(), stacks.max())\n",
    "# stacks -= stacks.min()\n",
    "# stacks = stacks / stacks.max()\n",
    "# stacks *= (stacks_max - stacks_min)\n",
    "# stacks += stacks_min\n",
    "# stacks = stacks.astype(stacks_dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c413e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    target_psf = stacks[i]\n",
    "    show_psf_axial(target_psf, str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a644aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyotf.utils import prep_data_for_PR\n",
    "target_spot = 5\n",
    "\n",
    "target_psf = stacks[target_spot].copy()\n",
    "df = locs\n",
    "\n",
    "z_step = 0.02\n",
    "z_range = target_psf.shape[0] * z_step / 2\n",
    "print(target_psf.shape)\n",
    "print(z_step)\n",
    "print(z_range)\n",
    "\n",
    "optical_params = dict(Nn=target_psf.shape[1], \n",
    "                          pixel_size=0.110, \n",
    "                          zrange=z_range, \n",
    "                          dz=z_step, \n",
    "                          magnification=111.11,\n",
    "                          ill_NA=1.4,\n",
    "                          det_NA=1.3,\n",
    "                          n=1.335,\n",
    "#                           fwhmz=3.0,\n",
    "                          ill_wavelength=635,\n",
    "                          det_wavelength=635)\n",
    "\n",
    "\n",
    "\n",
    "print(target_psf.min(), target_psf.max())\n",
    "show_psf_axial(target_psf)\n",
    "\n",
    "\n",
    "# fit_psf(target_psf, optical_params, n_coefs=16, lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feefba29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed522d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(results)\n",
    "print(df)\n",
    "import seaborn as sns\n",
    "sns.boxplot(data=df, x='opt', y='error')\n",
    "plt.show()\n",
    "\n",
    "sns.boxplot(data=df, x='opt', y='error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a66e5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 3)\n",
    "\n",
    "df = pd.DataFrame.from_records(results)\n",
    "print(f'{df[\"error\"].mean():.4E}')\n",
    "df['param_sum'] = df['params'].apply(lambda x: sum(abs(np.array(x[1]))))\n",
    "\n",
    "print(df.shape)\n",
    "df = df[df['param_sum'] < 100]\n",
    "print(df.shape)\n",
    "idx = df['error'] <= 1\n",
    "df = df[idx]\n",
    "\n",
    "\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values)\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(data=df, x='param_sum', y='error')\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "for c in list(df):\n",
    "    if 'coef' in c:\n",
    "        f, axs = plt.subplots(1, 2)\n",
    "        sns.scatterplot(data=df, x='x', y='y', hue=c, ax=axs[0])\n",
    "        sns.scatterplot(data=df, x='x', y='y', hue='error', ax=axs[1])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ca16ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "df2['x'] -= 450\n",
    "df2['y'] -= 450\n",
    "\n",
    "# Define a function to convert cartesian coordinates to polar coordinates\n",
    "def cartesian_to_polar(x, y):\n",
    "    radius = np.sqrt(x**2 + y**2)\n",
    "    angle = np.arctan2(y, x)\n",
    "    return radius, angle\n",
    "\n",
    "# Apply the conversion function to each row of the DataFrame\n",
    "df2['r'], df2['phi'] = zip(*df2.apply(lambda row: cartesian_to_polar(row['x'], row['y']), axis=1))\n",
    "\n",
    "from  matplotlib.colors import LinearSegmentedColormap\n",
    "cmap=LinearSegmentedColormap.from_list('gr',['g', 'black', 'r'], N=64) \n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 3)\n",
    "\n",
    "for c in list(df):\n",
    "    if 'coef' in c or 'offset' in c:\n",
    "        f, axs = plt.subplots(1, 2)\n",
    "        if 'coef' in c:\n",
    "            coef_id = int(c.split('_')[-1]) + 1\n",
    "            title = poppy.zernike.zern_name(coef_id)\n",
    "        else:\n",
    "            title = 'offset'\n",
    "        plt.title(title)\n",
    "        sns.scatterplot(data=df2, x='phi', y=c, hue='error', ax=axs[0], palette=cmap)\n",
    "        sns.scatterplot(data=df2, x='r', y=c, hue='error', ax=axs[1], palette=cmap)\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3674aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['x_norm'] = norm_zero_one(df['x'])\n",
    "df['y_norm'] = norm_zero_one(df['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d8adf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_psf_axial(stacks[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ea066e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from smlm_z.pipelines.preprocessing.nodes import norm_images, resize_stacks\n",
    "from data.estimate_offset import get_peak_sharpness\n",
    "\n",
    "# Eval data\n",
    "stack_psfs = stacks\n",
    "\n",
    "val_zs = []\n",
    "val_xy = []\n",
    "val_psfs = []\n",
    "for i in range(stack_psfs.shape[0]):\n",
    "    psf = stack_psfs[i]\n",
    "    \n",
    "#     peak = int(get_peak_sharpness(psf))\n",
    "#     z_range = 1.0\n",
    "#     n_slices_from_focus = int(z_range / z_step)\n",
    "#     start = max((peak-n_slices_from_focus, 0))\n",
    "#     end = min((peak+n_slices_from_focus, psf.shape[0]))\n",
    "    \n",
    "#     psf = psf[start:end]\n",
    "    peak = 0\n",
    "    val_zs.append((np.arange(psf.shape[0])-peak) * z_step * 1000)\n",
    "    xy_coords = df.iloc[i][['x_norm', 'y_norm']].tolist()\n",
    "#     xy_coords = [0, 0]\n",
    "    val_xy.append(xy_coords)\n",
    "    val_psfs.append(psf.astype(float))\n",
    "#     show_psf_axial(stack_psfs[i])\n",
    "\n",
    "val_xy = np.array(val_xy)\n",
    "val_zs = np.array(val_zs)\n",
    "for i in range(len(val_psfs)):\n",
    "    val_psfs[i] = norm_images(val_psfs[i])\n",
    "    val_psfs[i] = val_psfs[i][:, :, :, np.newaxis]\n",
    "    val_psfs[i] = np.repeat(val_psfs[i], 3, axis=-1)\n",
    "\n",
    "# show_psf_axial(val_psfs[i].mean(axis=-1))\n",
    "\n",
    "print(val_xy.shape)\n",
    "print(val_zs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee0b64a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "train_psfs = []\n",
    "train_zs = []\n",
    "train_xy = []\n",
    "for entry in df.to_dict(orient='records'):\n",
    "    psf = simul.get_scalar_psf(offset=0, zern_coefs=entry['params'][1])\n",
    "    plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "    show_psf_axial(psf)\n",
    "\n",
    "    psf = norm_images(psf)[:, :, :, np.newaxis]\n",
    "    psf = np.repeat(psf, 3, axis=-1)\n",
    "    zs = np.linspace(-simul.zrange, simul.zrange-simul.dz, psf.shape[0]) * 1000\n",
    "    train_psfs.append(psf)\n",
    "    train_zs.append(zs)\n",
    "    xy_coords = [entry['x_norm'], entry['y_norm']]\n",
    "    for _ in range(psf.shape[0]):\n",
    "        train_xy.append(xy_coords)\n",
    "\n",
    "train_psfs = np.concatenate(train_psfs)\n",
    "\n",
    "train_xy = np.array(train_xy)\n",
    "train_zs = np.array(train_zs).flatten()[:, np.newaxis]\n",
    "print(train_psfs.shape)\n",
    "print(train_xy.shape)\n",
    "print(train_zs.shape)\n",
    "\n",
    "X = (train_psfs, train_xy)\n",
    "y = train_zs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9606a105",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'train_size': 0.8, \n",
    "    'test_size': 0.1, \n",
    "    'val_size': 0.1, \n",
    "    'random_seed': 42,\n",
    "    'training': {\n",
    "        'learning_rate': 1e-4,\n",
    "        'batch_size': 128,\n",
    "        'max_epochs': 1000,\n",
    "        'aug_ratio': 4\n",
    "    },\n",
    "    'data_augmentation': {\n",
    "        'noise_std': 0.3,\n",
    "        'max_lateral_translation_px': 2,\n",
    "    },\n",
    "    'model_input_shape': [32, 32, 3],\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef304da2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from smlm_z.pipelines.train_model.nodes import split_train_val_test, train_classifier, check_data, augment_datasets, check_data\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = split_train_val_test(X, y, params)\n",
    "\n",
    "print(X_train[0].shape, X_train[1].shape, y_train.shape)\n",
    "X_train, y_train = augment_datasets(X_train, y_train, params)\n",
    "print(X_train[0].shape, X_train[1].shape, y_train.shape)\n",
    "plt.rcParams[\"figure.figsize\"] = (3, 3)\n",
    "# check_data(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "\n",
    "X_exp, y_exp = (np.concatenate(val_psfs), val_xy), np.concatenate(val_zs)\n",
    "print(X_exp[0].shape, X_exp[1].shape, y_exp.shape)\n",
    "\n",
    "check_data(X_train, y_train, X_val, y_val, X_exp, y_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb762052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "            \n",
    "LOAD=True\n",
    "\n",
    "if LOAD:\n",
    "    for fpath in os.listdir('./backup'):\n",
    "        varname = fpath.split('.')[0]\n",
    "        if 'pickle' in fpath:\n",
    "            with open(f'./backup/{fpath}', 'rb') as f:\n",
    "                globals()[varname] = pickle.load(f)\n",
    "        \n",
    "            \n",
    "print(X_train[0].shape, X_train[1].shape)\n",
    "print(y_train.shape, y_train.shape)\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from smlm_z.pipelines.train_model.nodes import split_train_val_test, train_classifier, check_data, augment_datasets, check_data\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = split_train_val_test(X, y, params)\n",
    "\n",
    "\n",
    "X_exp, y_exp = (np.concatenate(val_psfs), val_xy), np.concatenate(val_zs)\n",
    "print(X_exp[0].shape, X_exp[1].shape, y_exp.shape)\n",
    "\n",
    "check_data(X_train, y_train, X_val, y_val, X_exp, y_exp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303b8fc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import keras\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from pyotf.utils import prep_data_for_PR\n",
    "from smlm_z.pipelines.train_model.nodes import train_classifier\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (5, 5)\n",
    "\n",
    "def eval_bead_stack(model):\n",
    "\n",
    "    errors = []\n",
    "    i = 0 \n",
    "    for stack, xy, zs in zip(val_psfs, val_xy, val_zs):\n",
    "        xy = np.repeat(xy[np.newaxis, :], stack.shape[0], 0)\n",
    "        stack -= np.median(stack)\n",
    "    #     stack2 = (stack * 255 * 255).mean(axis=-1).astype(int)\n",
    "    #     stack2 = prep_data_for_PR(stack2, multiplier=1.01)\n",
    "    #     stack2 = norm_images(stack2)\n",
    "    #     stack2 = np.repeat(stack2[:, :, :, np.newaxis], 3, axis=-1)\n",
    "        pred = model.predict((stack, xy), verbose=0)\n",
    "        pred -= pred.min()\n",
    "        error = mean_absolute_error(pred, zs)\n",
    "        errors.append(float(error))\n",
    "        plt.plot(zs, pred, alpha=0.2)\n",
    "    plt.show()\n",
    "\n",
    "    errors = np.mean(errors)\n",
    "    tqdm.write(f'MAE: {errors:.3E}')\n",
    "    \n",
    "        \n",
    "\n",
    "class myCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 25 == 0:\n",
    "            eval_bead_stack(self.model)\n",
    "\n",
    "custom_callback = myCallback()\n",
    "\n",
    "\n",
    "\n",
    "model, _ = train_classifier(X_train, y_train, X_val, y_val, params, extra_callbacks=[custom_callback])\n",
    "\n",
    "model_file = './backup/model'\n",
    "model.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc64ee2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from data.visualise import grid_psfs, show_psf_axial\n",
    "\n",
    "for i in range(10):\n",
    "    show_psf_axial(val_psfs[i].mean(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a5a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "model = keras.models.load_model(model_file)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639cfad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smlm_z.pipelines.train_model.nodes import eval_classifier\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (3, 3)\n",
    "# Eval on training data\n",
    "eval_classifier(model, X_train, y_train, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6c6171",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def eval_bead_stack(model):\n",
    "\n",
    "    errors = []\n",
    "    i = 0 \n",
    "    for stack, xy, zs in zip(val_psfs, val_xy, val_zs):\n",
    "        xy = np.repeat(xy[np.newaxis, :], stack.shape[0], 0)\n",
    "        stack -= np.median(stack)\n",
    "#         stack = (stack * 255 * 255).mean(axis=-1).astype(int)\n",
    "#         stack = prep_data_for_PR(stack, multiplier=1.01)\n",
    "#         stack2 = norm_images(stack2)\n",
    "    #     stack2 = np.repeat(stack2[:, :, :, np.newaxis], 3, axis=-1)\n",
    "        pred = model.predict((stack, xy), verbose=0)\n",
    "        zs -= zs.min()\n",
    "        pred -= pred.min()\n",
    "        error = mean_absolute_error(pred, zs)\n",
    "        errors.append(float(error))\n",
    "        plt.plot(zs, pred)\n",
    "        plt.show()\n",
    "        show_psf_axial(stack.mean(axis=-1), '', 1)\n",
    "    errors = np.mean(errors)\n",
    "    tqdm.write(f'MAE: {errors:.3E}')\n",
    "eval_bead_stack(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c233c7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
