{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b3b663-70ec-4845-aaf8-bbcabe020698",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "df_fdloco = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/comparisons/fd-loco/fd_deeploc_results/fov1_locs_remapped_undrift_picked.hdf5', key='locs')\n",
    "df_ours = pd.read_hdf('/home/miguel/Projects/smlm_z/autofocus/VIT_zeiss_lowsnr_data/out_24_nvidia6/out_nup/nup_renders3/nup.hdf5', key='locs')\n",
    "\n",
    "# def gen_fake_df(add_group=False):\n",
    "#     points = [\n",
    "#         [0, 0],\n",
    "#         [0, 1],\n",
    "#         [1, 0],\n",
    "#         [1, 1]\n",
    "#     ]\n",
    "    \n",
    "#     if add_group:\n",
    "#         points = [\n",
    "#             [0, 0],\n",
    "#             [0, 1],\n",
    "#             [1, 0],\n",
    "#             [1, 1],\n",
    "#             [5, 5]\n",
    "#         ]\n",
    "#     points = np.array(points)\n",
    "#     groups = np.arange(points.shape[0])\n",
    "#     np.random.shuffle(groups)\n",
    "#     all_points = []\n",
    "#     for (x, y), g in zip(points, groups):\n",
    "#         all_x = np.random.normal(x, 0.05, size=10)\n",
    "#         all_y = np.random.normal(y, 0.05, size=10)\n",
    "#         all_g = np.array([g] * 10).astype(int)\n",
    "#         arr = np.vstack((all_x, all_y, all_g)).T\n",
    "#         all_points.append(arr)\n",
    "#     all_points = np.concatenate(all_points)\n",
    "#     df = pd.DataFrame(all_points, columns=['x', 'y', 'group'])\n",
    "#     df['group'] = df['group'].astype(int)\n",
    "#     return df\n",
    "        \n",
    "        \n",
    "# df_fdloco = gen_fake_df()\n",
    "# df_ours = gen_fake_df(True)\n",
    "# for df in (df_fdloco, df_ours):\n",
    "#     sns.scatterplot(data=df, x='x', y='y', hue='group')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b309f2-2787-4129-9187-6ebb81083901",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/miguel/Projects/smlm_z/publication/comparisons/results.csv')\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(df['Ours'], df['DECODE'])\n",
    "plt.title('Without 2d circularity filter')\n",
    "plt.xlabel('DECODE')\n",
    "plt.ylabel('Ours')\n",
    "plt.show()\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(df['Ours'], df['FD-Deeploc'])\n",
    "plt.title('Without 2d circularity filter')\n",
    "plt.xlabel('FD-Deeploc')\n",
    "plt.ylabel('Ours')\n",
    "plt.show()\n",
    "\n",
    "df = df[df['circular_fit']]\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(df['Ours'], df['DECODE'])\n",
    "plt.title('With 2d circularity filter')\n",
    "plt.xlabel('DECODE')\n",
    "plt.ylabel('Ours')\n",
    "plt.show()\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(df['Ours'], df['FD-Deeploc'])\n",
    "plt.title('With 2d circularity filter')\n",
    "plt.xlabel('FD-Deeploc')\n",
    "plt.ylabel('Ours')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96feb7b-c2b9-4363-89a9-2e4627fee265",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [\n",
    "    '/home/miguel/Projects/smlm_z/publication/comparisons/decode/emitter_remapped_undrift_picked.hdf5',\n",
    "    '/home/miguel/Projects/smlm_z/publication/comparisons/decode/emitter_remapped_undrift.hdf5',\n",
    "    '/home/miguel/Projects/smlm_z/publication/comparisons/fd-loco/fd_deeploc_results/fov1_locs_remapped_undrift_picked.hdf5',\n",
    "]\n",
    "for df_path in dfs:\n",
    "    df = pd.read_hdf(df_path, key='locs')\n",
    "    df['z'] /= 106\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e498166-c4d7-47c8-8ca5-e19ba5b276e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miguel/anaconda3/envs/smlm_z/lib/python3.11/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'x [nm]'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/miguel/anaconda3/envs/smlm_z/lib/python3.11/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'y [nm]'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n",
      "/home/miguel/anaconda3/envs/smlm_z/lib/python3.11/site-packages/tables/path.py:137: NaturalNameWarning: object name is not a valid Python identifier: 'z [nm]'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n",
      "  check_attribute_name(name)\n"
     ]
    }
   ],
   "source": [
    "locs_path = '/home/miguel/Projects/smlm_z/publication/comparisons/decode/emitter_remapped_undrift_picked_matched.hdf5'\n",
    "\n",
    "df = pd.read_hdf(locs_path, key='locs')\n",
    "df['z [nm]'] = df['z'] * 106\n",
    "\n",
    "import h5py\n",
    "if 'index' in list(df):\n",
    "    del df['index']\n",
    "with h5py.File(locs_path, \"w\") as locs_file:\n",
    "    locs_file.create_dataset(\"locs\", data=df.to_records())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5db0539-1f65-49c3-96eb-624a0a9db9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "PIXEL_SIZE = 106\n",
    "df = pd.read_hdf('/home/miguel/Projects/smlm_z/autofocus/VIT_zeiss_lowsnr_data/out_24_nvidia6_bak/out_nup_alt/nup_renders3/nup.hdf5', key='locs')\n",
    "from matplotlib import patches\n",
    "def is_circular(points, threshold=0.8):\n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(points)\n",
    "    # Check if the explained variance ratio is roughly equal for both components\n",
    "    return np.min(pca.explained_variance_ratio_) > threshold\n",
    "    \n",
    "\n",
    "def fit_circle(points, radius):\n",
    "    def objective(center):\n",
    "        return np.mean((np.sqrt(np.sum((points - center)**2, axis=1)) - radius)**2)\n",
    "    \n",
    "    # Initial guess: mean of the points\n",
    "    initial_center = np.mean(points, axis=0)\n",
    "    \n",
    "    # Optimize to find the best center\n",
    "    result = minimize(objective, initial_center, method='nelder-mead')\n",
    "    \n",
    "    return result.x, objective(result.x)\n",
    "\n",
    "\n",
    "def check_2d_fit(df):\n",
    "    if df.shape[0] < 20:\n",
    "        return False\n",
    "    radius_px = (110/PIXEL_SIZE)/2\n",
    "    points = df[['x', 'y']].to_numpy()\n",
    "\n",
    "    if not is_circular(points, threshold=0.2):\n",
    "        sns.scatterplot(data=sub_df, x='x', y='y')\n",
    "        plt.title('Uneven distrib')\n",
    "        plt.show()\n",
    "        return False\n",
    "    \n",
    "    center, error = fit_circle(points, radius_px)\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.kdeplot(data=sub_df, x='x', y='y', ax=ax, bw_adjust=0.5)\n",
    "    circle = patches.Circle(center, radius_px, fill=False)\n",
    "    ax.add_artist(circle)\n",
    "    \n",
    "    plt.title(f'{error < 0.05}, {str(round(error, 4))}')\n",
    "    plt.show()\n",
    "    return error < 0.05\n",
    "\n",
    "for g in sorted(set(df['group'])):\n",
    "    sub_df = df[df['group']==g]\n",
    "    check_2d_fit(sub_df)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "72406c0a-157c-4dfb-b04a-7e2ac8b72f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      202\n",
      "1      259\n",
      "2      470\n",
      "3      681\n",
      "4      762\n",
      "5      955\n",
      "6     1175\n",
      "7     1210\n",
      "8     1670\n",
      "9     1966\n",
      "10    2491\n",
      "Name: group, dtype: int64\n",
      "{762}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv('/home/miguel/Projects/smlm_z/publication/comparisons/kde_results.csv')\n",
    "if 'Unnamed: 0' in list(df):\n",
    "    del df['Unnamed: 0']\n",
    "\n",
    "import os\n",
    "\n",
    "def path_exists(g):\n",
    "    return os.path.exists(f'/home/miguel/Projects/smlm_z/publication/comparisons/comparison_plots/good/{g}.png')\n",
    "\n",
    "# df = df[(df['Ours_is_circular'] & df['DECODE_is_circular'] & df['FD-Deeploc_is_circular'])]\n",
    "# groups = list(df['group'].to_numpy())\n",
    "# groups = list(filter(path_exists, groups))\n",
    "# # print(groups)\n",
    "\n",
    "# df = df[df['bandwidth'] == 15]\n",
    "# df2 = df.pivot(index='group', columns='method', values='quality')\n",
    "# groups2 = df2[(df2['Ours']==1) & (df2['FD-Deeploc'] == 1)].reset_index()['group']\n",
    "# print(groups2)\n",
    "# print(set(groups).intersection(set(groups2)))\n",
    "\n",
    "\n",
    "# sns.lineplot(data=df.groupby(['bandwidth', 'method']).sum(), x='bandwidth', y='quality', hue='method')\n",
    "# plt.ylabel('Good reconstructions (seperation between [40,60] nm)')\n",
    "# plt.xlabel('KDE Bandwidth')\n",
    "# # plt.title('W/o filtering of groups for circular structure in X/Y ')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# for m in ('Ours', 'DECODE', 'FD-Deeploc'):\n",
    "#     f = (df['method'] == m) & (df[f'{m}_is_circular'] == False)\n",
    "#     df.loc[f, 'quality'] = 0\n",
    "    \n",
    "# # del df['is_circular']\n",
    "# df = df.groupby(['bandwidth', 'method']).sum()\n",
    "\n",
    "# sns.lineplot(data=df, x='bandwidth', y='quality', hue='method')\n",
    "# plt.ylabel('Good reconstructions (seperation between [40,60] nm)')\n",
    "# plt.xlabel('KDE Bandwidth')\n",
    "# plt.title('With filtering of groups for circular structure in X/Y ')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8dc274-21f0-4353-a42c-12467db3586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "df = pd.read_hdf('/home/miguel/Projects/smlm_z/autofocus/VIT_zeiss_lowsnr_data/out_24_nvidia6_bak/out_nup/nup_renders3/nup.hdf5', key='locs')\n",
    "for g in list(set(df['group']))[2:]:\n",
    "    sub_df = df[df['group']==g]\n",
    "    zs = sub_df['z [nm]'].to_numpy()[:, None]\n",
    "    \n",
    "    sns.histplot(zs, bins=40, stat='density')\n",
    "    for bw in [5, 20, 25]:\n",
    "        kde = KernelDensity(bandwidth=bw)\n",
    "        kde.fit(zs)\n",
    "\n",
    "        x = np.linspace(zs.min(), zs.max(), 1000)[:, None]\n",
    "        plt.plot(x, np.exp(kde.score_samples(x)), label=round(bw, 3))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f05f0d-a338-430f-bbad-327ba735e22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "\n",
    "def is_circular(points, threshold=0.8):\n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(points)\n",
    "    # Check if the explained variance ratio is roughly equal for both components\n",
    "    return np.min(pca.explained_variance_ratio_) > threshold\n",
    "    \n",
    "def fit_circle(points, radius):\n",
    "    def objective(center):\n",
    "        return np.mean((np.sqrt(np.sum((points - center)**2, axis=1)) - radius)**2)\n",
    "    \n",
    "    # Initial guess: mean of the points\n",
    "    initial_center = np.mean(points, axis=0)\n",
    "    \n",
    "    # Optimize to find the best center\n",
    "    result = minimize(objective, initial_center, method='nelder-mead')\n",
    "    \n",
    "    return result.x, objective(result.x)\n",
    "\n",
    "def check_2d_fit(df, group):\n",
    "    df = df[df['group']==group]\n",
    "    if df.shape[0] < 20:\n",
    "        return False\n",
    "    radius_px = (110/106)/2\n",
    "    points = df[['x', 'y']].to_numpy()\n",
    "\n",
    "    if not is_circular(points, threshold=0.3):\n",
    "        return False\n",
    "    \n",
    "    center, error = fit_circle(points, radius_px)\n",
    "\n",
    "    return error < 0.02\n",
    "\n",
    "\n",
    "def filter_df_groups(df):\n",
    "    groups = np.array(sorted(set(df['group'])))\n",
    "    good_idx = np.argwhere([check_2d_fit(df, i) for i in tqdm(groups)])[:, 0]\n",
    "    groups = groups[good_idx]\n",
    "    # df = df[df['group'].isin(groups)]\n",
    "    # remap_groups = {k: v for k, v in zip(groups, np.arange(len(set(groups))))}\n",
    "    # df['group'] = df['group'].map(lambda x: remap_groups[x])\n",
    "    print(len(set(df['group'])))\n",
    "    return groups\n",
    "\n",
    "good_groups = filter_df_groups(df_ours)\n",
    "# df_fdloco = filter_df_groups(df_fdloco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b3765a-1d57-4990-b9d8-add0b0e2c248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.optimize import linear_sum_assignment\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# import matplotlib.pyplot as plt\n",
    "# fd_loco_groups = df_fdloco.groupby('group').mean().reset_index().sort_values('group')\n",
    "# ours_groups = df_ours.groupby('group').mean().reset_index().sort_values('group')\n",
    "# from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# fd_loco_xy = fd_loco_groups[['x', 'y']].to_numpy()\n",
    "# ours_xy = ours_groups[['x', 'y']].to_numpy()\n",
    "\n",
    "# cost_matrix = pairwise_distances(fd_loco_xy, ours_xy)\n",
    "\n",
    "# # Solve the assignment problem\n",
    "# row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "# def gen_group_mapping(df1, df2, row_ind, col_ind):\n",
    "#     return {c: r for r, c in zip(row_ind, col_ind)}\n",
    "\n",
    "# group_mapping = gen_group_mapping(fd_loco_groups, df_ours, col_ind, row_ind)\n",
    "\n",
    "\n",
    "# def remap_df(df, group_mapping):\n",
    "#     df = df.copy(deep=True)\n",
    "#     df = df[df['group'].isin(group_mapping.keys())]\n",
    "#     df['group'] = df['group'].map(lambda x: group_mapping[x])\n",
    "#     df = df[df['group']!=-1]\n",
    "#     return df\n",
    "\n",
    "# # df_fdloco_remapped = remap_df(df_fdloco, group_mapping)\n",
    "\n",
    "\n",
    "# if 'index' in df_fdloco:\n",
    "#     del df_fdloco['index']\n",
    "# print(len(set(df_fdloco['group'])))\n",
    "# # import h5py\n",
    "# # outpath = '/home/miguel/Projects/smlm_z/publication/comparisons/fd-loco/fd_deeploc_results/fov1_locs_remapped_undrift_picked_matched.hdf5'\n",
    "# # with h5py.File(outpath, \"w\") as locs_file:\n",
    "# #     locs_file.create_dataset(\"locs\", data=df_fdloco.to_records())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebfe2d1-1c0c-4204-aec9-27a039525ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# for r, c in zip(row_ind, col_ind):\n",
    "#     x0, y0 = fd_loco_xy[r]\n",
    "#     x1, y1 = ours_xy[c]\n",
    "#     plt.plot([x0, x1], [y0, y1])\n",
    "\n",
    "#     i += 1\n",
    "#     if i > 10000:\n",
    "#         break\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91849b4c-d9cf-4869-ab78-027d54480d42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fd_loco_groups = df_fdloco.groupby('group').mean().reset_index().sort_values('group')\n",
    "ours_groups = df_ours.groupby('group').mean().reset_index().sort_values('group')\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "fd_loco_xy = fd_loco_groups[['x', 'y']].to_numpy()\n",
    "ours_xy = ours_groups[['x', 'y']].to_numpy()\n",
    "fd_loco_groups = fd_loco_groups['group'].to_numpy().astype(int)\n",
    "\n",
    "df_fdloco['newgroup'] = -1\n",
    "\n",
    "dist = pairwise_distances(ours_xy, fd_loco_xy)\n",
    "# print(ours_groups['group'].max())\n",
    "# print(dist.shape)\n",
    "# print(dist[i, :].shape)\n",
    "mapping = {}\n",
    "for i, g in enumerate(fd_loco_groups):\n",
    "    # xmin, xmax = df_fdloco[df_fdloco['group']==g]['x'].min(), df_fdloco[df_fdloco['group']==g]['x'].max()\n",
    "    # ymin, ymax = df_fdloco[df_fdloco['group']==g]['y'].min(), df_fdloco[df_fdloco['group']==g]['y'].max()\n",
    "    # sns.scatterplot(data=df_fdloco[df_fdloco['group']==g], x='x', y='y')\n",
    "    # df_ours2 = df_ours.copy(deep=True)\n",
    "    # df_ours2['group'] = df_ours2['group'].astype(str)\n",
    "    # df_ours2 = df_ours2[(df_ours2['x']>xmin-5)&(df_ours2['x']<xmax+5)&(df_ours2['y']>ymin-5)&(df_ours2['y']<ymax+5)]\n",
    "    # sns.scatterplot(data=df_ours2, x='x', y='y', hue='group')\n",
    "    # plt.xlim((xmin-5, xmax+5))\n",
    "    # plt.ylim((ymin-5, ymax+5))\n",
    "    # plt.show()\n",
    "    \n",
    "    idx = np.argmin(dist[:, i])\n",
    "    target_group = int(ours_groups['group'].to_numpy()[idx])\n",
    "    print(f'{g} -> {target_group}')\n",
    "    mapping[g] = target_group\n",
    "\n",
    "df_fdloco['group'] = df_fdloco['group'].map(lambda x: mapping.get(x))\n",
    "del df_fdloco['index']\n",
    "import h5py\n",
    "outpath = '/home/miguel/Projects/smlm_z/publication/comparisons/fd-loco/fd_deeploc_results/fov1_locs_remapped_undrift_picked_matched.hdf5'\n",
    "with h5py.File(outpath, \"w\") as locs_file:\n",
    "    locs_file.create_dataset(\"locs\", data=df_fdloco.to_records())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51286210-b12f-4e8d-84d2-8438248735ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6ed6b9-8d96-4725-9ee7-81c650148bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "locs = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/comparisons/decode/emitter_remapped_undrift.hdf5', key='locs')\n",
    "\n",
    "picked_locs = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/comparisons/decode/emitter_remapped_undrift_picked.hdf5', key='locs')\n",
    "\n",
    "picked_locs = pd.DataFrame.from_records(picked_locs)\n",
    "\n",
    "locs = locs.merge(picked_locs, on=['x', 'y', 'photons', 'bg', 'lpx', 'lpy', 'net_gradient', 'iterations', 'frame', 'likelihood', 'sx', 'sy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e75ff6-d061-4887-904d-dabd75dc3f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import pairwise_distances\n",
    "decode_df = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/comparisons/decode/emitter_remapped_undrift_picked_matched.hdf5', key='locs')\n",
    "for group in set(decode_df['group']):\n",
    "    sub_df = decode_df[decode_df['group']==group]\n",
    "    xy = sub_df[['x', 'y']].to_numpy()\n",
    "    dists = pairwise_distances(xy, xy)\n",
    "    if dists.max() > 1:\n",
    "        print(group, dists.max())\n",
    "        sns.scatterplot(data=sub_df, x='x', y='y')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9173579b-76b6-44b1-ab99-e7c19471020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "decode_df = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/comparisons/decode/emitter_remapped_undrift_picked.hdf5', key='locs')\n",
    "deeploc_df = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/comparisons/fd-loco/fd_deeploc_results/fov1_locs_remapped_undrift_picked.hdf5', key='locs')\n",
    "\n",
    "# decode_df['lpx'] = 0.05\n",
    "# decode_df['lpy'] = 0.05\n",
    "# decode_df['lpz'] = 0.05\n",
    "# # decode_df['sx'] = 20\n",
    "# # decode_df['sy'] = 20\n",
    "# # decode_df['sx'] /= 10\n",
    "# decode_df['sz'] /= 10\n",
    "# decode_df['z [nm]'] /= 106\n",
    "for c in sorted(set(decode_df).union(set(deeploc_df))):\n",
    "    print(c)\n",
    "    try:\n",
    "        print('\\t', 'Decode:', decode_df[c].min(), decode_df[c].max())\n",
    "    except KeyError:\n",
    "        pass\n",
    "    try:\n",
    "        print('\\t', 'FD-Deeploc', deeploc_df[c].min(), deeploc_df[c].max())\n",
    "    except KeyError:\n",
    "        pass\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc473b0f-d2a9-482d-83b2-d2be677c4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "del decode_df['index']\n",
    "with h5py.File('/home/miguel/Projects/smlm_z/publication/comparisons/decode/emitter_remapped_undrift_picked.hdf5', 'w') as f:\n",
    "    f.create_dataset('locs', data=decode_df.to_records())\n",
    "# decode_df.to_hdf, key='locs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7df15ea-0692-4b96-b453-cd47a6311ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df2 = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/comparisons/decode/emitter_remapped_undrift.hdf5', key='locs')\n",
    "sns.scatterplot(data=our_df, x='x', y='y')\n",
    "sns.scatterplot(data=df2, x='x', y='y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f53da7a-e519-4513-9da0-536f37719b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a056e438-6f55-4786-9e9e-519c997f1c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f426c9b9-4cc6-4e34-9591-8f12fe86a168",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fdloco = df_fdloco[df_fdloco['newgroup']!=-1]\n",
    "print(set(df_fdloco['newgroup']) == set(df_ours['group']))\n",
    "print(set(df_fdloco['newgroup']))\n",
    "print(set(df_ours['group']))\n",
    "for df in (df_fdloco, df_ours):\n",
    "    c = 'group'\n",
    "    if 'newgroup' in list(df):\n",
    "        c = 'newgroup'\n",
    "    print(c, df[c].min(), df[c].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0d65cd-a181-4527-887f-7840d746842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "sns.scatterplot(data=df_ours, x='x', y='y', hue='group')\n",
    "plt.show()\n",
    "sns.scatterplot(data=df_fdloco, x='x', y='y', hue='newgroup')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900503cd-7535-43d3-b96f-93aa83d21280",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, g in enumerate(set(df_ours['group'])):\n",
    "    sns.scatterplot(data=df_ours[df_ours['group'] == g], x='x', y='y', label='ours')\n",
    "    sns.scatterplot(data=df_fdloco[df_fdloco['group'] == g], x='x', y='y', label='fd-loco')\n",
    "    plt.title(str(g))\n",
    "    plt.show()\n",
    "\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4edb4f-1ad9-4a2f-b5f8-3a2bc9dfe99a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ours = pd.read_hdf('/home/miguel/Projects/smlm_z/autofocus/VIT_zeiss_lowsnr_data/out_24_nvidia6/out_nup/nup_renders3/nup.hdf5', key='locs')\n",
    "df_fdloco = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/comparisons/fd-loco/fd_deeploc_results/fov1_locs_remapped_undrift_picked_matched.hdf5', key='locs')\n",
    "\n",
    "dirs = [\n",
    "    '/home/miguel/Projects/smlm_z/publication/comparisons/fd-loco/fd_deeploc_results/nup_renders3/good_results',\n",
    "    '/home/miguel/Projects/smlm_z/autofocus/VIT_zeiss_lowsnr_data/out_24_nvidia6/out_nup/nup_renders3/good_results'\n",
    "]\n",
    "\n",
    "def load_img(imname, d):\n",
    "    state = 'Good'\n",
    "    impath = os.path.join(d, imname)\n",
    "    if not os.path.exists(impath):\n",
    "        impath = os.path.join(d.replace('good', 'other'), imname)\n",
    "        print(impath)\n",
    "        state = 'Bad'\n",
    "        if not os.path.exists(impath):\n",
    "            return None, 'N/A'\n",
    "    return mpimg.imread(impath), state\n",
    "        \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from natsort import natsorted\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "imgs = [set(os.listdir(d)) for d in dirs]\n",
    "imgs = natsorted(imgs[0].union(imgs[1]))\n",
    "confusion_matrix = np.zeros((2, 2))\n",
    "for imname in imgs:\n",
    "    i = int(imname.replace('nup_', '').replace('_gaussian.png', ''))\n",
    "    if i not in good_groups:\n",
    "        continue\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20,6), dpi=140)\n",
    "    states = []\n",
    "    for i2, d in enumerate(dirs):\n",
    "        img, state = load_img(imname, d)\n",
    "        if img is not None:\n",
    "            axs[i2].imshow(img)\n",
    "            axs[i2].set_xlabel(state)\n",
    "        states.append(state)\n",
    "    j = 0\n",
    "    k = 0\n",
    "    if states[0] == 'Good':\n",
    "        j = 1\n",
    "    if states[1] == 'Good':\n",
    "        k = 1\n",
    "    confusion_matrix[j][k] += 1\n",
    "\n",
    "    axs[0].set_title(f'FD-Deeploc ({states[0]})')\n",
    "    axs[1].set_title(f'Ours ({states[1]})')\n",
    "    axs[0].axis('off')\n",
    "    axs[1].axis('off')\n",
    "        \n",
    "\n",
    "    sns.scatterplot(data=df_fdloco[df_fdloco['group']==i], x='x', y='y', ax=axs[2], label='fd-loco')\n",
    "    \n",
    "    sns.scatterplot(data=df_ours[df_ours['group']==i], x='x', y='y', ax=axs[2], label='ours')\n",
    "\n",
    "    plt.title(imname)\n",
    "    plt.show()\n",
    "    # break\n",
    "\n",
    "print(confusion_matrix)\n",
    "    \n",
    "\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c54030-cd8b-47aa-8d4d-6dd7ede53cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "from glob import glob\n",
    "\n",
    "dirs = [\n",
    "    '/home/miguel/Projects/smlm_z/publication/comparisons/fd-loco/fd_deeploc_results/nup_renders3/',\n",
    "    '/home/miguel/Projects/smlm_z/autofocus/VIT_zeiss_lowsnr_data/out_24_nvidia6/out_nup/nup_renders3/'\n",
    "]\n",
    "\n",
    "def img_to_result(fpath):\n",
    "    status = 'good_results' in fpath\n",
    "    i = int(os.path.basename(fpath).replace('nup_', '').replace('_gaussian.png', ''))\n",
    "    return i, status\n",
    "fd_loco_imgs = dict([img_to_result(f) for f in glob(os.path.join(dirs[0], '*', '*.png'))])\n",
    "our_imgs = dict([img_to_result(f) for f in glob(os.path.join(dirs[1], '*', '*.png'))])\n",
    "\n",
    "max_img = max([max(fd_loco_imgs.keys()), max(our_imgs.keys())])\n",
    "res = []\n",
    "for i in set(fd_loco_imgs.keys()).intersection(our_imgs.keys()):\n",
    "    if i not in good_groups:\n",
    "        continue\n",
    "    res.append({\n",
    "        'id': i,\n",
    "        'ours': our_imgs[i],\n",
    "        'fd_loc': fd_loco_imgs[i]\n",
    "    })\n",
    "df = pd.DataFrame.from_records(res)\n",
    "# df = df[df['id'].isin(good_groups)]\n",
    "from sklearn.metrics import confusion_matrix\n",
    "disp = ConfusionMatrixDisplay.from_predictions(df['ours'], df['fd_loc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f255a13a-6a09-4ea1-b65d-4c55b5bb5b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "locs_path = '/home/miguel/Projects/smlm_z/autofocus/VIT_zeiss_lowsnr_data/out_24_nvidia6/out_nup/locs_3d.hdf5'\n",
    "locs = pd.read_hdf(locs_path, key='locs')\n",
    "sns.scatterplot(data=locs.groupby('frame').mean(), x='frame', y='z [nm]')\n",
    "plt.savefig('./tmp.png')\n",
    "# del locs['index']\n",
    "# with h5py.File(locs_path, \"w\") as locs_file:\n",
    "#     locs_file.create_dataset(\"locs\", data=locs.to_records())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a99d49-6e1c-414e-bf87-149639f87100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_hd', key='locs')\n",
    "df2 = pd.read_hdf('/home/miguel/Projects/smlm_z/autofocus/VIT_zeiss_lowsnr_data/out_24_nvidia6/out_nup/locs_3d_undrift_z2.hdf5', key='locs')\n",
    "\n",
    "cols = ['x [nm]', 'y [nm]', 'x', 'y']\n",
    "df[cols] = df2[cols]\n",
    "df.to_hdf('/home/miguel/Projects/smlm_z/autofocus/VIT_zeiss_lowsnr_data/out_24_nvidia6/out_nup/locs_3d_undrift_z.hdf5', key='locs')\n",
    "f('/home/miguel/Projects/smlm_z/autofocus/VIT_zeiss_lowsnr_data/out_24_nvidia6/out_nup/locs_3d_undrift_z.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a67266-ea91-4378-946b-41dfb4149290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.path.abspath('./model.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12689cf-194d-481c-a09f-fff2f1507bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.load_model('/home/miguel/Projects/smlm_z/autofocus/VIT_zeiss_lowsnr_data/out_24_nvidia6/latest_vit_model3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be86c68b-9824-48cd-a909-4d7e7031b640",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, show_shapes=True, show_layer_activations=True, to_file='./model.png')\n",
    "import os\n",
    "print(os.path.abspath('./model.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d280f1-39dc-48f6-936c-c9ee3fc36f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.scale.get_scale_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7078c8-d91a-4221-8ae5-ebee8001ad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "d = np.load('/home/miguel/Projects/smlm_z/publication/tmp.npy')\n",
    "\n",
    "plt.imshow(d, interpolation = 'sinc', vmax = np.percentile(d.flatten(), 99.9))\n",
    "plt.title(norm)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14bbb80-6892-419a-a464-6cd1cc2c8438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=''\n",
    "from tifffile import imread\n",
    "\n",
    "stacks = imread('/home/miguel/Projects/smlm_z/publication/VIT_openframe/stacks.ome.tif')\n",
    "stack = np.concatenate(stacks[0:1, 60:120]).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bebd43-e39e-43cd-bb1c-0ffc2d408cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/miguel/Projects/smlm_z/publication/')\n",
    "from util import util\n",
    "\n",
    "stack_c = stack[:, :, :, np.newaxis]\n",
    "\n",
    "stack_norm = util._apply_img_norm((stack_c, None), None, {'norm': 'frame-min'})[0][0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a5d5d1-5730-4dc2-a948-5e27de87ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stack_norm.min(axis=(1,2,3)))\n",
    "print(stack_norm.max(axis=(1,2,3)))\n",
    "\n",
    "args = {\n",
    "    'aug_gauss': 0,\n",
    "    'aug_brightness': 0.2,\n",
    "    'aug_poisson_lam': 0,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "extra_aug = Sequential([], name='extra_aug')\n",
    "\n",
    "if args['aug_gauss']:\n",
    "    extra_aug.add(layers.GaussianNoise(stddev=args['aug_gauss'], seed=args['seed']))\n",
    "\n",
    "if args['aug_brightness']:\n",
    "    extra_aug.add(layers.RandomBrightness(args['aug_brightness'], value_range=[0, 1], seed=args['seed']))\n",
    "    \n",
    "# layers.RandomTranslation(1/imshape[0], 1/imshape[0], seed=args['seed']),\n",
    "if args['aug_poisson_lam']:\n",
    "    extra_aug.add(RandomPoissonNoise(imshape, 1, args['aug_poisson_lam'], seed=args['seed']))\n",
    "\n",
    "stack_aug = extra_aug(stack_norm).numpy()\n",
    "print(stack_aug.min(axis=(1,2,3)))\n",
    "print(stack_aug.max(axis=(1,2,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d989b82e-c761-46a5-9416-c3691441ed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_img_norm(imgs):\n",
    "    print(imgs.dtype)\n",
    "    mins = tf.math.reduce_min(imgs, axis=(1,2), keepdims=True)\n",
    "    imgs -= mins\n",
    "    maxs = tf.math.reduce_max(imgs, axis=(1,2), keepdims=True)\n",
    "    imgs = tf.nn.relu(imgs / maxs)\n",
    "    return imgs\n",
    "\n",
    "data = data.map(_apply_img_norm)\n",
    "data = data.batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7500e2-d63e-44c1-a7ec-dcaec95758fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for batch in data.as_numpy_iterator():\n",
    "    imgs = batch.squeeze()\n",
    "\n",
    "for i in range(norm_stacks.shape[1]):\n",
    "    plt.figure(figsize=(1, 1), dpi=60)\n",
    "    plt.imshow(norm_stacks[0, i])\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(1, 1), dpi=60)\n",
    "    plt.imshow(imgs[i])\n",
    "    plt.show()\n",
    "    print('---------')\n",
    "\n",
    "\n",
    "print(d.shape)\n",
    "plt.plot(d.max(axis=(1,2)))\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfe920a-0adc-430c-8889-de7c54311114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "psfs = np.random.uniform(0, 1, size=(100, 10, 15, 15))\n",
    "\n",
    "i = 0\n",
    "psf_mins = psfs[i].min(axis=(1,2), keepdims=True)\n",
    "print(psf_mins.shape)\n",
    "psfs[i] -= psf_mins\n",
    "print(psf_mins.shape)\n",
    "psf_sums = psfs[i].sum(axis=(1,2), keepdims=True)\n",
    "psfs[i] /= psf_sums\n",
    "print(psfs[i].sum(axis=(1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c489898d-9392-4a76-97c0-34609c1161ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "\n",
    "run_dir = '/home/miguel/Projects/smlm_z/publication/VIT_Zeiss_gauss_aug5/out_roll_alignment/'\n",
    "train_data = tf.data.Dataset.load(run_dir + 'train')\n",
    "\n",
    "for (imgs, xy), z in train_data.as_numpy_iterator():\n",
    "    break\n",
    "\n",
    "img = imgs[500:502]\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers, Sequential\n",
    "\n",
    "class RandomPoissonNoise(layers.Layer):\n",
    "    def __init__(self, shape, lam_min, lam_max, rescale=65336, seed=42):\n",
    "        super(RandomPoissonNoise, self).__init__()\n",
    "        tf.random.set_seed(seed)\n",
    "\n",
    "        self.shape = shape\n",
    "        self.lam_min = lam_min\n",
    "        self.lam_max = lam_max\n",
    "        self.rescale = rescale\n",
    "\n",
    "    def call(self, input, training=False):\n",
    "        if training==False:\n",
    "            return input\n",
    "        lam = tf.random.uniform((1,), self.lam_min, self.lam_max)[0]\n",
    "        noise = tf.random.poisson(self.shape, lam, dtype=tf.float32) / self.rescale\n",
    "        return input + noise\n",
    "\n",
    "\n",
    "extra_aug = Sequential([\n",
    "    layers.GaussianNoise(stddev=0.005, seed=42),\n",
    "    layers.RandomTranslation(0.1, 0.1, seed=42),\n",
    "    layers.RandomBrightness(0.4, value_range=[0, 1], seed=42),\n",
    "    RandomPoissonNoise(img.shape, 100, 10000)\n",
    "], name='extra_aug')\n",
    "\n",
    "print(img.min(), img.max())\n",
    "plt.figure(figsize=(2, 2), dpi=60)\n",
    "plt.imshow(img[0].mean(axis=-1))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def norm_zero_one(x):\n",
    "    return (x-x.min()) / (x.max()-x.min())\n",
    "\n",
    "\n",
    "for _ in range(5):\n",
    "    aug_img = extra_aug(img, training=True).numpy()[0]\n",
    "    print(aug_img.min(), aug_img.max())\n",
    "    plt.figure(figsize=(2, 2), dpi=60)\n",
    "    plt.imshow(aug_img.mean(axis=-1))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e45a8a4-c071-4573-9139-8036000c879c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "\n",
    "run_dir = '/home/miguel/Projects/smlm_z/publication/VIT_Zeiss_gauss_aug10/out_roll_alignment/'\n",
    "train_data = tf.data.Dataset.load(run_dir + 'train')\n",
    "\n",
    "\n",
    "    \n",
    "import numpy as np\n",
    "train_img_mins = []\n",
    "train_img_maxs = []\n",
    "train_img_means = []\n",
    "for (imgs, xy), z in train_data.as_numpy_iterator():\n",
    "    train_img_mins.append(imgs.min(axis=(1,2,3)))\n",
    "    train_img_maxs.append(imgs.max(axis=(1,2,3)))\n",
    "    train_img_means.append(imgs.mean(axis=(1,2,3)))\n",
    "\n",
    "train_img_mins = np.concatenate(train_img_mins)\n",
    "train_img_maxs = np.concatenate(train_img_maxs)\n",
    "train_img_means = np.concatenate(train_img_means)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cd3880-2fb4-4e48-8187-20f4f50d42a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py \n",
    "\n",
    "datagen = joblib.load(run_dir + 'datagen.gz')\n",
    "\n",
    "exp_data = '/media/Data/smlm_z_data/20231121_nup_miguel_zeiss/FOV1/storm_1/old_locs/storm_1_MMStack_Default.ome_spots.hdf5'\n",
    "with h5py.File(exp_data, 'r') as f:\n",
    "    spots = np.array(f['spots']).astype(float)\n",
    "\n",
    "BASELINE = 100\n",
    "SENSITIVITY = 1\n",
    "GAIN = 1\n",
    "\n",
    "spots = (spots * GAIN / SENSITIVITY) + BASELINE\n",
    "\n",
    "spots = datagen.standardize(spots)\n",
    "spots_mins = spots.min(axis=(1,2))\n",
    "spots_maxs = spots.max(axis=(1,2))\n",
    "spots_mean = spots.mean(axis=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81dba47-0211-4753-b70a-1cfe0332e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.boxplot((spots_mins, train_img_mins))\n",
    "plt.show()\n",
    "plt.boxplot((spots_maxs, train_img_maxs))\n",
    "plt.show()\n",
    "plt.boxplot((spots_mean, train_img_means))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a80067a-7e32-4d35-8863-b348ab8a39b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from data.visualise import grid_psfs\n",
    "from tensorflow.keras import layers, Sequential\n",
    "import numpy as np\n",
    "    \n",
    "SEED = 42\n",
    "n = 200\n",
    "for (imgs, xy), z in test_data.as_numpy_iterator():\n",
    "    imgs = imgs[:n]\n",
    "    xy = xy[:n]\n",
    "    z = z[:n]\n",
    "    plt.imshow(grid_psfs(imgs.mean(axis=-1)).T)\n",
    "    plt.show()\n",
    "    z_pred = model.predict((imgs, xy), verbose=False).squeeze()\n",
    "    mae = mean_absolute_error(z_pred, z)\n",
    "\n",
    "    plt.title(str(round(mae, 2)))\n",
    "    plt.scatter(z, z_pred, marker='x')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    aug = Sequential([\n",
    "        layers.RandomBrightness(0.5, value_range=[0, 1], seed=SEED)\n",
    "    ])\n",
    "\n",
    "    aug_imgs = aug(imgs).numpy()\n",
    "\n",
    "    plt.imshow(grid_psfs(aug_imgs.mean(axis=-1)).T)\n",
    "    plt.show()\n",
    "    z_pred = model.predict((aug_imgs, xy), verbose=False).squeeze()\n",
    "\n",
    "    idx = np.argwhere(np.sum(aug_imgs, axis=(1,2,3))!=0).squeeze()\n",
    "    z = z[idx]\n",
    "    z_pred = z_pred[idx]\n",
    "    \n",
    "    mae = mean_absolute_error(z_pred, z)\n",
    "\n",
    "    plt.title(str(round(mae, 2)))\n",
    "    plt.scatter(z, z_pred, marker='x')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438a9e07-a70f-48e7-9d15-fb9661bb2de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/VIT_openframe_newaug5/out_roll_alignment/out_nup/locs_3d.hdf5', key='locs')\n",
    "picked_locs = pd.read_hdf('/home/miguel/Projects/data/20230601_MQ_celltype/nup/fov2/storm_1/storm_1_MMStack_Default.ome_locs_undrifted_picked_4.hdf5', key='locs')\n",
    "df = df.merge(picked_locs, on=['x', 'y', 'photons', 'bg', 'lpx', 'lpy', 'net_gradient', 'iterations', 'frame', 'likelihood', 'sx', 'sy'])\n",
    "df['clusterID'] = df['group']\n",
    "for i in range(100):\n",
    "    \n",
    "    data = df[df['group']==i][['z']].to_numpy()\n",
    "    sns.histplot(data=data, bins=40, stat='density')\n",
    "\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    \n",
    "    cov_type = 'full'\n",
    "    gm = GaussianMixture(n_components=2, n_init=20, covariance_type=cov_type).fit(data)\n",
    "    bic = gm.bic(data)\n",
    "    \n",
    "    labels = gm.predict(data).squeeze()\n",
    "    \n",
    "    weights = gm.weights_\n",
    "    \n",
    "    # sns.histplot(data=gm_df, x='pred', hue='cluster_id', stat='density', alpha=0.2, bins=20)\n",
    "    \n",
    "    # create necessary things to plot\n",
    "    x_axis = np.linspace(data.min(), data.max(), 50)\n",
    "    ys = []\n",
    "    sub_df2 = pd.DataFrame.from_dict({'x': x_axis})\n",
    "    for i in range(0, gm.n_components):\n",
    "        if cov_type == 'tied':\n",
    "            cov = gm.covariances_.squeeze()\n",
    "        elif cov_type == 'full' or cov_type == None:\n",
    "            cov = gm.covariances_[i][0][0]\n",
    "        elif cov_type == 'spherical':\n",
    "            cov = gm.covariances_[i]\n",
    "        elif cov_type == 'diag':\n",
    "            cov_type = gm.covariances_[i]\n",
    "    \n",
    "        sub_df2[f'y_{i}'] = norm.pdf(x_axis, float(gm.means_[i][0]), np.sqrt(cov))*gm.weights_[i]\n",
    "        sns.lineplot(data=sub_df2, x='x', y=f'y_{i}')\n",
    "\n",
    "    diff = gm.means_.max() - gm.means_.min()\n",
    "    if 40 < diff and diff < 60:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb13f8a-bacb-4aa4-bf8c-9330928f825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention map for VIT model\n",
    "import matplotlib.pyplot as plt\n",
    "from vit_keras import visualize\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "import joblib\n",
    "\n",
    "dataset = '/home/miguel/Projects/smlm_z/publication/VIT_openframe_newaug3/out_roll_alignment/test'\n",
    "model = '/home/miguel/Projects/smlm_z/publication/VIT_openframe_newaug3/out_roll_alignment/latest_vit_model3'\n",
    "model = tf.keras.models.load_model(model)\n",
    "model = model.layers[2]\n",
    "train_data = tf.data.Dataset.load(dataset)\n",
    "\n",
    "def norm_zero_one(x):\n",
    "    return (x-x.min()) / (x.max()-x.min())\n",
    "    \n",
    "for (imgs, xy), z in train_data.as_numpy_iterator():\n",
    "    for i in range(30, 50):\n",
    "        # for i in range(imgs.shape[0]):\n",
    "        #     plt.figure(figsize=(1, 1))\n",
    "        #     plt.title(str(i))\n",
    "        #     plt.imshow(norm_zero_one(imgs[i]))\n",
    "        #     plt.show()\n",
    "        image = imgs[i]\n",
    "        print(image.min(), image.max())\n",
    "        attention_map = visualize.attention_map(model=model, image=norm_zero_one(image)*255)\n",
    "        print(attention_map.min(), attention_map.max())\n",
    "        fig, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "        ax1.axis('off')\n",
    "        ax2.axis('off')\n",
    "        ax1.set_title('Original')\n",
    "        ax2.set_title('Attention Map')\n",
    "        _ = ax1.imshow(norm_zero_one(image))\n",
    "        _ = ax2.imshow(norm_zero_one(attention_map))\n",
    "        plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1151dcc8-8765-42ca-9a9a-2e5f98512e58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compare pixel vals between training data and exp data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "import joblib\n",
    "\n",
    "dataset = '/home/miguel/Projects/smlm_z/publication/VIT_openframe_no_imagenet2/out_roll_alignment/train'\n",
    "locs = '/home/miguel/Projects/data/20230601_MQ_celltype/nup/fov2/storm_1/storm_1_MMStack_Default.ome_spots.hdf5'\n",
    "datagen = joblib.load('/home/miguel/Projects/smlm_z/publication/VIT_openframe_no_imagenet2/out_roll_alignment/datagen.gz')\n",
    "\n",
    "train_data = tf.data.Dataset.load(dataset)\n",
    "\n",
    "psfs = []\n",
    "i = 0\n",
    "for (psf, xy), z in train_data.as_numpy_iterator():\n",
    "    psfs.append(psf)\n",
    "    i += 1\n",
    "    if i == 10:\n",
    "        break\n",
    "    \n",
    "psfs = np.concatenate(psfs)\n",
    "\n",
    "with h5py.File(locs, 'r') as f:\n",
    "    nup_spots = np.array(f['spots']).astype(np.uint16)\n",
    "nup_spots = datagen.standardize(nup_spots.astype(float))\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "def snr(x):\n",
    "    return x.max() / np.median(x)\n",
    "\n",
    "for op in [np.max, np.min, np.mean, snr]:\n",
    "    df = pd.DataFrame.from_dict({'val': [op(x) for x in psfs]})\n",
    "    df['ds'] = 'beads'\n",
    "    df2 = pd.DataFrame.from_dict({'val': [op(x) for x in nup_spots]})\n",
    "    df2['ds'] = 'locs' \n",
    "    df = pd.concat((df, df2))\n",
    "    fig = plt.figure(figsize=(5, 3))\n",
    "\n",
    "    plt.title(str(op.__name__))\n",
    "    sns.boxplot(data=df, x='ds', y='val')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da61f075-ea80-46f6-a374-c98bd8f39892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661c3958-6eb0-45e2-8c57-0ab755d819ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ea121d-5172-43c4-8718-030e201a7bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(psfs.mean(), np.std(psfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1176b14b-d02e-4304-a612-50f2eeaf3706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9464811-43ba-41a7-897d-cfe9ed70110e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tifffile import imread\n",
    "import h5py\n",
    "\n",
    "\n",
    "locs = '/home/miguel/Projects/smlm_z/publication/simul/fd-loco-simul/nup_spots.hdf5'\n",
    "with h5py.File(locs, 'r') as f:\n",
    "    simul_spots = np.array(f['spots']).astype(np.uint16)\n",
    "\n",
    "locs = '/home/miguel/Projects/data/fd-loco/roi_startpos_810_790_split.ome_spots.hdf5'\n",
    "with h5py.File(locs, 'r') as f:\n",
    "    real_spots = np.array(f['spots']).astype(np.uint16)\n",
    "\n",
    "print(simul_spots.min(), simul_spots.max(), simul_spots.mean())\n",
    "print(real_spots.min(), real_spots.max(), real_spots.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cf9754-ac11-416a-8ea9-fa417f7d0d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.visualise import show_psf_axial\n",
    "keep_beads = []\n",
    "\n",
    "for i in range(len(beads)):\n",
    "    max_val = np.argmax(beads[i].max(axis=(1,2)))\n",
    "    keep_beads.append(beads[i][max_val-100:max_val+100])\n",
    "\n",
    "keep_beads = np.concatenate(keep_beads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a62f9ad-bb2e-4e42-8aaa-cffce4466f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_beads = keep_beads[:, :, :, np.newaxis].astype(float)\n",
    "spots = spots[:, :, :, np.newaxis].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763034bd-f04c-48a6-b18d-142bcc2fd41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_beads.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4313624f-72c8-49ec-a67b-9d09a2a44b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snr(x):\n",
    "    return x.max() / np.median(x)\n",
    "\n",
    "\n",
    "for op in [np.max, np.min, np.mean, snr]:\n",
    "    \n",
    "    df = pd.DataFrame.from_dict({'val': [op(x) for x in keep_beads]})\n",
    "    df['ds'] = 'beads'\n",
    "    df4 = pd.DataFrame.from_dict({'val': [op(x) for x in spots]})\n",
    "    df4['ds'] = 'locs' \n",
    "    df = pd.concat((df, df4))\n",
    "    fig = plt.figure(figsize=(5, 3))\n",
    "\n",
    "    plt.title(str(op.__name__))\n",
    "    sns.boxplot(data=df, x='ds', y='val')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8cf27a-06f0-4899-afe6-1feddd68514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x, maxval=1):\n",
    "    return ((x - x.min()) / (x.max() - x.min())) * maxval\n",
    "\n",
    "\n",
    "keep_beads = np.stack([norm(x) for x in keep_beads])\n",
    "spots = np.stack([norm(x) for x in spots])\n",
    "\n",
    "print(keep_beads.max(), spots.max())\n",
    "print(keep_beads.min(), spots.min())\n",
    "print(keep_beads.mean(), spots.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf0e49-acd7-4707-a2bd-4dcf8091bd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fake_images = np.random.uniform(0, 255, size=(4, 255, 255, 3))\n",
    "\n",
    "def norm_zero_one(imgs):\n",
    "    mins = imgs.min(axis=(1,2,3), keepdims=True)\n",
    "    maxs = imgs.max(axis=(1,2,3), keepdims=True)\n",
    "    return (imgs-mins) / (maxs-mins)\n",
    "\n",
    "norm_imgs = norm_zero_one(fake_images)\n",
    "print(norm_imgs.min(axis=(1,2,3)))\n",
    "print(norm_imgs.max(axis=(1,2,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c708d272-8aee-4ae5-acd2-c42ab969fde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, layers\n",
    "\n",
    "aug = Sequential([\n",
    "   layers.RandomBrightness([-0.2, 0.2], value_range=(0, 1)),\n",
    "   layers.GaussianNoise(0.5),\n",
    "   layers.RandomContrast(0.2)\n",
    "])\n",
    "# new_aug = aug(keep_beads.copy()).numpy()\n",
    "\n",
    "# for i in [0, 25, 50, 600]:\n",
    "#     plt.figure(figsize=(1,1))\n",
    "#     img1 = keep_beads[i]\n",
    "#     print(img1.min(), img1.max(), img1.dtype)\n",
    "#     img2 = aug(img1).numpy()\n",
    "#     print(img2.min(), img2.max(), img2.dtype)\n",
    "#     img = np.concatenate((norm(img1), norm(img2)), axis=1)\n",
    "#     plt.imshow(img)\n",
    "#     plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e73bab-3d05-4c6e-838f-88085ff8dcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, layers\n",
    "\n",
    "aug_pipeline = Sequential([\n",
    "    layers.GaussianNoise(stddev=0.001*keep_beads.max()),\n",
    "    # layers.RandomTranslation(MAX_TRANSLATION_PX/img_size, MAX_TRANSLATION_PX/img_size, seed=args['seed']),\n",
    "    layers.RandomBrightness(0.01, value_range=[0, keep_beads.max()]),\n",
    "])\n",
    "\n",
    "old_aug = aug_pipeline(keep_beads.copy()).numpy()[:, :, : np.newaxis]\n",
    "old_aug = np.concatenate((old_aug, keep_beads))\n",
    "\n",
    "# aug = Sequential([\n",
    "#    layers.RandomBrightness([-0.4, 0], value_range=(0, 1)),\n",
    "#    layers.GaussianNoise(0.5),\n",
    "#    layers.RandomContrast(0.5)\n",
    "# ])\n",
    "\n",
    "new_aug = (aug(keep_beads.copy()).numpy())[:, :, : :, np.newaxis]\n",
    "new_aug = np.concatenate((new_aug, keep_beads))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2389c76f-08ee-4d0c-8e43-72e7061def08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def snr(x):\n",
    "    return x.max() / np.median(x)\n",
    "\n",
    "\n",
    "for op in [np.max, np.min, np.mean, snr]:\n",
    "    \n",
    "    df = pd.DataFrame.from_dict({'val': [op(x) for x in keep_beads]})\n",
    "    df['ds'] = 'beads'\n",
    "    df2 = pd.DataFrame.from_dict({'val': [op(x) for x in old_aug]})\n",
    "    df2['ds'] = 'old_aug'\n",
    "    df3 = pd.DataFrame.from_dict({'val': [op(x) for x in new_aug]})\n",
    "    df3['ds'] = 'new_aug'\n",
    "    df4 = pd.DataFrame.from_dict({'val': [op(x) for x in spots]})\n",
    "    df4['ds'] = 'locs' \n",
    "    df = pd.concat((df, df2, df3, df4))\n",
    "    fig = plt.figure(figsize=(5, 3))\n",
    "\n",
    "    plt.title(str(op.__name__))\n",
    "    sns.boxplot(data=df, x='ds', y='val')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6164a638-c111-4326-b1c7-affcdc169aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0/65336.0,\n",
    "    samplewise_center=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    horizontal_flip=False)\n",
    "\n",
    "datagen.fit(keep_beads)\n",
    "std_beads = datagen.standardize(keep_beads.copy())\n",
    "std_spots = datagen.standardize(spots.copy())\n",
    "\n",
    "print(old_aug.shape, keep_beads.shape)\n",
    "old_aug_data = np.concatenate((old_aug, keep_beads))\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0/65336.0,\n",
    "    samplewise_center=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    horizontal_flip=False)\n",
    "datagen.fit(old_aug_data)\n",
    "old_aug_data_norm = datagen.standardize(old_aug_data.copy())\n",
    "old_aug_spots = datagen.standardize(spots.copy())\n",
    "\n",
    "new_aug_data = np.concatenate((new_aug, keep_beads))\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0/65336.0,\n",
    "    samplewise_center=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    horizontal_flip=False)\n",
    "datagen.fit(new_aug_data)\n",
    "new_aug_data_norm = datagen.standardize(new_aug_data.copy())\n",
    "new_aug_spots = datagen.standardize(spots.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4d89ea-99cf-4f58-86ba-18b815c702c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snr(x):\n",
    "    return x.max() / np.median(x)\n",
    "\n",
    "for op in [np.max, np.min, np.mean, snr]:\n",
    "    df = pd.DataFrame.from_dict({'val': [op(x) for x in old_aug_data_norm]})\n",
    "    df['ds'] = 'beads'\n",
    "    df2 = pd.DataFrame.from_dict({'val': [op(x) for x in old_aug_spots]})\n",
    "    df2['ds'] = 'locs' \n",
    "    df3 = pd.DataFrame.from_dict({'val': [op(x) for x in new_aug_data_norm]})\n",
    "    df3['ds'] = 'beads_2'\n",
    "    df4 = pd.DataFrame.from_dict({'val': [op(x) for x in new_aug_spots]})\n",
    "    df4['ds'] = 'locs_2' \n",
    "    df = pd.concat((df, df2, df3, df4))\n",
    "    fig = plt.figure(figsize=(5, 3))\n",
    "\n",
    "    plt.title(str(op.__name__))\n",
    "    sns.boxplot(data=df, x='ds', y='val')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40578bf2-a6be-4fe7-9cb2-1b8be730430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "snrs_beads = [max(x) for x in keep_beads]\n",
    "snrs_locs = [max(x) for x in spots]\n",
    "\n",
    "plt.hist(snrs_beads, label='beads')\n",
    "plt.hist(snrs_locs, label='locs')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "snrs_beads = [max(x) for x in keep_beads]\n",
    "snrs_locs = [max(x) for x in spots]\n",
    "\n",
    "plt.hist(snrs_beads, label='beads')\n",
    "plt.hist(snrs_locs, label='locs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da93dfe8-ce33-4020-b916-e3d31ad0ed73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=''\n",
    "import tensorflow as tf\n",
    "\n",
    "ds = tf.data.Dataset.load('/home/miguel/Projects/smlm_z/publication/VIT_Zeiss_gauss_aug/out_roll_alignment/train')\n",
    "\n",
    "imgs = []\n",
    "for (img, xy), z in ds.as_numpy_iterator():\n",
    "    imgs.append(img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce38dbd-0157-4285-9e4d-82c6ac80fd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = np.concatenate(imgs)\n",
    "print(imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2787906f-14a6-44b9-8c67-4c8be928898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spots.min(), spots.max(), spots.mean())\n",
    "print(imgs.min(), imgs.max(), imgs.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b73e71-b0c6-4b29-bada-99c6dc083ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# new_nup = pd.read_csv('/home/miguel/Projects/smlm_z/publication/VIT_openframe_newaug/out_roll_alignment/out_nup/nup_renders3/nup_report.csv')\n",
    "# old_nup = pd.read_csv('/home/miguel/Projects/smlm_z/publication/VIT_openframe/out_roll_alignment/out_nup/nup_renders3/nup_report.csv')\n",
    "\n",
    "\n",
    "new_nup = pd.read_csv('/home/miguel/Projects/smlm_z/publication/VIT_Zeiss_gauss_aug/out_roll_alignment/out_nup/nup_renders3/nup_report.csv')\n",
    "old_nup = pd.read_csv('/home/miguel/Projects/smlm_z/publication/VIT_Zeiss/out_roll_alignment/out_nup/nup_renders3/nup_report.csv')\n",
    "\n",
    "new_nup['dataset'] = 'new'\n",
    "old_nup['dataset'] = 'old'\n",
    "\n",
    "df = pd.concat((old_nup, new_nup))\n",
    "\n",
    "sns.boxplot(data=df, x='dataset', y='seperation')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "def count_valid(x):\n",
    "    return len(np.argwhere((x>40) & (x<60)))\n",
    "\n",
    "print(count_valid(new_nup['seperation']))\n",
    "print(count_valid(old_nup['seperation']))\n",
    "\n",
    "# new_locs = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/VIT_Zeiss_gauss_aug/out_roll_alignment/out_nup/locs_3d.hdf5', key='locs')\n",
    "# old_locs = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/VIT_Zeiss/out_roll_alignment/out_nup/locs_3d.hdf5', key='locs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9d3873-72d1-42e3-8d1a-292690dcadb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(new_locs['z [nm]'])\n",
    "sns.histplot(old_locs['z [nm]'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c5f3e-5b8a-4d2f-8a43-edf8a6d61bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys, os\n",
    "\n",
    "\n",
    "\n",
    "# # TODO remove this\n",
    "if not os.environ.get('CUDA_VISIBLE_DEVICES'):\n",
    "    os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "import shutil\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Resizing, Lambda\n",
    "from tensorflow.keras import Sequential\n",
    "import tensorflow as tf\n",
    "from picasso import io\n",
    "\n",
    "from data.visualise import grid_psfs\n",
    "\n",
    "\n",
    "\n",
    "N_GPUS = max(1, len(tf.config.experimental.list_physical_devices(\"GPU\")))\n",
    "\n",
    "\n",
    "VERSION = '0.1'\n",
    "\n",
    "\n",
    "\n",
    "# Picasso localisation parameters\n",
    "BASELINE = 100\n",
    "SENSITIVITY = 1\n",
    "GAIN = 1\n",
    "\n",
    "\n",
    "DEFAULT_LOCS = None\n",
    "DEFAULT_SPOTS = None\n",
    "DEFAULT_PIXEL_SIZE = None\n",
    "PICKED = None\n",
    "XLIM, YLIM = None, None\n",
    "\n",
    "# NUP FD-LOCO\n",
    "# DEFAULT_LOCS = '/home/miguel/Projects/data/fd-loco/roi_startpos_810_790_split.ome_locs.hdf5'\n",
    "# DEFAULT_SPOTS = '/home/miguel/Projects/data/fd-loco/roi_startpos_810_790_split.ome_spots.hdf5'\n",
    "# PICKED = '/home/miguel/Projects/data/fd-loco/roi_startpos_810_790_split.ome_locs_picked.hdf5'\n",
    "# DEFAULT_PIXEL_SIZE = 110\n",
    "# XLIM, YLIM = None, None\n",
    "\n",
    "\n",
    "\n",
    "# NUP OPENFRAME\n",
    "# DEFAULT_LOCS = '/home/miguel/Projects/data/20230601_MQ_celltype/nup/fov2/storm_1/storm_1_MMStack_Default.ome_locs_undrifted.hdf5'\n",
    "# DEFAULT_SPOTS = '/home/miguel/Projects/data/20230601_MQ_celltype/nup/fov2/storm_1/storm_1_MMStack_Default.ome_spots.hdf5'\n",
    "# PICKED = '/home/miguel/Projects/data/20230601_MQ_celltype/nup/fov2/storm_1/storm_1_MMStack_Default.ome_locs_undrifted_picked_4.hdf5'\n",
    "# DEFAULT_PIXEL_SIZE = 86\n",
    "# XLIM, YLIM = None, None\n",
    "\n",
    "\n",
    "\n",
    "# Zeiss\n",
    "# DEFAULT_LOCS = '/media/Data/smlm_z_data/20231121_nup_miguel_zeiss/FOV1/storm_1/storm_1_MMStack_Default.ome_locs_undrifted.hdf5'\n",
    "# DEFAULT_SPOTS = '/media/Data/smlm_z_data/20231121_nup_miguel_zeiss/FOV1/storm_1/storm_1_MMStack_Default.ome_spots.hdf5'\n",
    "# PICKED = '/media/Data/smlm_z_data/20231121_nup_miguel_zeiss/FOV1/storm_1/storm_1_MMStack_Default.ome_locs_picked.hdf5'\n",
    "# DEFAULT_PIXEL_SIZE = 106\n",
    "# XLIM, YLIM = None, None\n",
    "\n",
    "\n",
    "# Unused below\n",
    "\n",
    "# # Mitochondria (older)\n",
    "# DEFAULT_LOCS = '/home/miguel/Projects/data/20231205_miguel_mitochondria/mitochondria/FOV2/storm_1/storm_1_MMStack_Default.ome_locs_undrifted.hdf5'\n",
    "# DEFAULT_SPOTS = '/home/miguel/Projects/data/20231205_miguel_mitochondria/mitochondria/FOV2/storm_1/storm_1_MMStack_Default.ome_spots.hdf5'\n",
    "# DEFAULT_PIXEL_SIZE = 86\n",
    "# XLIM, YLIM = None, None\n",
    "\n",
    "# Mitochondria (newer) (still not clearly working)\n",
    "# DEFAULT_LOCS = '/media/Data/smlm_z_data/20231212_miguel_openframe/mitochondria/FOV2/storm_1/storm_1_MMStack_Default.ome_locs_undrift.hdf5'\n",
    "# DEFAULT_SPOTS = '/media/Data/smlm_z_data/20231212_miguel_openframe/mitochondria/FOV2/storm_1/storm_1_MMStack_Default.ome_spots.hdf5'\n",
    "# PICKED = None\n",
    "# DEFAULT_PIXEL_SIZE = 86\n",
    "# XLIM = 400, 600\n",
    "# YLIM = 700, 1000\n",
    "\n",
    "\n",
    "\n",
    "# Tubulin\n",
    "# DEFAULT_LOCS = '/media/Data/smlm_z_data/20231212_miguel_openframe/tubulin/FOV1/storm_1/storm_1_MMStack_Default.ome_locs_undrifted.hdf5'\n",
    "# DEFAULT_SPOTS = '/media/Data/smlm_z_data/20231212_miguel_openframe/tubulin/FOV1/storm_1/storm_1_MMStack_Default.ome_spots.hdf5'\n",
    "# PICKED = None\n",
    "# DEFAULT_PIXEL_SIZE = 86\n",
    "# XLIM = 200, 800\n",
    "# YLIM = 500, 1000\n",
    "\n",
    "def write_arg_log(args):\n",
    "    outfile = os.path.join(args['outdir'], 'config.json')\n",
    "    with open(outfile, 'w') as fp:\n",
    "        json_dumps_str = json.dumps(args, indent=4)\n",
    "        print(json_dumps_str, file=fp)\n",
    "\n",
    "\n",
    "def save_copy_script(outdir):\n",
    "    outpath = os.path.join(outdir, 'localise_exp_sample.py.bak')\n",
    "    shutil.copy(os.path.abspath(__file__), outpath)\n",
    "\n",
    "\n",
    "def gen_2d_plot(locs, outdir):\n",
    "    print('Gen 2d plot')\n",
    "    sns.scatterplot(data=locs, x='x', y='y', marker='.', alpha=0.1)\n",
    "    plt.axis('equal')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.savefig(os.path.join(outdir, '2d_scatterplot.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def gen_example_spots(spots, outdir):\n",
    "    print('Gen example splots')\n",
    "    plt.rcParams['figure.figsize'] = [10, 10]\n",
    "    plt.imshow(grid_psfs(spots[0:100]))\n",
    "    plt.savefig(os.path.join(outdir, 'example_spots.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def apply_normalisation(locs, spots, args):\n",
    "    print('Applying pre-processing')\n",
    "    scaler = joblib.load(args['coords_scaler'])\n",
    "    datagen = joblib.load(args['datagen'])\n",
    "\n",
    "    coords = scaler.transform(locs[['x', 'y']].to_numpy())\n",
    "    spots = datagen.standardize(spots.astype(np.float32))[:, :, :, np.newaxis]\n",
    "\n",
    "    return coords, spots\n",
    "\n",
    "\n",
    "\n",
    "def pred_z(model, spots, coords):\n",
    "\n",
    "    spots = spots.astype(np.float32)\n",
    "    print('Predicting z locs')\n",
    "\n",
    "    # exp_spots = tf.data.Dataset.from_generator(\n",
    "    #     generator=lambda: iter(spots),\n",
    "    #     output_signature=tf.TensorSpec(shape=spots.shape[1:], dtype=tf.float32)\n",
    "    # )\n",
    "    exp_spots = tf.data.Dataset.from_tensor_slices(spots)\n",
    "    exp_coords = tf.data.Dataset.from_tensor_slices(coords)\n",
    "\n",
    "    exp_X = tf.data.Dataset.zip((exp_spots, exp_coords))\n",
    "\n",
    "    fake_z = np.zeros((coords.shape[0],))\n",
    "    exp_z = tf.data.Dataset.from_tensor_slices(fake_z)\n",
    "\n",
    "    exp_data = tf.data.Dataset.zip((exp_X, exp_z))\n",
    "\n",
    "    image_size = 64\n",
    "    imshape = (image_size, image_size)\n",
    "    img_preprocessing = Sequential([\n",
    "        Resizing(*imshape),\n",
    "        Lambda(tf.image.grayscale_to_rgb)\n",
    "    ])\n",
    "\n",
    "    def apply_rescaling(x, y):\n",
    "        x = [x[0], x[1]]\n",
    "        x[0] = img_preprocessing(x[0])\n",
    "        return tuple(x), y\n",
    "\n",
    "    BATCH_SIZE = 2048\n",
    "    exp_data = exp_data.map(apply_rescaling, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE)\n",
    "\n",
    "    pred_z = model.predict(exp_data, batch_size=BATCH_SIZE, workers=4)\n",
    "\n",
    "    \n",
    "\n",
    "    # sns.histplot(pred_z)\n",
    "    # plt.show()\n",
    "    # plt.savefig(os.path.join(outdir, 'z_histplot.png'))\n",
    "    # plt.close()\n",
    "    return pred_z\n",
    "\n",
    "def write_locs(locs, z_coords, args):\n",
    "    locs['z [nm]'] = z_coords\n",
    "    locs['z'] = locs['z [nm]']\n",
    "    # locs['z'] = z_coords / args['pixel_size']\n",
    "    locs['x [nm]'] = locs['x'] * args['pixel_size']\n",
    "    locs['y [nm]'] = locs['y'] * args['pixel_size']\n",
    "\n",
    "    locs_path = os.path.join(args['outdir'], 'locs_3d.hdf5')\n",
    "    with h5py.File(locs_path, \"w\") as locs_file:\n",
    "        locs_file.create_dataset(\"locs\", data=locs.to_records())\n",
    "\n",
    "    yaml_file = args['locs'].replace('.hdf5', '.yaml')\n",
    "    if os.path.exists(yaml_file):\n",
    "        dest_yaml = locs_path.replace('.hdf5', '.yaml')\n",
    "        shutil.copy(yaml_file, dest_yaml)\n",
    "    else:\n",
    "        dest_yaml = None\n",
    "        print('Could not write yaml file (original from 2D localisation not found)')\n",
    "    print('Wrote results to:')\n",
    "    print(f'\\t- {os.path.abspath(locs_path)}')\n",
    "    if dest_yaml:\n",
    "        print(f'\\t- {os.path.abspath(dest_yaml)}')\n",
    "\n",
    "\n",
    "def write_report_data(args):\n",
    "    report_data = {\n",
    "        'code_version': VERSION\n",
    "    }\n",
    "    report_data.update(args)\n",
    "    with open(os.path.join(args['outdir'], 'report.json'), 'w') as fp:\n",
    "        json_dumps_str = json.dumps(report_data, indent=4)\n",
    "        print(json_dumps_str, file=fp)\n",
    "\n",
    "\n",
    "def extract_fov(spots, locs):\n",
    "    print(locs.shape)\n",
    "    idx = np.argwhere((XLIM[0]<locs['x']) & (XLIM[1]>locs['x']) & (YLIM[0]<locs['y']) & (YLIM[1]>locs['y'])).squeeze()\n",
    "    spots = spots[idx]\n",
    "    locs = locs.iloc[idx]\n",
    "    return spots, locs\n",
    "\n",
    "def tmp_filter_locs(new_locs, spots, args):\n",
    "    old_locs = pd.read_hdf(args['picked_locs'], key='locs')\n",
    "\n",
    "    idx = np.argwhere(new_locs['x'].isin(old_locs['x'])).squeeze()\n",
    "    new_locs = new_locs.iloc[idx]\n",
    "    spots = spots[idx]\n",
    "    return new_locs, spots\n",
    "\n",
    "\n",
    "\n",
    "args = {\n",
    "    'model': '/home/miguel/Projects/smlm_z/publication/VIT_Zeiss/out_roll_alignment/latest_vit_model3',\n",
    "    'locs': '/media/Data/smlm_z_data/20231121_nup_miguel_zeiss/FOV1/storm_1/storm_1_MMStack_Default.ome_locs_undrifted.hdf5',\n",
    "    'spots': '/media/Data/smlm_z_data/20231121_nup_miguel_zeiss/FOV1/storm_1/storm_1_MMStack_Default.ome_spots.hdf5',\n",
    "    'coords_scaler': '/home/miguel/Projects/smlm_z/publication/VIT_Zeiss/out_roll_alignment/scaler.save',\n",
    "    'datagen': '/home/miguel/Projects/smlm_z/publication/VIT_Zeiss/out_roll_alignment/datagen.gz',\n",
    "    'picked_locs': '/media/Data/smlm_z_data/20231121_nup_miguel_zeiss/FOV1/storm_1/storm_1_MMStack_Default.ome_locs_picked.hdf5'\n",
    "}\n",
    "\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with mirrored_strategy.scope():\n",
    "    model = tf.keras.models.load_model(args['model'])\n",
    "\n",
    "locs, info = io.load_locs(args['locs'])\n",
    "locs = pd.DataFrame.from_records(locs)\n",
    "\n",
    "with h5py.File(args['spots'], 'r') as f:\n",
    "    spots = np.array(f['spots']).astype(np.uint16)\n",
    "\n",
    "spots = (spots * GAIN / SENSITIVITY) + BASELINE\n",
    "\n",
    "\n",
    "\n",
    "# # TODO remove temp subset of locs\n",
    "if args['picked_locs']:\n",
    "    locs, spots = tmp_filter_locs(locs, spots, args)\n",
    "\n",
    "assert locs.shape[0] == spots.shape[0]\n",
    "print(locs.shape)\n",
    "if XLIM or YLIM:\n",
    "    spots, locs = extract_fov(spots, locs)\n",
    "\n",
    "# gen_2d_plot(locs, args['outdir'])\n",
    "# gen_example_spots(spots, args['outdir'])\n",
    "coords, spots = apply_normalisation(locs, spots, args)\n",
    "print(coords.shape)\n",
    "\n",
    "z_coords = pred_z(model, spots, coords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564fdafb-e24e-4b5a-94e5-1a8c8f5e984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "import warnings\n",
    "from tables import NaturalNameWarning\n",
    "warnings.filterwarnings('ignore', category=NaturalNameWarning)\n",
    "\n",
    "decode_df = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/comparisons/decode/emitter_remapped_undrift_picked_matched.hdf5', key='locs')\n",
    "our_df = pd.read_hdf('/home/miguel/Projects/smlm_z/autofocus/VIT_zeiss_lowsnr_data/out_24_nvidia6_bak/out_nup/locs_3d_undrift_z_merge_picked.hdf5', key='locs')\n",
    "deeploc_df = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/comparisons/fd-loco/fd_deeploc_results/fov1_locs_remapped_undrift_picked_matched.hdf5', key='locs')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c7f6b9-f4a1-46b3-b433-e7cb737c292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeploc_df['z'].max(), decode_df['z'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2b3023-5200-4aa8-b4a7-8b3c8ffe3df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeploc_df['z'] /= 106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554addfd-b733-430f-b7c1-6293c03848da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "if 'index' in list(deeploc_df):\n",
    "    del deeploc_df['index']\n",
    "locs_path = '/home/miguel/Projects/smlm_z/publication/comparisons/fd-loco/fd_deeploc_results/fov1_locs_remapped_undrift_picked_matched.hdf5'\n",
    "with h5py.File(locs_path, \"w\") as locs_file:\n",
    "    locs_file.create_dataset(\"locs\", data=deeploc_df.to_records())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4d6d82-3ffa-4742-8935-ee2b277b0f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.signal import find_peaks\n",
    "decode_df = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/comparisons/decode/emitter_remapped_undrift_picked_matched.hdf5', key='locs')\n",
    "our_df = pd.read_hdf('/home/miguel/Projects/smlm_z/autofocus/VIT_zeiss_lowsnr_data/out_24_nvidia6_bak/out_nup/locs_3d_undrift_z_merge_picked.hdf5', key='locs')\n",
    "deeploc_df = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/comparisons/fd-loco/fd_deeploc_results/fov1_locs_remapped_undrift_picked_matched.hdf5', key='locs')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def crop_z_view(locs, zrange=200):\n",
    "#     zs = locs['z [nm]']\n",
    "#     bin_width = 25\n",
    "#     hist, bins = np.histogram(zs, bins=np.arange(zs.min(), zs.max(), bin_width))\n",
    "#     try:\n",
    "#         max_bin_idx = np.argmax(hist)\n",
    "#         bin_val = bins[max_bin_idx] + (bin_width // 2)\n",
    "#     except ValueError:\n",
    "#         bin_val = np.mean(zs)\n",
    "\n",
    "#     locs = locs[(bin_val-zrange <=locs['z [nm]']) & (locs['z [nm]'] <= bin_val+zrange)]\n",
    "\n",
    "#     return locs\n",
    "\n",
    "\n",
    "# g = 419\n",
    "# df = df[df['group']==g]\n",
    "\n",
    "# df = crop_z_view(df)\n",
    "# print(df['z [nm]'].sum())\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# sns.histplot(data=df, x='z [nm]', bins=40, stat='density', ax=ax)\n",
    "\n",
    "# kde = gaussian_kde(df['z [nm]'].to_numpy())\n",
    "# kde.set_bandwidth(bw_method='silverman')\n",
    "# kde.set_bandwidth(kde.factor * 0.25)\n",
    "\n",
    "# zvals = np.linspace(df['z [nm]'].min()-25, df['z [nm]'].max()+25, 5000)\n",
    "# score = kde(zvals)\n",
    "# zvals = zvals.squeeze()\n",
    "\n",
    "# ax.plot(zvals, score, label='KDE')\n",
    "\n",
    "# peak_idx, _ = find_peaks(score, prominence=0.0005)\n",
    "# peak_z = zvals[peak_idx]\n",
    "# peak_score = score[peak_idx]\n",
    "\n",
    "# peak_scores_sorted = np.argsort(peak_score)\n",
    "# print(peak_z[peak_scores_sorted])\n",
    "# plt.scatter(peak_z[peak_scores_sorted], kde(peak_z[peak_scores_sorted]), marker='x')\n",
    "\n",
    "# if len(peak_scores_sorted) > 2:\n",
    "#     peak_scores_sorted = peak_scores_sorted[-2:]\n",
    "\n",
    "# z_peaks = peak_z[peak_scores_sorted]\n",
    "# print(z_peaks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce50792-a0d2-4c7a-96c9-6b0d911667e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_coords2 = pred_z(model, spots, np.zeros(coords.shape))\n",
    "plt.scatter(z_coords, z_coords2)\n",
    "plt.show()\n",
    "print(np.abs(z_coords-z_coords2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d232cc4-06f2-4237-9b91-d0f25c395d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "im1 = grid_psfs(spots[0:100].mean(axis=-1))\n",
    "im2 = im1+np.random.normal(0, 0.05, size=im1.shape)\n",
    "plt.imshow(im1)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(im2)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(abs(im2-im1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a12ff15-10d1-4b66-a3ca-d1a5068ef7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "import os\n",
    "from glob import glob\n",
    "from natsort import natsorted\n",
    "import subprocess\n",
    "import h5py\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from tifffile import imread, imwrite\n",
    "from skimage.feature import match_template\n",
    "from skimage.filters import butterworth\n",
    "from skimage.filters import gaussian\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.special import erf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import json\n",
    "import shutil\n",
    "from data.visualise import grid_psfs\n",
    "import seaborn as sns\n",
    "\n",
    "def norm_zero_one(s):\n",
    "    max_s = s.max()\n",
    "    min_s = s.min()\n",
    "    return (s - min_s) / (max_s - min_s)\n",
    "\n",
    "def validate_args(args):\n",
    "    args['bead_stacks'] = [b for b in args['bead_stacks'] if 'ignored' not in b]\n",
    "    n_stacks = len(args['bead_stacks'])\n",
    "    print(f\"Found {n_stacks} bead stacks\")\n",
    "    if n_stacks == 0:\n",
    "        quit(1)\n",
    "    for f in natsorted(args['bead_stacks']):\n",
    "        print(f'\\t - {f}')\n",
    "\n",
    "\n",
    "def test_picasso_exec():\n",
    "    res = subprocess.run(['picasso', '-h'], capture_output=True, text=True)\n",
    "    if res.returncode != 0:\n",
    "        print(res.stdout)\n",
    "        print(res.stderr)\n",
    "        print('\\n')\n",
    "        raise EnvironmentError('Picasso not found/working (see above)')\n",
    "\n",
    "def transform_args(args):\n",
    "    fnames = glob(f\"{args['bead_stacks']}/**/*.tif\", recursive=True)\n",
    "\n",
    "    args['outpath'] = args['bead_stacks']\n",
    "    fnames = [os.path.abspath(f) for f in fnames if '_slice.ome.tif' not in f and os.path.basename(f) != 'stacks.ome.tif']\n",
    "    args['bead_stacks'] = fnames\n",
    "\n",
    "    args['gaussian_blur'] = list(map(int, args['gaussian_blur'].split(',')))\n",
    "    return args\n",
    "\n",
    "def get_or_create_slice(bead_stack, slice_path):\n",
    "    if not os.path.exists(slice_path):\n",
    "        im_slice = bead_stack[bead_stack.shape[0]//2]\n",
    "        # plt.imshow(im_slice)\n",
    "        # plt.show()\n",
    "        imwrite(slice_path, im_slice.astype(np.uint16))\n",
    "    return slice_path\n",
    "\n",
    "def get_or_create_locs(slice_path, args):\n",
    "    spots_path = slice_path.replace('.ome.tif', '.ome_spots.hdf5')\n",
    "    locs_path = slice_path.replace('.ome.tif', '.ome_locs.hdf5')\n",
    "\n",
    "    if not os.path.exists(spots_path) or not os.path.exists(locs_path) or args['regen']:\n",
    "        cmd = ['picasso', 'localize', slice_path, '-b', args['box_size_length'], '-g', args['gradient'], '-px', args['pixel_size']]\n",
    "        print(f'Running {\" \".join(list(map(str, cmd)))}')\n",
    "        for extra_arg in ['qe', 'sensitivity', 'gain', 'baseline', 'fit-method']:\n",
    "            if extra_arg in args and args[extra_arg]:\n",
    "                cmd.extend([f'-{extra_arg}', args[extra_arg]])\n",
    "        cmd = ' '.join(list(map(str, cmd)))\n",
    "        tqdm.write('Running picasso...', end='')\n",
    "        res = subprocess.run(cmd, capture_output=True, shell=True, text=True)\n",
    "        if res.returncode != 0:\n",
    "            print('Picasso error occured')\n",
    "            print(res.stdout)\n",
    "            print(res.stderr)\n",
    "            return\n",
    "        tqdm.write('finished!')\n",
    "\n",
    "    with h5py.File(spots_path) as f:\n",
    "        spots = np.array(f['spots'])\n",
    "\n",
    "    locs = pd.read_hdf(locs_path, key='locs')\n",
    "    locs['fname'] = '___'.join(slice_path.split('/')[-3:])\n",
    "    print(f'Found {locs.shape[0]} beads')\n",
    "    return locs, spots\n",
    "\n",
    "def remove_colocal_beads(locs, spots, args):\n",
    "    tqdm.write('Removing overlapping beads...')\n",
    "    coords = locs[['x', 'y']].to_numpy()\n",
    "    dists = euclidean_distances(coords, coords)\n",
    "    np.fill_diagonal(dists, np.inf)\n",
    "    min_dists = dists.min(axis=1)\n",
    "    \n",
    "    error_margin = 0.8\n",
    "    min_seperation = (np.sqrt(2)  * args['box_size_length']) * error_margin\n",
    "    idx = np.argwhere(min_dists > min_seperation).squeeze()\n",
    "    locs = locs.iloc[idx]\n",
    "    spots = spots[idx]\n",
    "\n",
    "    return locs, spots\n",
    "\n",
    "\n",
    "def extract_training_stacks(spots, bead_stack, args) -> np.array:\n",
    "    spot_size = args['box_size_length']\n",
    "    frame_idx = bead_stack.shape[0]//2\n",
    "    frame = bead_stack[frame_idx]\n",
    "    stacks = []\n",
    "    for spot in spots:\n",
    "        res = match_template(frame, spot)\n",
    "        i, j = np.unravel_index(np.argmax(res), res.shape)\n",
    "        stack = bead_stack[:, i:i+spot_size, j:j+spot_size]\n",
    "        stacks.append(stack)\n",
    "    return np.array(stacks)\n",
    "\n",
    "def snr(psf):\n",
    "    return psf.max() / np.median(psf)\n",
    "\n",
    "\n",
    "def has_fwhm(psf, args):\n",
    "    psf = butterworth(psf, cutoff_frequency_ratio=0.2, high_pass=False)\n",
    "    y = np.max(gaussian(psf), axis=(1,2))\n",
    "    max_val = np.max(y)\n",
    "    min_val = np.min(y)\n",
    "    half_max = min_val + ((max_val-min_val) / 2)\n",
    "    crossCount = np.sum((y[:-1]>half_max) != (y[1:]>half_max))\n",
    "    # if args['debug'] and crossCount < 2:\n",
    "    #     plt.plot(y, label='raw')\n",
    "    #     plt.plot([0, len(y)], [half_max, half_max])\n",
    "    #     plt.show()\n",
    "    # if not (crossCount >= 2):\n",
    "    #     fig = plt.figure(layout=\"constrained\", figsize=(20, 15), dpi=64)\n",
    "    #     gs = plt.GridSpec(1, 2, figure=fig)\n",
    "    #     ax1 = fig.add_subplot(gs[0, 0])\n",
    "    #     ax2 = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "    #     ax1.imshow(grid_psfs(psf, cols=20))\n",
    "    #     ax2.plot(psf)\n",
    "    #     plt.show()\n",
    "    return crossCount >= 2\n",
    "\n",
    "\n",
    "def filter_mse_zprofile(psf, args, i):\n",
    "    z_step = args['zstep']\n",
    "\n",
    "    # Define the skewed Gaussian function\n",
    "    def skewed_gaussian(x, A, x0, sigma, alpha, offset):\n",
    "        \"\"\"\n",
    "        A: Amplitude\n",
    "        x0: Center\n",
    "        sigma: Standard Deviation\n",
    "        alpha: Skewness parameter\n",
    "        offset: Vertical offset\n",
    "        \"\"\"\n",
    "        return A * np.exp(-(x - x0)**2 / (2 * sigma**2)) * (1 + erf(alpha * (x - x0))) + offset\n",
    "\n",
    "\n",
    "    # Fit the skewed Gaussian to the data\n",
    "    x_data = np.arange(psf.shape[0]) * z_step\n",
    "    y_data = psf.max(axis=(1,2))\n",
    "    y_data = norm_zero_one(y_data)\n",
    "    initial_guess = [1, psf.shape[0] * z_step / 2, psf.shape[0] * z_step/4, 0.0, np.median(y_data)]\n",
    "\n",
    "    bounds = [\n",
    "        (0.6, 1.2),\n",
    "        (psf.shape[0] * z_step/8, psf.shape[0] * z_step),\n",
    "        (psf.shape[0] * z_step/20, psf.shape[0] * z_step/4),\n",
    "        (-np.inf, np.inf),\n",
    "        (y_data.min(), y_data.max())\n",
    "    ]\n",
    "    try:\n",
    "        params, _ = curve_fit(skewed_gaussian, x_data, y_data, p0=initial_guess, bounds=list(zip(*bounds)))\n",
    "    except RuntimeError:\n",
    "        print('Failed to find Z fit')\n",
    "        params = initial_guess\n",
    "\n",
    "    y_fit = skewed_gaussian(x_data, *params)\n",
    "\n",
    "    mse = (y_fit - y_data) ** 2\n",
    "    avg_mse = np.mean(mse)\n",
    "    max_mse = np.max(mse)\n",
    "\n",
    "    permitted_avg_mse = 0.02\n",
    "    permitted_max_mse = 0.1\n",
    "    # if (avg_mse < permitted_avg_mse and max_mse < permitted_max_mse):\n",
    "    #     fig = plt.figure(layout=\"constrained\", figsize=(10, 8), dpi=64)\n",
    "    #     gs = plt.GridSpec(1, 2, figure=fig)\n",
    "    #     ax1 = fig.add_subplot(gs[0, 0])\n",
    "    #     ax2 = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "    #     ax1.imshow(grid_psfs(psf, cols=20))\n",
    "    #     print(avg_mse, max_mse)\n",
    "    #     ax2.plot(x_data, y_data)\n",
    "    #     ax2.plot(x_data, y_fit)\n",
    "    #     plt.show()\n",
    "    return avg_mse < permitted_avg_mse and max_mse < permitted_max_mse\n",
    "        \n",
    "\n",
    "def get_sharpness(array):\n",
    "    gy, gx = np.gradient(array)\n",
    "    gnorm = np.sqrt(gx**2 + gy**2)\n",
    "    sharpness = np.average(gnorm)\n",
    "    return sharpness\n",
    "\n",
    "\n",
    "def reduce_img(psf):\n",
    "    return np.stack([get_sharpness(x) for x in psf])\n",
    "\n",
    "\n",
    "def est_bead_offsets(psfs, locs, args):\n",
    "    UPSCALE_RATIO = 10\n",
    "\n",
    "    def denoise(img):\n",
    "        \n",
    "        sigmas = np.array(args['gaussian_blur'])\n",
    "        return gaussian_filter(img.copy(), sigma=sigmas)\n",
    "\n",
    "    def find_peak(psf):\n",
    "        if psf.ndim == 4:\n",
    "            psf = psf.mean(axis=-1)\n",
    "        x = np.arange(psf.shape[0]) * args['zstep']\n",
    "        psf = denoise(psf)\n",
    "        \n",
    "        inten = norm_zero_one(reduce_img(psf))\n",
    "\n",
    "        cs = UnivariateSpline(x, inten, k=3, s=0.2)\n",
    "\n",
    "        x_ups = np.linspace(0, psf.shape[0], len(x) * UPSCALE_RATIO) * args['zstep']\n",
    "\n",
    "        peak_xups = x_ups[np.argmax(cs(x_ups))] \n",
    "\n",
    "        return peak_xups\n",
    "    offsets = np.array(map(find_peak, psfs))\n",
    "\n",
    "    locs['offset'] = offsets\n",
    "\n",
    "\n",
    "def filter_mse_xy(stack, max_mse, i):\n",
    "    # Define a 2D Gaussian function\n",
    "    def gaussian_2d(xy, amplitude, xo, yo, sigma_x, sigma_y, theta, offset):\n",
    "        x, y = xy\n",
    "        xo = float(xo)\n",
    "        yo = float(yo)\n",
    "        a = (np.cos(theta)**2) / (2 * sigma_x**2) + (np.sin(theta)**2) / (2 * sigma_y**2)\n",
    "        b = -(np.sin(2 * theta)) / (4 * sigma_x**2) + (np.sin(2 * theta)) / (4 * sigma_y**2)\n",
    "        c = (np.sin(theta)**2) / (2 * sigma_x**2) + (np.cos(theta)**2) / (2 * sigma_y**2)\n",
    "        g = offset + amplitude * np.exp(- (a * ((x - xo)**2) + 2 * b * (x - xo) * (y - yo) + c * ((y - yo)**2)))\n",
    "        return g.ravel()\n",
    "\n",
    "    sharp = reduce_img(stack)\n",
    "    idx = np.argmax(sharp)\n",
    "    image = stack[idx]\n",
    "\n",
    "\n",
    "    # Load and preprocess the image (e.g., convert to grayscale)\n",
    "    # For simplicity, let's generate a simple image for demonstration\n",
    "    image_size = image.shape[1]\n",
    "    x = np.linspace(0, image_size - 1, image_size)\n",
    "    y = np.linspace(0, image_size - 1, image_size)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    \n",
    "    image = image / image.max()\n",
    "\n",
    "    # Fit the Gaussian to the image data\n",
    "    p0 = [1, image_size / 2, image_size / 2, 2, 2, 0, 0]  # Initial guess for parameters\n",
    "    bounds = [\n",
    "        (0, np.inf),\n",
    "        (image_size * (1/5), image_size * (4/5)),\n",
    "        (image_size * (1/5), image_size * (4/5)),\n",
    "        (0, image_size/3),\n",
    "        (0, image_size/3),\n",
    "        (-np.inf, np.inf),\n",
    "        (0, np.inf),\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        popt, pcov = curve_fit(gaussian_2d, (x, y), image.ravel(), p0=p0, bounds=list(zip(*bounds)))\n",
    "    except RuntimeError:\n",
    "        print('XY fit failed')\n",
    "        popt = p0\n",
    "    render = gaussian_2d((x, y), *popt).reshape(image.shape)\n",
    "\n",
    "    error = mean_squared_error(render, image)\n",
    "\n",
    "    # if error > max_mse:\n",
    "    #     print(i)\n",
    "    #     # Visualize the original image and the fitted Gaussian\n",
    "    #     plt.plot(sharp)\n",
    "    #     plt.show()\n",
    "    #     plt.figure(figsize=(5, 3))\n",
    "    #     plt.subplot(1, 2, 1)\n",
    "    #     plt.imshow(image)\n",
    "    #     plt.title('Original Image')\n",
    "        \n",
    "    #     plt.subplot(1, 2, 2)\n",
    "    #     plt.imshow(render)\n",
    "    #     plt.title('Fitted Gaussian')\n",
    "        \n",
    "    #     plt.tight_layout()\n",
    "    #     plt.show()\n",
    "    #     print(error, max_mse, error <= max_mse)\n",
    "\n",
    "    return error <= max_mse\n",
    "\n",
    "\n",
    "def filter_beads(spots, locs, stacks, args, rejected_outpath):\n",
    "    print('Removing poorly imaged beads...', end='')\n",
    "    # Filter by SNR threshold\n",
    "\n",
    "    for i in range(stacks.shape[0]):\n",
    "        psf = stacks[i]\n",
    "        plt.title(str(i))\n",
    "        plt.imshow(grid_psfs(psf))\n",
    "        plt.show()\n",
    "    mse_xy = np.array([filter_mse_xy(psf, 10000, i) for i, psf in enumerate(stacks)])\n",
    "    snrs = np.array([snr(psf) > args['min_snr'] for psf in stacks])\n",
    "    fwhms = np.array([has_fwhm(psf, args) for psf in stacks])\n",
    "    mse_z = np.array([filter_mse_zprofile(psf, args, i) for i, psf in enumerate(stacks)])\n",
    "    # TODO re-enable\n",
    "    # mse_xy[:] = True\n",
    "    \n",
    "    # snrs[:] = True\n",
    "    # mse_filters[:] = True\n",
    "\n",
    "    for i in range(stacks.shape[0]):\n",
    "        psf = stacks[i]\n",
    "        print(filter_mse_xy(psf, 10000, i))\n",
    "        print(snr(psf) > args['min_snr'])\n",
    "        print(has_fwhm(psf, args))\n",
    "        print(filter_mse_zprofile(psf, args, i))\n",
    "        plt.imshow(grid_psfs(psf))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    idx = np.argwhere(snrs & fwhms & mse_z & mse_xy).squeeze()\n",
    "    reasons = [''] * len(snrs)\n",
    "    for i in range(spots.shape[0]):\n",
    "        if not fwhms[i]:\n",
    "            reasons[i] += 'fwhm'\n",
    "        if not snrs[i]:\n",
    "            reasons[i] += f',snrs({round(snr(stacks[i]), 3)})' \n",
    "        if not mse_z[i]:\n",
    "            reasons[i] += ',mse_z'\n",
    "        if not mse_xy[i]:\n",
    "            reasons[i] += ',mse_xy'\n",
    "\n",
    "    locs['rejected'] = reasons\n",
    "\n",
    "\n",
    "    est_bead_offsets(stacks, locs, args)\n",
    "\n",
    "    if args['debug']:\n",
    "        rejected_idx = np.argwhere(np.invert(snrs & fwhms & mse_z & mse_xy))[:, 0]\n",
    "        print('\\n', 'Rejected: ', rejected_idx)\n",
    "\n",
    "        if len(rejected_idx):\n",
    "            print('Writing rejected figures...')\n",
    "\n",
    "            write_stack_figures(stacks[rejected_idx], locs.iloc[rejected_idx], rejected_outpath)\n",
    "\n",
    "    spots = spots[idx]\n",
    "    locs = locs.iloc[idx]\n",
    "    stacks = stacks[idx]\n",
    "\n",
    "    print('finished!')\n",
    "\n",
    "    return spots, locs, stacks\n",
    "\n",
    "\n",
    "def write_combined_data(stacks, locs, args):\n",
    "\n",
    "    outpath = os.path.join(args['outpath'], 'combined')\n",
    "    os.makedirs(outpath, exist_ok=True)\n",
    "\n",
    "    locs_outpath = os.path.join(outpath, 'locs.hdf')\n",
    "    stacks_outpath = os.path.join(outpath, 'stacks.ome.tif')\n",
    "\n",
    "    imwrite(stacks_outpath, stacks)\n",
    "    locs.to_hdf(locs_outpath, key='locs')\n",
    "\n",
    "    stacks_config = {\n",
    "        'zstep': args['zstep'],\n",
    "        'gen_args': args\n",
    "    }\n",
    "    \n",
    "    stacks_config_outpath = os.path.join(outpath, 'stacks_config.json')\n",
    "    with open(stacks_config_outpath, 'w') as fp:\n",
    "        json_dumps_str = json.dumps(stacks_config, indent=4)\n",
    "        print(json_dumps_str, file=fp)\n",
    "\n",
    "    figpath = os.path.join(outpath, 'offsets.png')\n",
    "    sns.scatterplot(data=locs, x='x', y='y', hue='offset')\n",
    "    plt.savefig(figpath)\n",
    "    plt.close()\n",
    "\n",
    "    print('Saved results to:')\n",
    "    print(f'\\t{locs_outpath}')\n",
    "    print(f'\\t{stacks_outpath}')\n",
    "    print(f'\\t{stacks_config_outpath}')\n",
    "    print(f'Total beads: {locs.shape[0]}')\n",
    "\n",
    "\n",
    "def write_stack_figure(i, stacks, locs, outpath, fname):\n",
    "    stack = stacks[i]\n",
    "    loc = locs.iloc[i].to_dict()\n",
    "\n",
    "    fig = plt.figure(layout=\"constrained\", figsize=(20, 15), dpi=64)\n",
    "    gs = plt.GridSpec(2, 3, figure=fig)\n",
    "    ax1 = fig.add_subplot(gs[0:, 0])\n",
    "    ax2 = fig.add_subplot(gs[0, 1:])\n",
    "    ax3 = fig.add_subplot(gs[1, 1:])\n",
    "\n",
    "\n",
    "    fig.suptitle(f'Bead: {i}')\n",
    "\n",
    "    ax1.imshow(grid_psfs(stack, cols=20))\n",
    "    ax1.set_title('Ordered by frame')\n",
    "\n",
    "    intensity = stack.max(axis=(1,2))\n",
    "    min_val = min(intensity)\n",
    "    max_val = max(intensity)\n",
    "    frame_zpos = (np.arange(len(intensity)) * args['zstep']) - loc['offset']\n",
    "    ax2.plot(frame_zpos, intensity)\n",
    "    ax2.vlines(0, min_val, max_val, colors='orange')\n",
    "    ax2.set_title('Max normalised pixel intensity over z')\n",
    "    ax2.set_xlabel('z (nm)')\n",
    "    ax2.set_ylabel('pixel intensity')    \n",
    "\n",
    "\n",
    "    for k, v in loc.items():\n",
    "        if isinstance(v, float):\n",
    "            loc[k] = round(v, 5)\n",
    "    text = json.dumps(loc, indent=4)\n",
    "    ax3.axis((0, 10, 0, 10))\n",
    "    ax3.text(0,0, text, fontsize=18, wrap=True)\n",
    "    outfpath = os.path.join(outpath, f'{fname}_bead_{i}.png')\n",
    "    plt.savefig(outfpath)\n",
    "    plt.close()\n",
    "    print(f'Wrote {outfpath}')\n",
    "\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from itertools import repeat\n",
    "\n",
    "\n",
    "def write_stack_figures(stacks, locs, outpath):\n",
    "    fname = set(locs['fname']).pop().replace('.ome.tif', '')\n",
    "    os.makedirs(outpath, exist_ok=True)\n",
    "\n",
    "    idx = np.arange(stacks.shape[0])\n",
    "    with Pool(8) as pool:\n",
    "        res = pool.starmap(write_stack_figure, zip(idx, repeat(stacks), repeat(locs), repeat(outpath), repeat(fname)))\n",
    "\n",
    "# def filter_by_tmp_locs(locs, spots):\n",
    "#     original_locs = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/original_locs.hdf', key='locs')\n",
    "#     x_coords = set(original_locs['x'])\n",
    "#     idx = np.argwhere([x in x_coords for x in locs['x']]).squeeze()\n",
    "#     locs = locs.iloc[idx]\n",
    "#     spots = spots[idx]\n",
    "#     print(locs.shape)\n",
    "#     return locs, spots\n",
    "\n",
    "\n",
    "\n",
    "stackss = None\n",
    "def main(args):\n",
    "    all_stacks = []\n",
    "    all_spots = []\n",
    "    all_locs = []\n",
    "\n",
    "    found_beads = 0\n",
    "    retained_beads = 0\n",
    "\n",
    "    rejected_outpath = os.path.join(args['outpath'], 'combined', 'rejected')\n",
    "    shutil.rmtree(rejected_outpath, ignore_errors=True)\n",
    "\n",
    "    if args['debug']:\n",
    "        os.makedirs(rejected_outpath, exist_ok=True)\n",
    "\n",
    "    for bead_stack_path in tqdm(natsorted(args['bead_stacks'])):\n",
    "        if 'stack__2_' not in bead_stack_path:\n",
    "            continue\n",
    "        tqdm.write(f'Preparing {os.path.basename(bead_stack_path)}')\n",
    "\n",
    "        bead_stack = imread(bead_stack_path)\n",
    "        slice_path = bead_stack_path.replace('.ome', '_slice.ome')\n",
    "        slice_path = get_or_create_slice(bead_stack, slice_path)\n",
    "\n",
    "        raw_locs, spots = get_or_create_locs(slice_path, args)\n",
    "        \n",
    "        # raw_locs, spots = filter_by_tmp_locs(raw_locs, spots)\n",
    "        found_beads += raw_locs.shape[0]\n",
    "        locs, spots = remove_colocal_beads(raw_locs, spots, args)\n",
    "        perc_removed = round(100*(1-(locs.shape[0]/raw_locs.shape[0])), 2)\n",
    "        print(f'Removed {perc_removed}% due to co-location')\n",
    "\n",
    "        stacks = extract_training_stacks(spots, bead_stack, args)\n",
    "        stackss = stacks\n",
    "        raise EnvironmentError\n",
    "        spots, locs, stacks = filter_beads(spots, locs, stacks, args, rejected_outpath)\n",
    "        retained_beads += locs.shape[0]\n",
    "        tqdm.write(f'Retained {stacks.shape[0]} beads')\n",
    "        all_stacks.append(stacks)\n",
    "        all_spots.append(spots)\n",
    "        all_locs.append(locs)\n",
    "\n",
    "    print(f'Found {found_beads} total beads')\n",
    "    min_stack_length = min(list(map(lambda s: s.shape[1], all_stacks)))\n",
    "    stacks = [s[:, :min_stack_length] for s in all_stacks]\n",
    "    locs = pd.concat(all_locs)\n",
    "    stacks = np.concatenate(stacks)\n",
    "\n",
    "    # original_locs = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/original_locs.hdf', key='locs')\n",
    "    # x_coords = set(original_locs['x'])\n",
    "    # print(len(set(locs['x'])), len(set(original_locs['x'])))\n",
    "    # print(len(set(locs['x']).intersection(set(original_locs['x']))))\n",
    "    # locs = pd.concat((locs, locs))\n",
    "    # stacks = np.concatenate((stacks, stacks))\n",
    "\n",
    "    print(locs.shape)\n",
    "    print(stacks.shape)\n",
    "    print(f'Kept {locs.shape[0]} total beads')\n",
    "\n",
    "    write_combined_data(stacks, locs, args)\n",
    "\n",
    "    if args['debug']:\n",
    "        outpath = os.path.join(args['outpath'], 'combined', 'debug')\n",
    "        write_stack_figures(stacks, locs, outpath)\n",
    "        \n",
    "\n",
    "def parse_args():\n",
    "    parser = ArgumentParser(description='')\n",
    "    parser.add_argument('bead_stacks', help='Path to TIFF bead stacks / directory containing bead stacks.')\n",
    "    parser.add_argument('-z', '--zstep', help='Pixel size (nm)', default=10, type=int)\n",
    "    parser.add_argument('-px', '--pixel_size', help='Pixel size (nm)', default=86, type=int)\n",
    "    parser.add_argument('-g', '--gradient', help='Min. net gradient', default=1000, type=int)\n",
    "    parser.add_argument('-b', '--box-size-length', help='Box size', default=15, type=int)\n",
    "    parser.add_argument('-qe', '--qe', help='Quantum efficiency', type=float)\n",
    "    parser.add_argument('-s', '--sensitivity', help='Sensitivity', type=float)\n",
    "    parser.add_argument('-ga', '--gain', help='Gain', type=float)\n",
    "    parser.add_argument('-bl', '--baseline', help='Baseline', type=int)\n",
    "    parser.add_argument('-a', '--fit-method', help='Fit method', choices=['mle', 'lq', 'avg'])\n",
    "    parser.add_argument('--regen', action='store_true')\n",
    "    parser.add_argument('-snr', '--min-snr', type=float, default=2.0)\n",
    "    parser.add_argument('-gb', '--gaussian-blur', default='3,2,2', help='Gaussian pixel-blur in Z/Y/X for bead offset estimation')\n",
    "    parser.add_argument('--debug', action='store_true')\n",
    "\n",
    "    args = vars(parser.parse_args())\n",
    "    return args\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaf7af9-ea82-47c9-80c1-bf2c44cc8036",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'zstep': 10,\n",
    "    'pixel_size': 86,\n",
    "    'bead_stacks': '/media/Data/smlm_z_data/20231121_nup_miguel_zeiss/stacks/',\n",
    "    'gaussian_blur': '3,2,2',\n",
    "    'debug': False,\n",
    "    'regen': False,\n",
    "    'box_size_length': 15\n",
    "}\n",
    "\n",
    "args = transform_args(args)\n",
    "validate_args(args)\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707d337f-8d07-49ea-8546-5999f6985fd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bead_stack_path = '/media/Data/smlm_z_data/20231121_nup_miguel_zeiss/stacks/stack__2/stack__2_MMStack_Default.ome.tif'\n",
    "\n",
    "bead_stack = imread(bead_stack_path)\n",
    "slice_path = bead_stack_path.replace('.ome', '_slice.ome')\n",
    "slice_path = get_or_create_slice(bead_stack, slice_path)\n",
    "\n",
    "raw_locs, spots = get_or_create_locs(slice_path, args)\n",
    "\n",
    "# raw_locs, spots = filter_by_tmp_locs(raw_locs, spots)\n",
    "locs, spots = remove_colocal_beads(raw_locs, spots, args)\n",
    "perc_removed = round(100*(1-(locs.shape[0]/raw_locs.shape[0])), 2)\n",
    "print(f'Removed {perc_removed}% due to co-location')\n",
    "\n",
    "stacks = extract_training_stacks(spots, bead_stack, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f90d53-fc5f-4c53-b5c1-1243383aea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse_xy = np.array([filter_mse_xy(psf, 10000, i) for i, psf in enumerate(stacks)])\n",
    "snrs = np.array([snr(psf) > args['min_snr'] for psf in stacks])\n",
    "fwhms = np.array([has_fwhm(psf, args) for psf in stacks])\n",
    "mse_z = np.array([filter_mse_zprofile(psf, args, i) for i, psf in enumerate(stacks)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ade36c-2e5b-420a-bca6-b9d02c800593",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(stacks.shape[0]):\n",
    "    plt.title(str(i))\n",
    "    plt.imshow(grid_psfs(stacks[i]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b057b10-a32d-4f06-95ca-87ff18e9447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "df = pd.read_csv('~/Projects/smlm_z/publication/comparisons/inspr/reconstruction.csv')\n",
    "df['x'] = df['x [nm]'] / 86\n",
    "df['y'] = df['y [nm]'] / 86\n",
    "df['z'] = df['z [nm]'] / 86\n",
    "\n",
    "df['photons'] = 1000\n",
    "df['sx'] = 1\n",
    "df['sy'] = 1\n",
    "df['bg'] = 0\n",
    "df['lpx'] = 0.1\n",
    "df['lpy'] = 0.1\n",
    "df['frame'] = df['Frame-ID']\n",
    "\n",
    "locs_path = '/home/miguel/Projects/smlm_z/publication/comparisons/inspr/reconstruction.hdf5'\n",
    "with h5py.File(locs_path, \"w\") as locs_file:\n",
    "    locs_file.create_dataset(\"locs\", data=df.to_records())\n",
    "        \n",
    "print(list(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df53868-53c6-4bc5-97f9-361ac6da8172",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['x [nm]'], df['y [nm]'], marker='.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4aa7ea-849a-483c-820a-862215c617ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(df['z [nm]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52dd868-035b-4c73-a216-13a6b4784e36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mat73\n",
    "import scipy\n",
    "import pandas as pd\n",
    "data_dict = mat73.loadmat('/home/miguel/Projects/smlm_z/publication/comparisons/spline/nup_11_sml.mat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8877a6-a3dc-42ef-9cc1-17dec96abfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['saveloc']['file']['info']['cam_pixelsize_um']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e61c5a4-37d2-4873-83f6-55c9d972387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "df = pd.DataFrame.from_records(data_dict['saveloc']['loc'])\n",
    "\n",
    "df2 = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/VIT_openframe/out_roll_alignment/out_nup/locs_3d.hdf5', key='locs')\n",
    "\n",
    "PIXEL_SIZE = 86\n",
    "\n",
    "map_cols = [\n",
    "    ('xnm', 'x [nm]'),\n",
    "    ('ynm', 'y [nm]'),\n",
    "    ('znm', 'z [nm]'),\n",
    "    ('xpix', 'x'),\n",
    "    ('ypix', 'y'),\n",
    "    ('PSFxnm', 'sx'),\n",
    "    ('PSFynm', 'sy'),\n",
    "    ('phot', 'photons'),\n",
    "    ('locprecnm', 'lpx'),\n",
    "    ('locprecnm', 'lpy'),\n",
    "    ('znm', 'z'),\n",
    "    ('zerr', 'lpz'),\n",
    "]\n",
    "\n",
    "for src, trgt in map_cols:\n",
    "    df[trgt] = df[src].copy()\n",
    "\n",
    "src_cols = list(set([x[0] for x in map_cols]))\n",
    "for col in src_cols:\n",
    "    del df[col]\n",
    "\n",
    "rescale_cols = ['lpx', 'lpy', 'sx', 'sy', 'lpz']\n",
    "for col in rescale_cols:\n",
    "    df[col] = df[col] / PIXEL_SIZE\n",
    "\n",
    "df = df[[c for c in list(df) if c in list(df2)] + ['lpz']]\n",
    "df['frame'] = df['frame'].astype(int)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(list(df))\n",
    "import h5py\n",
    "\n",
    "locs_path = '/home/miguel/Projects/smlm_z/publication/comparisons/spline/locs_3d.hdf5'\n",
    "with h5py.File(locs_path, \"w\") as locs_file:\n",
    "    locs_file.create_dataset(\"locs\", data=df.to_records())\n",
    "\n",
    "# shutil.copy('/home/miguel/Projects/smlm_z/publication/VIT_openframe/out_roll_alignment/out_nup/locs_3d.yaml', locs_path.replace('hdf5', 'yaml'))\n",
    "# frame=frame\n",
    "# xnm=x\n",
    "# ynm=y\n",
    "# phot=photons\n",
    "# PSFxnm=sx\n",
    "# PSFynm=sy\n",
    "# bg=bg\n",
    "# locprecnm=lpx\n",
    "# znm=z\n",
    "# cam_pixelsize_um=100\n",
    "# factor=100    1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930af224-6301-41be-af0e-79e950d5ea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import euclidean_distances\n",
    "df_new = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/comparisons/spline/locs_3d_undrift.hdf5', key='locs')\n",
    "df_orig = pd.read_hdf('/home/miguel/Projects/data/20230601_MQ_celltype/nup/fov2/storm_1/storm_1_MMStack_Default.ome_locs_undrifted_picked_4.hdf5', key='locs')\n",
    "print(df_new.shape)\n",
    "sns.scatterplot(data=df_new, x='x', y='y', alpha=0.01)\n",
    "sns.scatterplot(data=df_orig, x='x', y='y', alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35235484-ca04-438c-816d-1a13959c1759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import tensorflow as tf\n",
    "\n",
    "def _euclidean_distances(coords1, coords2, reduce_min=None):\n",
    "    \"\"\"\n",
    "    Compute the Euclidean distances between two sets of 2D coordinates.\n",
    "\n",
    "    Args:\n",
    "        coords1 (tf.Tensor): A tensor of shape (N, 2) containing N 2D coordinates.\n",
    "        coords2 (tf.Tensor): A tensor of shape (M, 2) containing M 2D coordinates.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: A tensor of shape (N, M) containing the Euclidean distances\n",
    "                   between each pair of coordinates from coords1 and coords2.\n",
    "    \"\"\"\n",
    "    # Ensure inputs are TensorFlow tensors\n",
    "    coords1 = tf.convert_to_tensor(coords1, dtype=tf.float32)\n",
    "    coords2 = tf.convert_to_tensor(coords2, dtype=tf.float32)\n",
    "\n",
    "    t1 = tf.reshape(coords1, (1, *coords1.shape))\n",
    "    t2 = tf.reshape(coords2, (coords2.shape[0],1,coords2.shape[1]))\n",
    "    result = tf.norm(t1-t2, ord='euclidean', axis=2,)\n",
    "    if reduce_min is not None:\n",
    "        result = tf.math.reduce_min(result, axis=reduce_min, keepdims=False)\n",
    "    res = result.numpy()\n",
    "    del result\n",
    "    return res\n",
    "    \n",
    "def batched_euclidean_distance(coords1, coords2, batch_size, reduce_min=None):\n",
    "    min_dists = np.zeros((coords1.shape[0],))\n",
    "    from tqdm import trange\n",
    "    for i in trange(0, coords1.shape[0]-1, batch_size):\n",
    "        start, end = i, i+batch_size\n",
    "        _coords1 = coords1[start:end]\n",
    "        _min_dists = _euclidean_distances(_coords1, coords2, reduce_min)\n",
    "        min_dists[start:end] = _min_dists\n",
    "    return min_dists\n",
    "\n",
    "xy_new = df_new[['x', 'y']].to_numpy()\n",
    "xy_orig = df_orig[['x', 'y']].to_numpy()\n",
    "\n",
    "# # Adjust batch size to fit in GPU memory\n",
    "BATCH_SIZE = 2**14\n",
    "\n",
    "# print(xy_new.shape)\n",
    "# print(BATCH_SIZE)\n",
    "\n",
    "min_dists = batched_euclidean_distance(xy_new, xy_orig, BATCH_SIZE, 0)\n",
    "\n",
    "print(min_dists.min(), min_dists.mean(), min_dists.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55402cac-a8d4-4697-9a73-50c661b575ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import euclidean_distances\n",
    "import matplotlib.pyplot as plt\n",
    "cutoff = (120/2) / 86\n",
    "sub_df = df_new[min_dists<cutoff]\n",
    "\n",
    "sub_df['lpz'] /= 10\n",
    "# sns.scatterplot(data=df_orig, x='x', y='y', alpha=0.01)\n",
    "# plt.show()\n",
    "# sns.scatterplot(data=sub_df, x='x', y='y', alpha=0.01)\n",
    "# plt.show()\n",
    "\n",
    "nearest_neighbour = np.zeros((sub_df.shape[0]), dtype=object)\n",
    "xy_new = sub_df[['x', 'y']].to_numpy()\n",
    "\n",
    "df_orig_groups = df_orig.groupby('group').mean()\n",
    "\n",
    "df_orig_groups.reset_index(inplace=True, drop=False)\n",
    "\n",
    "group_centers = df_orig_groups[['x', 'y']].to_numpy()\n",
    "\n",
    "print(xy_new.shape, group_centers.shape)\n",
    "dists = euclidean_distances(xy_new, group_centers)\n",
    "\n",
    "\n",
    "dists_idx = np.argmin(dists, axis=1)\n",
    "sub_df['clusterID'] = df_orig_groups['group'][dists_idx].to_numpy().astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b047e8-1d29-4349-9a49-217c7fab6db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "if 'index' in list(sub_df):\n",
    "    del sub_df['index']\n",
    "sub_df['group'] = sub_df['clusterID']\n",
    "locs_path = '/home/miguel/Projects/smlm_z/publication/comparisons/spline/locs_3d_undrift.hdf5'\n",
    "with h5py.File(locs_path, \"w\") as locs_file:\n",
    "    locs_file.create_dataset(\"locs\", data=sub_df.to_records())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b400c3f5-ff9f-4d4e-a765-18552cdb8638",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/miguel/Projects/smlm_z/publication/spline_comparison/nup_renders3/*/nup_88_gaussian.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694f3d5f-4fb5-4612-97a0-886ff4ab1ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "from natsort import natsorted\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "dir1 = '/home/miguel/Projects/smlm_z/publication/comparisons/spline/nup_renders3/*/'\n",
    "dir2 = '/home/miguel/Projects/smlm_z/publication/VIT_openframe/out*/out_nup/nup_renders3/*/'\n",
    "\n",
    "imgs = [set(list(map(os.path.basename, glob.glob(f'{dirname}/*.png')))) for dirname in (dir1, dir2)]\n",
    "imnames = imgs[0].intersection(imgs[1])\n",
    "print(len(imnames))\n",
    "\n",
    "for imname in natsorted(imnames):\n",
    "    impath = glob.glob(dir1+imname)[0]\n",
    "    img1 = mpimg.imread(impath)\n",
    "\n",
    "    impath2 = glob.glob(dir2+imname)[0]\n",
    "    if 'good' not in impath2:\n",
    "        continue\n",
    "    print(imname)\n",
    "\n",
    "    img2 = mpimg.imread(impath2)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(30, 15))\n",
    "    # Display the first image\n",
    "    ax1.imshow(img1)\n",
    "    ax1.set_title('MLE')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Display the second image\n",
    "    ax2.imshow(img2)\n",
    "    ax2.set_title('Vision transformer')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    # Adjust the spacing between subplots\n",
    "    plt.subplots_adjust(wspace=0)\n",
    "    \n",
    "    # Display the figure\n",
    "    plt.show()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203d2c0-24a0-41bb-a6e5-aeed0e24b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in sorted(set(sub_df['group'])):\n",
    "    tmp_df = sub_df[sub_df['group']==c]\n",
    "    print(tmp_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fbd2ec-7d0c-4f0c-b0f4-1de3c3b3ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "new_df = '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333cef1e-4a87-4762-b4f6-25e3e81539cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from picasso import io\n",
    "from picasso.render import render\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.signal import find_peaks\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "import matplotlib.font_manager as fm\n",
    "fontprops = fm.FontProperties(size=18)\n",
    "\n",
    "min_sigma = 0\n",
    "max_sigma = 3\n",
    "\n",
    "z_min = 400\n",
    "z_max = 600\n",
    "min_log_likelihood = -100\n",
    "# min_kde = np.log(0.007)\n",
    "min_kde = 0.05\n",
    "\n",
    "cmap_min_z = -600\n",
    "cmap_max_z = -300\n",
    "BLUR = 'gaussian'\n",
    "color_by_depth = False\n",
    "\n",
    "MIN_BLUR=0.001\n",
    "\n",
    "records = []\n",
    "\n",
    "def filter_locs(l):\n",
    "    n_points = l.shape[0]\n",
    "    print(f'From {n_points} points')\n",
    "\n",
    "    l = l[(min_sigma < l['sx']) & (l['sx'] < max_sigma)]\n",
    "    l = l[(min_sigma < l['sy']) & (l['sy'] < max_sigma)]\n",
    "    # print(f'{n_points-l.shape[0]} removed by sx/sy')\n",
    "\n",
    "\n",
    "    X = l[['z']]\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(X)\n",
    "    l['kde'] = kde.score_samples(X)\n",
    "\n",
    "    # l = l[l['z [nm]'] > z_min]\n",
    "    # l = l[l['z [nm]'] < z_max]\n",
    "    # sns.scatterplot(data=l, x='z', y='kde')\n",
    "    # plt.show()\n",
    "    \n",
    "    l = l[np.power(10, l['kde']) > min_kde]\n",
    "    # print(f'{n_points-l.shape[0]} removed by kde')\n",
    "\n",
    "    # l = l[l['likelihood']>min_log_likelihood]\n",
    "    \n",
    "    n_points2 = l.shape[0]\n",
    "    # print(f'Removed {n_points-n_points2} pts')\n",
    "    # print(f'{n_points2} remaining')\n",
    "    print(f'N points: {n_points2}')\n",
    "\n",
    "    return l\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [18, 6]\n",
    "\n",
    "\n",
    "def apply_cmap_img(img, cmap_min_coord, cmap_max_coord, img_min_coord, img_max_coord, cmap='gist_rainbow', brightness_factor = 20):\n",
    "    img = img.squeeze()\n",
    "    \n",
    "    cmap_zrange = cmap_max_coord - cmap_min_coord\n",
    "    \n",
    "    def map_z_to_cbar(z_val):\n",
    "        return (z_val - cmap_min_coord) / cmap_zrange\n",
    "        \n",
    "    min_coord_color = map_z_to_cbar(img_min_coord)\n",
    "    max_coord_color = map_z_to_cbar(img_max_coord)\n",
    "    \n",
    "    cmap = plt.get_cmap('gist_rainbow')\n",
    "    \n",
    "    gradient = np.repeat(np.linspace(min_coord_color, max_coord_color, img.shape[1])[np.newaxis, :], img.shape[0], 0)\n",
    "    \n",
    "    base = cmap(gradient)\n",
    "    img = img[:, :, np.newaxis]\n",
    "    cmap_img = img * base\n",
    "    # cmap_img /= 2\n",
    "    # Black background\n",
    "    cmap_img = (cmap_img / cmap_img.max()) * 255\n",
    "    cmap_img *= brightness_factor\n",
    "\n",
    "    cmap_img[:, :, 3] = 255 \n",
    "    \n",
    "    cmap_img = cmap_img.astype(int)\n",
    "\n",
    "    return cmap_img\n",
    "    \n",
    "def color_histplot(barplot, cmap_min_z, cmap_max_z):\n",
    "    from matplotlib.colors import rgb2hex\n",
    "    cmap = plt.get_cmap('gist_rainbow')\n",
    "    \n",
    "    bar_centres = [bar._x0 + bar._width/2 for bar in barplot.patches]\n",
    "    bar_centres = np.array(list(map(lambda x: (x-cmap_min_z) / (cmap_max_z-cmap_min_z), bar_centres)))\n",
    "    rgb_colors = cmap(bar_centres)\n",
    "    hex_colors = [rgb2hex(x) for x in rgb_colors]\n",
    "    \n",
    "    for bar, hex_color in zip(barplot.patches, hex_colors):\n",
    "        bar.set_facecolor(hex_color)\n",
    "        \n",
    "\n",
    "def center_view(locs, zrange=200):\n",
    "    zs = locs['z [nm]']\n",
    "    bin_width = 25\n",
    "    hist, bins = np.histogram(zs, bins=np.arange(zs.min(), zs.max(), bin_width))\n",
    "    try:\n",
    "        max_bin_idx = np.argmax(hist)\n",
    "        bin_val = bins[max_bin_idx] + (bin_width // 2)\n",
    "    except ValueError:\n",
    "        bin_val = np.mean(zs)\n",
    "\n",
    "    locs = locs[(bin_val-zrange <=locs['z [nm]']) & (locs['z [nm]'] <= bin_val+zrange)]\n",
    "\n",
    "    return locs\n",
    "\n",
    "def get_viewport(locs, axes, margin=1):\n",
    "    mins = np.array([locs[ax].min()-margin for ax in axes])\n",
    "    maxs = np.array([locs[ax].max()+margin for ax in axes])\n",
    "    # mins[:] = min(mins)\n",
    "    # maxs[:] = max(maxs)\n",
    "    return np.array([mins, maxs])\n",
    "\n",
    "def disable_axis_ticks():\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "def get_extent(viewport, pixel_size):\n",
    "    mins, maxs = viewport\n",
    "    return np.array([mins[1], maxs[1], mins[0], maxs[0]]) * pixel_size\n",
    "\n",
    "\n",
    "def render_locs(locs, args, ang_xyz=(0,0,0), barsize=None, ax=None):\n",
    "    \n",
    "    locs = locs.copy()\n",
    "    locs['lpz'] = np.mean(locs[['lpx', 'lpy']].to_numpy()) / 2\n",
    "    locs['sz'] = np.mean(locs[['sx', 'sy']].to_numpy()) / 3\n",
    "    # locs['lpx'] = 0.1\n",
    "    # locs['sx'] = 0.1\n",
    "    # locs['lpy'] = 0.1\n",
    "    # locs['sy'] = 0.1\n",
    "    disable_axis_ticks()\n",
    "    locs['x [nm]'] -= locs['x [nm]'].mean()\n",
    "    locs['y [nm]'] -= locs['y [nm]'].mean()\n",
    "    locs['z [nm]'] -= locs['z [nm]'].mean()\n",
    "    locs['x'] -= locs['x'].mean()\n",
    "    locs['y'] -= locs['y'].mean()\n",
    "    locs['z'] -= locs['z'].mean()\n",
    "\n",
    "    viewport = get_viewport(locs, ('y', 'x'))\n",
    "\n",
    "    _, img = render(locs.to_records(), blur_method=args['blur_method'], viewport=viewport, min_blur_width=args['min_blur'], ang=ang_xyz, oversampling=args['oversample'])\n",
    "    if ang_xyz == (0, 0, 0):\n",
    "        plt.xlabel('x [nm]')\n",
    "        plt.ylabel('y [nm]')\n",
    "    elif ang_xyz == (np.pi/2, 0, 0):\n",
    "        plt.xlabel('z [nm]')\n",
    "        plt.ylabel('x [nm]')\n",
    "        img = img.T\n",
    "        viewport = np.fliplr(viewport)\n",
    "\n",
    "    elif ang_xyz == (0, np.pi/2, 0):\n",
    "        plt.xlabel('z [nm]')\n",
    "        plt.ylabel('y [nm]')\n",
    "    else:\n",
    "        print('Axis labels uncertain due to rotation angle')\n",
    "\n",
    "    extent = get_extent(viewport, args['pixel_size'])\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    img_plot = plt.imshow(img, extent=extent)\n",
    "    plt.colorbar(img_plot)\n",
    "\n",
    "    if barsize is not None:\n",
    "        scalebar = AnchoredSizeBar(ax.transData,\n",
    "                            barsize, f'{barsize} nm', 'lower center', \n",
    "                            pad=0.1,\n",
    "                            color='white',\n",
    "                            frameon=False,\n",
    "                            size_vertical=1,\n",
    "                            fontproperties=fontprops)\n",
    "        ax.add_artist(scalebar)\n",
    "\n",
    "def write_nup_plots(locs, args, good_dir, other_dir):\n",
    "    for cid in set(locs['clusterID']):\n",
    "        # if not cid in [1, 6, 18, 19, 21, 22]:\n",
    "        if not cid in [6]:\n",
    "            continue\n",
    "        print('Cluster ID', cid)\n",
    "\n",
    "        df = locs[locs['clusterID']==cid]\n",
    "        df = filter_locs(df)\n",
    "\n",
    "        if df.shape[0] == 0:\n",
    "            continue\n",
    "        df = center_view(df)\n",
    "\n",
    "        try:\n",
    "            del df['index']\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        if df.shape[0] < 5:\n",
    "            print('No remaining localisations, continuing...')\n",
    "            continue\n",
    "\n",
    "        fig = plt.figure()\n",
    "        gs = fig.add_gridspec(1, 4)\n",
    "        plt.subplots_adjust(wspace=0.3, hspace=0)\n",
    "        \n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        render_locs(df, args, (0,0,0), barsize=110, ax=ax1)\n",
    "\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        render_locs(df, args, (np.pi/2,0,0), barsize=50, ax=ax2)\n",
    "\n",
    "        ax3 = fig.add_subplot(gs[0, 2])\n",
    "        render_locs(df, args, (0, np.pi/2,0), barsize=50, ax=ax3)\n",
    "\n",
    "        ax4 = fig.add_subplot(gs[0, 3])\n",
    "        \n",
    "        histplot = sns.histplot(data=df, x='z [nm]', ax=ax4, stat='density', legend=False)\n",
    "        if color_by_depth:\n",
    "            color_histplot(histplot, cmap_min_z, cmap_max_z)\n",
    "        sns.kdeplot(data=df, x='z [nm]', ax=ax4, bw_adjust=0.5, color='black', bw_method='silverman')\n",
    "\n",
    "        x = ax4.lines[0].get_xdata()\n",
    "        y = ax4.lines[0].get_ydata()\n",
    "        peaks, _ = find_peaks(y)\n",
    "\n",
    "        sorted_peaks = sorted(peaks, key=lambda peak_index: y[peak_index], reverse=True)\n",
    "        peak_vals = y[peaks]\n",
    "        if len(peak_vals) == 1:\n",
    "            n_peaks = 1\n",
    "        else:\n",
    "            n_peaks = 2\n",
    "            \n",
    "        sorted_peaks = sorted_peaks[:n_peaks]\n",
    "\n",
    "        peak_x = x[sorted_peaks]\n",
    "        peak_y = y[sorted_peaks]\n",
    "        for x, y in zip(peak_x, peak_y):\n",
    "            ax4.vlines(x, 0, y, label=str(round(x)), color='black')\n",
    "\n",
    "        sep = abs(max(peak_x) - min(peak_x))\n",
    "\n",
    "        septxt = 'Sep: '+ str(round(sep))+ 'nm'\n",
    "\n",
    "        records.append({\n",
    "            'id': cid,\n",
    "            'seperation': sep,\n",
    "        })\n",
    "\n",
    "        margin=10\n",
    "        if 50-margin <= sep and sep <= 50+margin:\n",
    "            cluster_outdir = good_dir\n",
    "        else:\n",
    "            cluster_outdir = other_dir\n",
    "        plt.suptitle(f'Nup ID: {cid}, N points: {df.shape[0]}, {septxt}')\n",
    "        plt.show()\n",
    "\n",
    "def load_and_filter_locs(args):\n",
    "    locs, info = io.load_locs(args['locs'])\n",
    "    locs = pd.DataFrame.from_records(locs)\n",
    "    try:\n",
    "        assert info[1]['Pixelsize'] == args['pixel_size']\n",
    "    except AssertionError:\n",
    "        print('Pixel size mismatch', info[1]['Pixelsize'],  args['pixel_size'])\n",
    "        quit(1)\n",
    "\n",
    "    if args['picked_locs']:\n",
    "        picked_locs, old_info = io.load_locs(args['picked_locs'])\n",
    "        picked_locs = pd.DataFrame.from_records(picked_locs)\n",
    "        locs = locs.merge(picked_locs, on=['x', 'y', 'photons', 'bg', 'lpx', 'lpy', 'net_gradient', 'iterations', 'frame', 'likelihood', 'sx', 'sy'])\n",
    "    locs['clusterID'] = locs['group']\n",
    "    locs['z'] = locs['z [nm]'] / args['pixel_size']\n",
    "    return locs\n",
    "\n",
    "locs = '/home/miguel/Projects/smlm_z/publication/VIT_openframe/out_roll_alignment/out_nup/locs_3d.hdf5'\n",
    "picked_locs = '/home/miguel/Projects/data/20230601_MQ_celltype/nup/fov2/storm_1/storm_1_MMStack_Default.ome_locs_undrifted_picked_4.hdf5'\n",
    "\n",
    "args = {\n",
    "    'locs': locs,\n",
    "    'picked_locs': picked_locs,\n",
    "    'pixel_size': 86,\n",
    "    'blur_method': 'gaussian',\n",
    "    'min_blur': 0.001,\n",
    "    'oversample': 20,\n",
    "}\n",
    "\n",
    "locs = load_and_filter_locs(args)\n",
    "\n",
    "write_nup_plots(locs, args, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445bae89-e921-4cb2-95fc-c808bce62617",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "\n",
    "for cid in [6, 18, 19, 21, 22]:\n",
    "    cid_locs = locs[locs['clusterID']==cid]\n",
    "    df2 = filter_locs(cid_locs).copy()\n",
    "    del df2['index']\n",
    "    \n",
    "    kde = gaussian_kde(df2['z [nm]'].to_numpy())\n",
    "    kde.set_bandwidth(bw_method='silverman')\n",
    "    kde.set_bandwidth(kde.factor * 0.75)\n",
    "    \n",
    "    zvals = np.linspace(df2['z [nm]'].min()-25, df2['z [nm]'].max()+25, 50)\n",
    "    \n",
    "    score = kde(zvals)\n",
    "    zvals = zvals.squeeze()\n",
    "    \n",
    "    peaks, _ = find_peaks(score)\n",
    "    \n",
    "    sorted_peaks = sorted(peaks, key=lambda peak_index: zvals[peak_index], reverse=False)\n",
    "    peak_vals = zvals[peaks]\n",
    "    \n",
    "    if len(peak_vals) == 1:\n",
    "        n_peaks = 1\n",
    "    else:\n",
    "        n_peaks = 2\n",
    "        \n",
    "    sorted_peaks = sorted_peaks[-2:]\n",
    "    z_peaks = zvals[sorted_peaks]\n",
    "\n",
    "    seperation = np.diff(z_peaks)[0]\n",
    "\n",
    "    z_between_peaks = np.linspace(min(z_peaks), max(z_peaks), 50)\n",
    "    scores = kde(z_between_peaks)\n",
    "    \n",
    "    density_cutoff = min(scores) * 1.05\n",
    "    print('Cutoff', density_cutoff)\n",
    "    \n",
    "    df2['density'] = kde(df2['z [nm]'].to_numpy())\n",
    "    \n",
    "    df3 = df2[df2['density']>=density_cutoff]\n",
    "    \n",
    "    \n",
    "    for _df, title in zip([df2, df3], ['raw', 'w/ min density']):\n",
    "        fig = plt.figure()\n",
    "        plt.axis('off')\n",
    "        plt.subplots_adjust(wspace=0.3, hspace=0)\n",
    "        plt.title(title)\n",
    "        gs = fig.add_gridspec(1, 3)\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        render_locs(_df, args, (np.pi/2,0,0), barsize=50, ax=ax1)\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        render_locs(_df, args, (0,np.pi/2,0), barsize=50, ax=ax2)\n",
    "\n",
    "        ax3 = fig.add_subplot(gs[0,2])\n",
    "        histplot = sns.histplot(data=_df, x='z [nm]', stat='density', legend=False)\n",
    "        if title == 'raw':\n",
    "            # Plot KDE\n",
    "            ax3.plot(zvals, score)\n",
    "            # PLOT cutoff line\n",
    "            x = [_df['z [nm]'].min(), _df['z [nm]'].max()]\n",
    "            y = [density_cutoff, density_cutoff]\n",
    "            ax3.plot(x, y, 'r--')\n",
    "\n",
    "            # Plot peaks\n",
    "            for peak in z_peaks:\n",
    "                x = [peak, peak]\n",
    "                y = [0, kde(peak).squeeze()]\n",
    "                ax3.plot(x, y, 'r--')\n",
    "                ax3.set_title(f'Sep: {round(seperation, 2)}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75fa74f-1366-4823-81a6-46a570d865f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaborn._statistics import KDE\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [3, 3]\n",
    "\n",
    "locs = locs[locs['clusterID']==6]\n",
    "df2 = filter_locs(locs).copy()\n",
    "del df2['index']\n",
    "fig = plt.figure()\n",
    "ax2 = plt.gca()\n",
    "histplot = sns.histplot(data=df2, x='z [nm]', ax=ax2, stat='density', legend=False)\n",
    "sns.kdeplot(data=df2, x='z [nm]', ax=ax2, bw_adjust=0.5, color='black', bw_method='silverman')\n",
    "\n",
    "kde = KDE(bw_method='silverman', bw_adjust=0.5).fit(df2[['z [nm]']])\n",
    "\n",
    "zvals = np.linspace(df2['z [nm]'].min()-25, df2['z [nm]'].max()+25, 50).reshape(-1, 1)\n",
    "\n",
    "score = kde.score_samples(zvals)\n",
    "score = np.exp(score)\n",
    "print(score)\n",
    "zvals = zvals.squeeze()\n",
    "plt.plot(zvals, score, c='red')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# peaks, _ = find_peaks(y)\n",
    "\n",
    "# sorted_peaks = sorted(peaks, key=lambda peak_index: y[peak_index], reverse=True)\n",
    "# peak_vals = y[peaks]\n",
    "# if len(peak_vals) == 1:\n",
    "#     n_peaks = 1\n",
    "# else:\n",
    "#     n_peaks = 2\n",
    "    \n",
    "# sorted_peaks = sorted_peaks[:n_peaks]\n",
    "\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = plt.gca()\n",
    "# render_locs(df2, args, (np.pi/2,0,0), barsize=50, ax=ax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ee4268-4701-48ac-abc4-506d7e02e282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14584ede-a5cb-4fe5-bcda-32d9bddae89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import euclidean_distances\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def norm_zero_one(s):\n",
    "    min_val = s.min()\n",
    "    max_val = s.max()\n",
    "    return (s - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.metrics import mean_squared_error\n",
    "from tqdm import tqdm, trange\n",
    "from tifffile import imread\n",
    "\n",
    "UPSCALE_RATIO = 1\n",
    "\n",
    "def norm_sum_imgs(psf):\n",
    "    psf_sums = psf.sum(axis=(1,2))\n",
    "    psf = psf / psf_sums[:, np.newaxis, np.newaxis]\n",
    "    return psf\n",
    "\n",
    "\n",
    "def tf_eval_roll(ref_psf, psf, roll):\n",
    "    return tf.reduce_mean(mean_squared_error(ref_psf, tf.roll(psf, roll, axis=0)))\n",
    "    \n",
    "def tf_find_optimal_roll(ref_tf, img, upscale_ratio=UPSCALE_RATIO):\n",
    "    \n",
    "    img_tf = tf.convert_to_tensor(img)\n",
    "\n",
    "    roll_range = ref_tf.shape[0]//4\n",
    "    rolls = np.arange(-roll_range, roll_range).astype(int)\n",
    "    errors = tf.map_fn(lambda roll: tf_eval_roll(ref_tf, img_tf, roll), rolls, dtype=tf.float64)\n",
    "    # idx = 0\n",
    "    # for roll in tqdm(rolls):\n",
    "    #     error = tf.eval_roll(ref_tf, img_tf, roll)\n",
    "    #     print(i, error)\n",
    "    #     errors[idx] = error\n",
    "    #     idx += 1\n",
    "    best_roll = rolls[tf.argmin(errors).numpy()]\n",
    "    # Prefer small backwards roll to large forwards roll\n",
    "    if abs(best_roll - img.shape[0]) < best_roll:\n",
    "        best_roll = best_roll - img.shape[0]\n",
    "    return best_roll/upscale_ratio\n",
    "\n",
    "\n",
    "\n",
    "def realign_beads(psfs, df, z_step, i):\n",
    "    from sklearn.metrics import euclidean_distances\n",
    "    df['dist'] = euclidean_distances(df[['x', 'y']].to_numpy(), [[df['x'].max()/2, df['y'].max()/2]])\n",
    "    ref_idx = np.argsort(df['dist'].to_numpy())[i]\n",
    "    print(ref_idx)\n",
    "    ref_offset = df.iloc[ref_idx]['offset']\n",
    "\n",
    "    ref_psf = norm_zero_one(psfs[ref_idx])\n",
    "    ref_tf = tf.convert_to_tensor(ref_psf)\n",
    "    rolls = []\n",
    "    for idx in trange(df.shape[0]):\n",
    "        if idx == ref_idx:\n",
    "            roll = 0\n",
    "        else:\n",
    "            psf2 = norm_zero_one(psfs[idx])\n",
    "            roll = -tf_find_optimal_roll(ref_tf, psf2)\n",
    "        print(roll)\n",
    "        rolls.append(roll)\n",
    "    rolls = np.array(rolls)\n",
    "    return rolls * z_step\n",
    "\n",
    "\n",
    "psfs = imread('/home/miguel/Projects/smlm_z/publication/VIT_openframe/stacks.ome.tif')\n",
    "locs = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/VIT_openframe/locs.hdf', key='locs')\n",
    "z_step = 10\n",
    "\n",
    "psfs = psfs[::5]\n",
    "locs = locs.iloc[::5]\n",
    "\n",
    "for i in range(5):\n",
    "    locs[f'offset_{i}'] = realign_beads(psfs, locs, z_step, i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dec8a6-9c11-44f0-90db-b7123f951106",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_psf = np.zeros(psfs.shape[1:])\n",
    "from data.visualise import show_psf_axial\n",
    "for p, offset in zip(psfs, locs['offset_0'].to_numpy()):\n",
    "    p = norm_zero_one(p)\n",
    "    rolled = np.roll(p, shift=-int(offset//z_step), axis=0)\n",
    "    empty_psf += rolled\n",
    "\n",
    "ref_psf = norm_zero_one(empty_psf)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80621172-fc9d-4d69-a287-46f52e4dc602",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(locs['offset_avg_ref'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cae89e-0031-4b65-908d-6bbc2c77f2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(ref_psf.max(axis=(1,2))) * z_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b5792c-ba7b-4f01-8342-2a1739d0eb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def realign_beads2(psfs, df, z_step, ref_psf):\n",
    "    ref_psf = norm_zero_one(ref_psf)\n",
    "    ref_tf = tf.convert_to_tensor(ref_psf)\n",
    "    rolls = []\n",
    "    for idx in trange(df.shape[0]):\n",
    "        if idx == ref_idx:\n",
    "            roll = 0\n",
    "        else:\n",
    "            psf2 = norm_zero_one(psfs[idx])\n",
    "            roll = -tf_find_optimal_roll(ref_tf, psf2)\n",
    "        rolls.append(roll)\n",
    "    rolls = np.array(rolls)\n",
    "    return rolls * z_step\n",
    "\n",
    "locs['offset_avg_ref'] = realign_beads2(psfs, locs, 10, ref_psf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3cc6bb-53d9-4b46-b06d-958b2e0971f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in list(locs):\n",
    "    if 'offset' in c:\n",
    "        plt.scatter(x, (locs[c]-locs[c].mean())-locs['offset_avg_ref'], label=c)\n",
    "        \n",
    "# plt.scatter(x, locs['offset_avg_ref'], label='avg')\n",
    "# for i in range(5):\n",
    "#     plt.scatter(x, locs[f'offset_{i}'], label=str(i))\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a31ed29-957b-4e19-a186-f91fe6101507",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(locs.shape[0])\n",
    "\n",
    "ds = []\n",
    "for i in range(5):\n",
    "    y = abs(locs[f'offset_{i}'] - locs['offset_0'].to_numpy())\n",
    "    plt.scatter(x, y)\n",
    "    print(np.mean(y))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2c855b-ad9e-4a0d-b751-3383e87a71fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871f6a41-83a4-4345-b1af-d4052372ff47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1488e74-9a32-401e-8624-8cd17c709e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22401917-2baf-4c47-bc4b-c41185234184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "old_df = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/VIT_013/locs.hdf', key='locs')\n",
    "new_df = pd.read_hdf('/media/Data/smlm_z_data/20231121_nup_miguel_zeiss/stacks/combined/locs.hdf', key='locs')\n",
    "\n",
    "print(old_df.shape, new_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d97e85-3dfd-4e34-8d34-7c59b8664172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tifffile import imread\n",
    "\n",
    "d = imread('/home/miguel/Projects/smlm_z/publication/tmp/tmp.tif')\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f48000-4155-4aa2-a7b6-1a49dec20744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def snr(p):\n",
    "    return p.max() / p.mean()\n",
    "\n",
    "snrs = np.array([snr(p) for p in d])\n",
    "min_snr = 2.0\n",
    "idx = np.argwhere(snrs>min_snr).squeeze()\n",
    "print(len(idx))\n",
    "\n",
    "from data.visualise import show_psf_axial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2211bcd-7a13-4a24-bb52-dd6563aeca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_sharpness(array):\n",
    "    gy, gx = np.gradient(array)\n",
    "    gnorm = np.sqrt(gx**2 + gy**2)\n",
    "    sharpness = np.average(gnorm)\n",
    "    return sharpness\n",
    "\n",
    "\n",
    "def reduce_img(psf):\n",
    "    return np.stack([get_sharpness(x) for x in psf])\n",
    "\n",
    "\n",
    "def filter_mse_xy(stack, max_mse, i):\n",
    "    # Define a 2D Gaussian function\n",
    "    def gaussian_2d(xy, amplitude, xo, yo, sigma_x, sigma_y, theta, offset):\n",
    "        x, y = xy\n",
    "        xo = float(xo)\n",
    "        yo = float(yo)\n",
    "        a = (np.cos(theta)**2) / (2 * sigma_x**2) + (np.sin(theta)**2) / (2 * sigma_y**2)\n",
    "        b = -(np.sin(2 * theta)) / (4 * sigma_x**2) + (np.sin(2 * theta)) / (4 * sigma_y**2)\n",
    "        c = (np.sin(theta)**2) / (2 * sigma_x**2) + (np.cos(theta)**2) / (2 * sigma_y**2)\n",
    "        g = offset + amplitude * np.exp(- (a * ((x - xo)**2) + 2 * b * (x - xo) * (y - yo) + c * ((y - yo)**2)))\n",
    "        return g.ravel()\n",
    "\n",
    "    sharp = reduce_img(stack)\n",
    "    idx = np.argmax(sharp)\n",
    "    image = stack[idx]\n",
    "\n",
    "\n",
    "    # Load and preprocess the image (e.g., convert to grayscale)\n",
    "    # For simplicity, let's generate a simple image for demonstration\n",
    "    image_size = image.shape[1]\n",
    "    x = np.linspace(0, image_size - 1, image_size)\n",
    "    y = np.linspace(0, image_size - 1, image_size)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    \n",
    "    # Fit the Gaussian to the image data\n",
    "    p0 = [1, image_size / 2, image_size / 2, 2, 2, 0, 0]  # Initial guess for parameters\n",
    "    bounds = [\n",
    "        (0, np.inf),\n",
    "        (image_size * (1/5), image_size * (4/5)),\n",
    "        (image_size * (1/5), image_size * (4/5)),\n",
    "        (0, image_size/3),\n",
    "        (0, image_size/3),\n",
    "        (-np.inf, np.inf),\n",
    "        (0, np.inf),\n",
    "    ]\n",
    "\n",
    "    image = image / image.max()\n",
    "\n",
    "    try:\n",
    "        popt, pcov = curve_fit(gaussian_2d, (x, y), image.ravel(), p0=p0, bounds=list(zip(*bounds)))\n",
    "    except RuntimeError:\n",
    "        print('XY fit failed')\n",
    "        popt = p0\n",
    "    render = gaussian_2d((x, y), *popt).reshape(image.shape)\n",
    "\n",
    "    render0 = gaussian_2d((x, y), *p0).reshape(image.shape)\n",
    "    error = mean_squared_error(render, image)\n",
    "\n",
    "    if error > max_mse:\n",
    "        d = 'bad'\n",
    "    else:\n",
    "        d = 'good'\n",
    "\n",
    "    \n",
    "    # show_psf_axial(stack, f'{i} {d} ' + '{:.3E}'.format(error), 30)\n",
    "    # Visualize the original image and the fitted Gaussian\n",
    "    print(p0)\n",
    "    print(popt)\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.title('Error' +  '{:.8E}'.format(error))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title('Original Image')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(render0)\n",
    "\n",
    "    plt.title('Initial Gaussian')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(render)\n",
    "    plt.title('Fitted Gaussian')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # plt.savefig(f'./{d}/{i}.png')\n",
    "    # plt.close()\n",
    "    print(error, max_mse, error <= max_mse)\n",
    "\n",
    "    return error <= max_mse\n",
    "\n",
    "# problem_idx = \n",
    "for i in [1548]:\n",
    "    filter_mse_xy(d[i], 0.005, i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4be75a-a3a7-4645-b1b9-360148210b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "model_dir = '/home/miguel/Projects/smlm_z/publication/VIT_035/out_roll_alignment'\n",
    "\n",
    "args = {\n",
    "    'outdir': model_dir\n",
    "}\n",
    "\n",
    "model = keras.models.load_model(os.path.join(args['outdir'], './latest_vit_model3'))\n",
    "test_data = tf.data.Dataset.load(os.path.join(args['outdir'], 'test'))\n",
    "pred_zs = model.predict(test_data, batch_size=4096)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7d0385-f3eb-4fd9-bd04-4f3eca45757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_z_coordinates(dataset):\n",
    "    zs = []\n",
    "    xys = []\n",
    "    for (_, xy), z in dataset.as_numpy_iterator():\n",
    "        zs.append(z)\n",
    "        xys.append(xy)\n",
    "\n",
    "    return np.concatenate(xys).squeeze(), np.concatenate(zs).squeeze()\n",
    "\n",
    "xys, zs = get_z_coordinates(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51686bfd-4d19-4a23-9da0-1455e19c37ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_zs = pred_zs.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c862aa1-a800-4eb4-9791-f273ff8aff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize as opt\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "def bestfit_error(z_true, z_pred):\n",
    "    def linfit(x, c):\n",
    "        return x + c\n",
    "\n",
    "    x = z_true\n",
    "    y = z_pred\n",
    "    popt, _ = opt.curve_fit(linfit, x, y, p0=[0])\n",
    "\n",
    "    x = np.linspace(z_true.min(), z_true.max(), len(y))\n",
    "    y_fit = linfit(x, popt[0])\n",
    "    error = root_mean_squared_error(y_fit, y)\n",
    "    return error, popt[0], y_fit, abs(y_fit-y)\n",
    "\n",
    "def remove_constant_error(xys, zs, pred_zs):\n",
    "    coords2 = ['_'.join(x.astype(str)) for x in xys]\n",
    "    all_errors = []\n",
    "    for num, c in enumerate(set(coords2)):\n",
    "        idx = [i for i, val in enumerate(coords2) if val==c]\n",
    "        _, _, _, errors = bestfit_error(zs[idx], pred_zs[idx])\n",
    "        all_errors.append(errors)\n",
    "    all_errors = np.concatenate(all_errors)\n",
    "    return all_errors\n",
    "\n",
    "corrected_errors = remove_constant_error(xys, zs, pred_zs)\n",
    "errors = abs(pred_zs-zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41f7500-8b45-4199-8d7a-083ba42260df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ries_data = pd.read_csv('/home/miguel/Projects/smlm_z/publication/ries_comparison_data.csv')\n",
    "cols = list(ries_data)\n",
    "ries_deeploc = ries_data[[c for c in cols if ('DeepLoc' in c) or ('z(nm)' in c)]].dropna().set_index('z(nm)')\n",
    "crlb_deeploc = ries_data[[c for c in cols if ('CRLB' in c) or ('z(nm)' in c)]].dropna().set_index('z(nm)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af60de4-e28c-45dd-80cc-5da5072a9080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "pred_zs = pred_zs.squeeze()\n",
    "zs = zs.squeeze()\n",
    "\n",
    "sns.regplot(x=pred_zs, y=corrected_errors, scatter=True, ci=95, order=5, x_bins=np.arange(-1000, 1000, 50), label='Our method')\n",
    "sns.lineplot(data=ries_deeploc)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sns.regplot(x=pred_zs, y=corrected_errors, scatter=True, ci=95, order=5, x_bins=np.arange(-1000, 1000, 50), label='Our method')\n",
    "sns.lineplot(data=crlb_deeploc)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bbe62c-0693-4363-99c1-c79a545d92b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad1edb-c77c-4129-80f2-45bb59f3d6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c6c7fd-562e-432a-aa8e-df4814002211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd69d53-9431-456f-825d-dc903a2b350f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf46ceb-0b72-416c-9d6c-f7940ca71520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22608d12-d843-4415-8196-0402ee184332",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stacks = imread('/media/Data/smlm_z_data/20231121_nup_miguel_zeiss/stacks/tmp.tif')\n",
    "print(stacks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc40522-4573-4165-893e-c919abbb7bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tifffile import imread\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def get_sharpness(array):\n",
    "    gy, gx = np.gradient(array)\n",
    "    gnorm = np.sqrt(gx**2 + gy**2)\n",
    "    sharpness = np.average(gnorm)\n",
    "    return sharpness\n",
    "\n",
    "\n",
    "def reduce_img(psf):\n",
    "    return np.stack([get_sharpness(x) for x in psf])\n",
    "    \n",
    "def filter_mse_xy(stack, max_mse, i):\n",
    "    # Define a 2D Gaussian function\n",
    "    def gaussian_2d(xy, amplitude, xo, yo, sigma_x, sigma_y, theta, offset):\n",
    "        x, y = xy\n",
    "        xo = float(xo)\n",
    "        yo = float(yo)\n",
    "        a = (np.cos(theta)**2) / (2 * sigma_x**2) + (np.sin(theta)**2) / (2 * sigma_y**2)\n",
    "        b = -(np.sin(2 * theta)) / (4 * sigma_x**2) + (np.sin(2 * theta)) / (4 * sigma_y**2)\n",
    "        c = (np.sin(theta)**2) / (2 * sigma_x**2) + (np.cos(theta)**2) / (2 * sigma_y**2)\n",
    "        g = offset + amplitude * np.exp(- (a * ((x - xo)**2) + 2 * b * (x - xo) * (y - yo) + c * ((y - yo)**2)))\n",
    "        return g.ravel()\n",
    "\n",
    "    sharp = reduce_img(stack)\n",
    "    idx = np.argmax(sharp)\n",
    "    image = stack[idx]\n",
    "\n",
    "    # Load and preprocess the image (e.g., convert to grayscale)\n",
    "    # For simplicity, let's generate a simple image for demonstration\n",
    "    image_size = image.shape[1]\n",
    "    x = np.linspace(0, image_size - 1, image_size)\n",
    "    y = np.linspace(0, image_size - 1, image_size)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "\n",
    "    # Fit the Gaussian to the image data\n",
    "    p0 = [1, image_size / 2, image_size / 2, 5, 5, 0, 0]  # Initial guess for parameters\n",
    "    bounds = [\n",
    "        (-np.inf, np.inf),\n",
    "        (image_size * (2/5), image_size * (3/5)),\n",
    "        (image_size * (2/5), image_size * (3/5)),\n",
    "        (0, np.inf),\n",
    "        (0, np.inf),\n",
    "        (-np.inf, np.inf),\n",
    "        (0, np.inf),\n",
    "    ]\n",
    "\n",
    "    fit_failed = False\n",
    "    try:\n",
    "        popt, pcov = curve_fit(gaussian_2d, (x, y), image.ravel(), p0=p0, bounds=list(zip(*bounds)))\n",
    "    except RuntimeError:\n",
    "        print('XY fit failed')\n",
    "        fit_failed = True\n",
    "        popt = p0\n",
    "\n",
    "    render = gaussian_2d((x, y), *popt).reshape(image.shape)\n",
    "\n",
    "    error = mean_squared_error(render, image)\n",
    "\n",
    "    # Visualize the original image and the fitted Gaussian\n",
    "    if error >= max_mse or fit_failed:\n",
    "        print(i)\n",
    "\n",
    "        for limits, val in zip(bounds, popt):\n",
    "            print(limits, val)\n",
    "    \n",
    "        plt.plot(sharp)\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(5, 3))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title('Original Image')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(render)\n",
    "        plt.title('Fitted Gaussian')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(error, max_mse, error <= max_mse)\n",
    "        show_psf_axial(stack, '', 30)\n",
    "\n",
    "\n",
    "    return error <= max_mse\n",
    "from data.visualise import show_psf_axial\n",
    "for i in range(stacks.shape[0]):\n",
    "    print(i)\n",
    "    filter_mse_xy(stacks[i], 10000, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85324714-17e1-4342-9bd3-8a0117eb53dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plane alignment for beads\n",
    "\n",
    "\n",
    "from numpy.linalg import lstsq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import linear_model\n",
    "def realign_beads(psfs, df):\n",
    "        \n",
    "    X_data = df[['x', 'y']].to_numpy()\n",
    "    Y_data = df['offset'].to_numpy()\n",
    "    \n",
    "    reg = linear_model.LinearRegression().fit(X_data, Y_data)\n",
    "    \n",
    "    print(\"coefficients of equation of plane, (a1, a2): \", reg.coef_)\n",
    "    \n",
    "    print(\"value of intercept, c:\", reg.intercept_)\n",
    "    \n",
    "    z_fit = reg.predict(X_data)\n",
    "    \n",
    "    error = abs(z_fit-Y_data)\n",
    "    perc_cutoff = np.percentile(error, 95)\n",
    "    \n",
    "    idx = np.argwhere(error<=perc_cutoff).squeeze()\n",
    "    X_data = X_data[idx]\n",
    "    Y_data = Y_data[idx]\n",
    "    \n",
    "    psfs = psfs[idx]\n",
    "    df = df.iloc[idx]\n",
    "    \n",
    "    reg = linear_model.LinearRegression().fit(X_data, Y_data)\n",
    "    print(\"coefficients of equation of plane, (a1, a2): \", reg.coef_)\n",
    "    \n",
    "    print(\"value of intercept, c:\", reg.intercept_)\n",
    "    \n",
    "    z_fit = reg.predict(X_data)\n",
    "    df['offset'] = z_fit\n",
    "    \n",
    "    return psfs, df\n",
    "\n",
    "df = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/VIT_031_redo2/locs.hdf', key='locs')\n",
    "\n",
    "realign_beads(np.zeros(df.shape), df)\n",
    "plt.hist(error)\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(data=df, x='x', y='y', hue='offset')\n",
    "plt.show()\n",
    "sns.scatterplot(data=df.iloc[idx], x='x', y='y', hue=z_fit)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f32e075-d8f8-4af5-ba24-3a8c56458f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "for n2 in glob.glob('/home/miguel/Projects/smlm_z/publication/VIT_03*/out*'):\n",
    "    n = n2.replace('/home/miguel/Projects/smlm_z/publication/', '')\n",
    "    if 'subset' in n or '035' in n:\n",
    "        continue\n",
    "    dirname, outdir = n.split('/')\n",
    "\n",
    "    nup_path = os.path.join(n2, 'out_nup', 'nup_renders2')\n",
    "    model_path = os.path.join(n2, 'latest_vit_model3')\n",
    "    if not os.path.exists(nup_path) and os.path.exists(model_path):\n",
    "        print(f'cd /home/miguel/Projects/smlm_z/publication/VIT_031_redo/{outdir} && python3 ../../localise_exp_sample.py -mo . -o out_nup && cd out_nup && python3 ../../../render_nup.py && cd /home/miguel/Projects/smlm_z/publication/VIT_031_redo;')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e21fb36-41d2-4c68-88ed-e40e8ff84f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "records = []\n",
    "\n",
    "reports = glob.glob('/home/miguel/Projects/smlm_z/publication/VIT_03*/out*/out_nup/nup_renders2/*.csv')\n",
    "\n",
    "def within_n(seps, sep):\n",
    "    return sum((50-sep<=seps) & (seps<=50+sep))\n",
    "\n",
    "for r in reports:\n",
    "    if 'plane_alignment' in r and not 'plane_alignment_3' in r:\n",
    "        continue\n",
    "    outdir = '/'.join(r.split('/')[0:8])\n",
    "    name = '_'.join(outdir.split('/')[-2:])\n",
    "    training_report = os.path.join(outdir, 'results', 'report.json')\n",
    "    with open(training_report) as f:\n",
    "        training_report_data = json.load(f)\n",
    "\n",
    "    nup_report = pd.read_csv(r)\n",
    "\n",
    "    bead_report = os.path.join(outdir, 'results', 'results.csv')\n",
    "    bead_report_data = pd.read_csv(bead_report)\n",
    "    bead_report_data = bead_report_data[bead_report_data['dataset']=='test']\n",
    "    \n",
    "    args = training_report_data['args']\n",
    "    gen_args = args['gen_args']\n",
    "    records.append({\n",
    "        'name': name,\n",
    "        'train_mae': training_report_data['train_mae'],\n",
    "        'val_mae': training_report_data['val_mae'],\n",
    "        'test_mae': training_report_data['test_mae'],\n",
    "        'aug_ratio': args['aug_ratio'],\n",
    "        'brightness': args['brightness'],\n",
    "        'gauss': args['gauss'],\n",
    "        'min_snr': gen_args['min_snr'],\n",
    "        'mean_seperation': np.mean(nup_report['seperation']),\n",
    "        'stdev_seperation': np.std(nup_report['seperation']),\n",
    "        'sep_count_40-60': within_n(nup_report['seperation'], 10),\n",
    "        'sep_count_45-55': within_n(nup_report['seperation'], 5),\n",
    "        'mean_bead_offset': np.mean(np.abs(bead_report_data['offset']))\n",
    "    })\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "df = pd.DataFrame.from_records(records)\n",
    "df.sort_values(['sep_count_45-55'], ascending=False, inplace=True)\n",
    "print(df.shape)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd5d62f-8071-47cd-a90f-8e63bfc29161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.regplot(data=df, x='mean_bead_offset', y='test_mae')\n",
    "plt.show()\n",
    "\n",
    "for c in ['train_mae', 'val_mae', 'test_mae', 'min_snr', 'aug_ratio', 'gauss', 'brightness', 'stdev_seperation', 'mean_seperation', 'mean_bead_offset']:\n",
    "    sns.scatterplot(data=df, x=c, y='sep_count_40-60')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bde2fc9-5707-4cad-bb6c-baabae29be3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tifffile import imread\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "args = {\n",
    "    'stacks': '/home/miguel/Projects/smlm_z/publication/VIT_031_redo_subset/stacks.ome.tif',\n",
    "    'locs': '/home/miguel/Projects/smlm_z/publication/VIT_031_redo_subset/locs.hdf',\n",
    "    'zstep': 10,\n",
    "    'zrange': 1000\n",
    "}\n",
    "psfs = imread(args['stacks'])[:, :, :, :, np.newaxis]\n",
    "locs = pd.read_hdf(args['locs'], key='locs')\n",
    "locs['idx'] = np.arange(locs.shape[0])\n",
    "# idx = (xlim[0] < all_locs['x']) & (all_locs['x'] < xlim[1]) & (ylim[0] < all_locs['y']) & (all_locs['y'] < ylim[1])\n",
    "# locs = all_locs[idx]\n",
    "# psfs = all_psfs[locs['idx']]\n",
    "\n",
    "def filter_zrange(X, zs):\n",
    "    psfs = X\n",
    "    valid_ids = np.argwhere(abs(zs.squeeze()) < args['zrange']).squeeze()\n",
    "    return psfs[valid_ids], zs[valid_ids]\n",
    "        \n",
    "ys = []\n",
    "for offset in locs['offset']:\n",
    "    zs = ((np.arange(psfs.shape[1])) * args['zstep']) - offset\n",
    "    ys.append(zs)\n",
    "\n",
    "ys = np.array(ys)\n",
    "\n",
    "psfs = np.concatenate(psfs)\n",
    "ys = np.concatenate(ys)\n",
    "\n",
    "psfs, ys = filter_zrange(psfs, ys)\n",
    "\n",
    "print(psfs.shape, ys.shape)\n",
    "\n",
    "print(ys.min(), ys.max())\n",
    "\n",
    "train = psfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd182945-1dc4-4371-a41c-792edce2a75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "spots_path = '/home/miguel/Projects/data/20230601_MQ_celltype/nup/fov2/storm_1/storm_1_MMStack_Default.ome_spots.hdf5'\n",
    "with h5py.File(spots_path, \"r\") as f:\n",
    "    test = np.array(f['spots'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d5a03c-ad48-4dd4-9d6e-3ab7f4f6237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensitivity = 0.45, quantum efficiency = 0.9\n",
    "\n",
    "# em gain (1) baseline (100)\n",
    "# new_spots = (spots - baseline) * sensitivity / (gain)\n",
    "\n",
    "baseline = 100\n",
    "sensitivity = 0.45\n",
    "gain = 1\n",
    "\n",
    "test2 = (raw_test * gain / sensitivity) + baseline\n",
    "\n",
    "test2 = test2.astype(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ded300-849e-4ddf-bc2a-9885a2cc1ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train = imread('/home/miguel/Projects/data/all_openframe_beads/20231205_miguel_mitochondria/stack__10_MMStack_Default.ome.tif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2334fb0a-6b58-4f0f-bfd9-75007473558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test = imread('/home/miguel/Projects/data/20230601_MQ_celltype/nup/fov2/storm_1_MMStack_Default_2.ome.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da839ac0-1305-4110-8bd9-a91330206686",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.min(), train.max(), train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aba8240-d453-47b0-af45-01baa8c70f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.min(), test.max(), test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceb28aa-96e0-4b47-a348-b1703a00bc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train.min(), raw_train.max(), raw_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed35892a-b0f7-4dbd-9392-a0b6ab8ffd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test.min(), raw_test.max(), raw_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bbda9b-0829-41eb-988a-7eb98f7abdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2.min(), test2.max(), test2.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7112b284-6f95-44b4-95fc-e05bf354fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras \n",
    "\n",
    "# from keras import layers\n",
    "# from keras.layers.pre\n",
    "\n",
    "# class NoClipRandomContrast(layers.RandomContrast):\n",
    "#     def __init__(self, factor, seed=None, clip=False, **kwargs):\n",
    "#         super().__init__(factor, seed, **kwargs)\n",
    "#         self.clip = clip\n",
    "\n",
    "#     def call(self, inputs, training=True):\n",
    "#         # inputs = self.backend.cast(inputs, self.compute_dtype)\n",
    "#         if training:\n",
    "#             seed_generator = self._get_seed_generator(self.backend._backend)\n",
    "#             factor = self.backend.random.uniform(\n",
    "#                 shape=(),\n",
    "#                 minval=1.0 - self.lower,\n",
    "#                 maxval=1.0 + self.upper,\n",
    "#                 seed=seed_generator,\n",
    "#                 dtype=self.compute_dtype,\n",
    "#             )\n",
    "\n",
    "#             outputs = self._adjust_constrast(inputs, factor)\n",
    "#             if self.clip:\n",
    "#                 outputs = self.backend.numpy.clip(outputs, 0, 255)\n",
    "#             self.backend.numpy.reshape(outputs, self.backend.shape(inputs))\n",
    "#             return outputs\n",
    "#         else:\n",
    "#             return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6f07b4-2940-4266-8358-21eb3b9b756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential, layers\n",
    "MAX_GAUSS_NOISE = 0.001\n",
    "MAX_TRANSLATION_PX = 0\n",
    "BRIGHTNESS = 0.2\n",
    "aug_pipeline = Sequential([\n",
    "    # layers.GaussianNoise(stddev=MAX_GAUSS_NOISE*X_train[0].max(), seed=args['seed']),\n",
    "    # layers.RandomTranslation(MAX_TRANSLATION_PX/img_size, MAX_TRANSLATION_PX/img_size, seed=args['seed']),\n",
    "    layers.RandomBrightness(BRIGHTNESS, value_range=[0, psfs.max()], seed=42),\n",
    "    NoClipRandomContrast(0.2, seed=42),\n",
    "    layers.RandomContrast(0.2, seed=42)\n",
    "])\n",
    "\n",
    "new_ds = aug_pipeline(train.astype(float), training=True).numpy()\n",
    "print(new_ds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bb43d0-bc97-4c5c-827d-43ac4e1e7488",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in [train, test, new_ds]:\n",
    "    print(ds.min(), ds.std(), ds.mean(), ds.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7373d51b-10c8-4016-a700-9bf70ee48f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_hdf('/home/miguel/Projects/data/20231212_miguel_openframe/tubulin/FOV1/storm_1/storm_1_MMStack_Default.ome_locs_undrifted.hdf5', key='locs')\n",
    "\n",
    "\n",
    "sns.scatterplot(data=df, x='x', y='y', alpha=0.1)\n",
    "plt.xlim((400, 600))\n",
    "plt.ylim((700, 1000))\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82790dc6-222c-4bae-803a-0552984bad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "old_locs = '/home/miguel/Projects/data/20230601_MQ_celltype/nup/fov2/storm_1/storm_1_MMStack_Default.ome_locs_undrifted_picked_4.hdf5'\n",
    "old_locs = pd.read_hdf(old_locs, key='locs')\n",
    "for c in set(old_locs['group']):\n",
    "    if c != 10:\n",
    "        continue\n",
    "    sub_df = old_locs[old_locs['group']==c]\n",
    "    print(sub_df['x'].max()-sub_df['x'].min())\n",
    "    # sns.scatterplot(data=sub_df, x='x', y='y')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4d9a82-9f62-485a-829c-2412587dab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_locs = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/VIT_031_redo/out6/out_nup/locs_3d.hdf5', key='locs')\n",
    "new_locs = new_locs[new_locs['x'].isin(old_locs['x'])]\n",
    "\n",
    "locs = new_locs.merge(old_locs, on=['x', 'y', 'photons', 'bg', 'lpx', 'lpy', 'net_gradient', 'iterations', 'frame', 'likelihood', 'sx', 'sy'])\n",
    "locs['clusterID'] = locs['group']\n",
    "print(list(locs))\n",
    "sns.scatterplot(data=old_locs[old_locs['group']==10], x='x', y='y')\n",
    "plt.show()\n",
    "sns.scatterplot(data=locs[locs['group']==10], x='x', y='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abf3dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyotf.otf import HanserPSF, apply_aberration, apply_named_aberration\n",
    "from pyotf.phaseretrieval import retrieve_phase\n",
    "from pyotf.utils import prep_data_for_PR, center_data\n",
    "import sys\n",
    "sys.path.append('/home/miguel/Projects/uni/phd/smlm_z')\n",
    "from data.estimate_offset import get_peak_sharpness\n",
    "from data.visualise import grid_psfs, show_psf_axial\n",
    "\n",
    "def mse(y, y_pred):\n",
    "    return np.mean((y - y_pred) ** 2).sum()\n",
    "\n",
    "psf = X[0]\n",
    "model_kwargs = dict(\n",
    "    wl=0.665,\n",
    "    na=0.9,\n",
    "    ni=1.34,\n",
    "    res=0.09,\n",
    "    zres=0.01,\n",
    "    size=psf.shape[1],\n",
    "    zsize=psf.shape[0],\n",
    "    vec_corr=\"none\",\n",
    "    condition=\"none\",\n",
    ")\n",
    "print(model_kwargs)\n",
    "psf = prep_data_for_PR(psf, multiplier=1.0)\n",
    "\n",
    "# Retrieve phase for experimental PSF\n",
    "PR_result = retrieve_phase(psf, model_kwargs, 100, 1e-4, 1e-4)\n",
    "\n",
    "PR_result.fit_to_zernikes(16)\n",
    "PR_result.plot()\n",
    "PR_result.zd_result.plot()\n",
    "PR_result.zd_result.plot_named_coefs()\n",
    "PR_result.plot_convergence()\n",
    "\n",
    "# Simulate HanserPSF with parameters\n",
    "\n",
    "result_psf = PR_result.generate_zd_psf(sphase=slice(None))\n",
    "\n",
    "# this part is very kludgy\n",
    "PR_result.model.PSFi = psf\n",
    "\n",
    "PR_result.model.PSFi = result_psf\n",
    "\n",
    "psf = psf / psf.max()\n",
    "result_psf = result_psf / result_psf.max()\n",
    "\n",
    "print(mse(psf, result_psf))\n",
    "\n",
    "plt.show()\n",
    "print('Experimental')\n",
    "show_psf_axial(psf / psf.max(), '', 15)\n",
    "print('Simulated')\n",
    "show_psf_axial(result_psf / result_psf.max(), '', 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11d3a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_img = np.zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94bb91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(PR_result.model.OTFa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bae29e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "xy = np.random.uniform(-1, 1, size=(50, 2))\n",
    "vals = np.random.uniform(0, 500, size=(50, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfef597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyotf\n",
    "import pyotf.zernike\n",
    "pyotf.zernike.name2noll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aba325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import euclidean_distances\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "s = np.linspace(-1, 1, 25)\n",
    "x, y = np.meshgrid(s, s)\n",
    "x = x.flatten()\n",
    "y = y.flatten()\n",
    "xy = np.stack((x,y)).T\n",
    "\n",
    "dists = euclidean_distances([[0, 0]], xy).squeeze()\n",
    "dists = np.power(dists, 3)\n",
    "dists /= dists.max()\n",
    "sns.scatterplot(x=x, y=y, hue=dists)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ef1fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/miguel/Projects/uni/phd/smlm_z')\n",
    "from data.visualise import grid_psfs, show_psf_axial\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import euclidean_distances\n",
    "import seaborn as sns\n",
    "model_kwargs = dict(\n",
    "    wl=647,\n",
    "    na=1.3,\n",
    "    ni=1.51,\n",
    "    res=90,\n",
    "    zres=50,\n",
    "    size=32,\n",
    "    zsize=100,\n",
    "    vec_corr=\"none\",\n",
    "    condition=\"none\",\n",
    ")\n",
    "from pyotf.otf import HanserPSF, apply_aberration, apply_named_aberration\n",
    "\n",
    "def get_ab(a1, a2):\n",
    "    base_args = np.array([0, 0, 0, 0, 0, a1, a2])\n",
    "    return base_args\n",
    "    \n",
    "def gen_fake_psf(model_kwargs, ab):\n",
    "    model = HanserPSF(**model_kwargs)\n",
    "    # model = apply_named_aberration(model, 'oblique astigmatism', 2)\n",
    "    model = apply_aberration(model, np.zeros(ab.shape), ab)\n",
    "\n",
    "    psf = model.PSFi\n",
    "    psf = psf.astype(float)\n",
    "    # psf = psf / psf.max()\n",
    "    return psf\n",
    "\n",
    "def add_noise(psf):\n",
    "    return psf + np.random.normal(0, 1e-2, size=psf.shape)\n",
    "\n",
    "coords = []\n",
    "psfs = []\n",
    "\n",
    "n_points = 50\n",
    "\n",
    "lim = 1\n",
    "xy = np.random.uniform(-1, 1, size=(n_points, 2))\n",
    "# xy = np.stack([np.linspace(-lim, lim, n_points), np.linspace(-lim, lim, n_points)]).T\n",
    "center = [[0, 0]]\n",
    "dists = euclidean_distances(xy, center).squeeze()\n",
    "# dists = 2**dists\n",
    "dists = np.power(dists, 3)\n",
    "dists /= dists.max()\n",
    "\n",
    "a1s = []\n",
    "a2s = []\n",
    "\n",
    "for i in range(n_points):\n",
    "    x, y = xy[i]\n",
    "    dist = dists[i]\n",
    "\n",
    "    a1 = (1 if x>0 else -1) * dist\n",
    "    a2 = (1 if y>0 else -1) * dist\n",
    "    a1s.append(a1)\n",
    "    a2s.append(a2)\n",
    "    ab = get_ab(a1, a2)\n",
    "    psf = gen_fake_psf(model_kwargs, ab*2)\n",
    "#     psf = add_noise(psf)\n",
    "    coords.append([x, y])\n",
    "    psfs.append(psf)\n",
    "#     plt.imshow(grid_psfs(psf[::7]).squeeze())\n",
    "#     plt.show()\n",
    "\n",
    "psfs = np.array(psfs)\n",
    "df = pd.DataFrame(coords, columns=['x', 'y'])\n",
    "sns.scatterplot(x=xy[:, 0], y=xy[:, 1], hue=a1s)\n",
    "plt.show()\n",
    "sns.scatterplot(x=xy[:, 0], y=xy[:, 1], hue=a2s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f76e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(psfs.shape)\n",
    "psfs.reshape((psfs.shape[0], -1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732760d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "pca = PCA().fit(psfs.reshape((psfs.shape[0], -1)))\n",
    "\n",
    "d = pca.transform(psfs.reshape((psfs.shape[0], -1)))\n",
    "sns.scatterplot(x=d[:, 0], y=d[:, 1], hue=dists)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac52e8e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# randomly roll psfs\n",
    "import seaborn as sns\n",
    "\n",
    "z_step = 50\n",
    "\n",
    "# subsample psfs\n",
    "# subsample = 5\n",
    "# psfs = psfs[:, ::subsample, :, :]\n",
    "# z_step *= subsample\n",
    "\n",
    "def roll_psf(psf, roll):\n",
    "    rolled_psf = np.roll(psfs[i], shift=roll, axis=0)\n",
    "#     if roll < 0:\n",
    "#         rolled_psf[roll:] = 0\n",
    "#     else:\n",
    "#         rolled_psf[:roll] = 0\n",
    "    return rolled_psf\n",
    "#     show_psf_axial(psfs[i], '', 7)\n",
    "#     show_psf_axial(rolled_psf, roll, 7)\n",
    "\n",
    "rolls = []\n",
    "for i in range(psfs.shape[0]):\n",
    "    roll = np.random.randint(-5, 5)\n",
    "    psfs[i] = roll_psf(psfs[i], roll)\n",
    "    rolls.append(roll)\n",
    "df['roll'] = np.array(rolls) * z_step\n",
    "# df['roll'] *= 10\n",
    "\n",
    "\n",
    "\n",
    "sns.scatterplot(data=df, x='x', y='y', hue='roll')\n",
    "plt.show()\n",
    "for i in range(psfs.shape[0]):\n",
    "    show_psf_axial(psfs[i], i, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0836608a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pca.transform(psfs.reshape((psfs.shape[0], -1)))\n",
    "sns.scatterplot(x=d[:, 0], y=d[:, 1], hue=dists)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3936f083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load real data\n",
    "# import pandas as pd\n",
    "# from tifffile import imread\n",
    "# import matplotlib.pyplot as plt\n",
    "# df = pd.read_csv('/home/miguel/Projects/uni/phd/smlm_z/smlm-z/data/05_model_input/coords.csv')\n",
    "# psfs = imread('/home/miguel/Projects/uni/phd/smlm_z/smlm-z/data/05_model_input/spots.tif')\n",
    "# df['roll'] = 0\n",
    "# assert psfs.shape[0] == df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295417e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.exposure import match_histograms\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "from multiprocessing import Pool\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "import tqdm\n",
    "from data.estimate_offset import get_peak_sharpness\n",
    "from data.visualise import show_psf_axial\n",
    "from keras.losses import MeanSquaredError\n",
    "from multiprocessing.spawn import prepare\n",
    "import numpy as np\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from tqdm import trange\n",
    "import tensorflow as tf\n",
    "\n",
    "mse = MeanSquaredError()\n",
    "DEBUG = False\n",
    "UPSCALE_RATIO = 10\n",
    "def norm_zero_one(s):\n",
    "    max_s = s.max()\n",
    "    min_s = s.min()\n",
    "    return (s - min_s) / (max_s - min_s)\n",
    "\n",
    "\n",
    "def pad_and_fit_spline(coords, psf, z, z_ups):\n",
    "    x, y = coords\n",
    "    zs = psf[:, x, y]\n",
    "    cs = UnivariateSpline(z, zs, k=1, s=1e-3)\n",
    "    if False:\n",
    "        plt.scatter(z, zs, label='raw')\n",
    "        plt.plot(z_ups, cs(z_ups), label='smooth')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return x, y, cs(z_ups)\n",
    "    \n",
    "def upsample_psf(psf, ratio=UPSCALE_RATIO):\n",
    "    pad_width = 10\n",
    "    z = np.arange(-pad_width, psf.shape[0] + pad_width)\n",
    "    z_ups = np.arange(0, psf.shape[0], 1/ratio)\n",
    "    upsampled_psf = np.zeros((z_ups.shape[0], *psf.shape[1:]))\n",
    "    \n",
    "    psf = np.pad(psf, ((pad_width, pad_width), (0, 0), (0, 0)), mode='edge')\n",
    "    xys = list(product(np.arange(psf.shape[1]), np.arange(psf.shape[2])))\n",
    "    func = partial(pad_and_fit_spline, psf=psf, z=z, z_ups=z_ups)\n",
    "    res = list(map(func, xys))\n",
    "    # with Pool(8) as p:\n",
    "    #     res = list(p.imap(func, xys))\n",
    "    for x, y, z_col in res:\n",
    "        upsampled_psf[:, x, y] = z_col\n",
    "\n",
    "    return upsampled_psf\n",
    "\n",
    "\n",
    "def plot_correction(target, img, psf_corrected, errors):\n",
    "    if True:\n",
    "        plt.plot(target.max(axis=(1,2)), label='target')\n",
    "        plt.plot(img.max(axis=(1,2)),  label='original')\n",
    "        plt.plot(psf_corrected.max(axis=(1,2)), label='corrected', )\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "mse = MeanSquaredError(reduction='sum')\n",
    "def loss_func(true_m, pred_m):\n",
    "    m = tf.math.abs(true_m-pred_m)\n",
    "    m = tf.math.square(m*pred_m)\n",
    "    return tf.math.reduce_mean(m)\n",
    "\n",
    "def tf_find_optimal_roll(target, img, upscale_ratio=UPSCALE_RATIO):\n",
    "    ref_tf = tf.convert_to_tensor(target)\n",
    "    img_tf = tf.convert_to_tensor(img)\n",
    "    errors = []\n",
    "\n",
    "    for i in range(img.shape[0]):\n",
    "#         error = loss_func(ref_tf, img_tf)\n",
    "        error = mse(ref_tf, img_tf)\n",
    "        errors.append(error)\n",
    "        img_tf = tf.roll(img_tf, 1, axis=0)\n",
    "\n",
    "    best_i = tf.argmin(errors).numpy()\n",
    "    # Prefer small backwards roll to large forwards roll\n",
    "    if abs(best_i - img.shape[0]) < best_i:\n",
    "        best_i = best_i - img.shape[0]\n",
    "\n",
    "    psf_corrected = np.roll(img, int(best_i), axis=0)\n",
    "    plot_correction(target, img, psf_corrected, errors)\n",
    "\n",
    "    return best_i/upscale_ratio\n",
    "\n",
    "def prepare_psf(psf):\n",
    "#     psf = gaussian(psf, sigma=1)\n",
    "    psf = psf.copy()\n",
    "    psf = np.square(psf)\n",
    "    psf = norm_zero_one(psf)\n",
    "    psf = upsample_psf(psf)\n",
    "#     psf = mask_img_stack(psf, 12)\n",
    "    return psf\n",
    "\n",
    "\n",
    "def align_psfs(psf, psf2):\n",
    "    psf = prepare_psf(psf)\n",
    "    psf2 = prepare_psf(psf2)\n",
    "    psf = match_histograms(psf, psf2)\n",
    "    offset = tf_find_optimal_roll(psf, psf2)\n",
    "    return offset * z_step\n",
    "\n",
    "def find_seed_psf(df):\n",
    "    # Seed PSF - most centered PSF in FOV\n",
    "    center = df[['x', 'y']].mean(axis=0).to_numpy()\n",
    "    coords = df[['x', 'y']].to_numpy()\n",
    "    dists = euclidean_distances([center], coords).squeeze()\n",
    "    first_point = np.argmin(dists)\n",
    "    print(first_point)\n",
    "    return first_point\n",
    "\n",
    "def get_or_prepare_psf(prepped_psfs, raw_psfs, idx):\n",
    "    if idx not in prepped_psfs:\n",
    "        prepped_psfs[idx] = prepare_psf(raw_psfs[idx])\n",
    "    return prepped_psfs[idx]\n",
    "\n",
    "xys = df[['x', 'y']].to_numpy()\n",
    "\n",
    "errors = []\n",
    "def classic_align_psfs(psfs, df):\n",
    "    print(f'Aligning {psfs.shape} psfs...')\n",
    "\n",
    "    seed_psf = find_seed_psf(df)\n",
    "    ref_psf = prepare_psf(psfs[seed_psf])\n",
    "    offsets = np.zeros((psfs.shape[0]))\n",
    "\n",
    "    ref_0 = get_peak_sharpness(psfs[seed_psf])\n",
    "\n",
    "    for i in trange(0, psfs.shape[0]):\n",
    "        if i == seed_psf:\n",
    "            offsets[i] = 0\n",
    "            errors.append(0)\n",
    "            continue\n",
    "        psf = psfs[i]\n",
    "        psf = prepare_psf(psf)\n",
    "#         psf = match_histograms(psf, ref_psf)\n",
    "        offset = tf_find_optimal_roll(ref_psf, psf) * z_step\n",
    "        offsets[i] = offset\n",
    "        correct_dist = df['roll'][seed_psf] - df['roll'][i]\n",
    "        euc_dist = euclidean_distances([xys[i]], [xys[seed_psf]]).squeeze()\n",
    "        errors.append(correct_dist)\n",
    "        print(f\"{seed_psf} -> {i}, {offset}, {correct_dist}\")\n",
    "        if DEBUG:\n",
    "            offset_psf = np.roll(psf, shift=-int(offset), axis=0)\n",
    "            imgs = np.concatenate((ref_psf, offset_psf), axis=2)\n",
    "            show_psf_axial(imgs, subsample_n=30)\n",
    "            \n",
    "#         plt.imshow(grid_psfs(psf[::5]).T)\n",
    "#         plt.show()\n",
    "#         plt.imshow(grid_psfs(ref_psf[::5]).T)\n",
    "#         plt.show()\n",
    "\n",
    "#     offsets -= ref_0\n",
    "\n",
    "    return offsets\n",
    "\n",
    "classic_offsets = classic_align_psfs(psfs, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f51f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import skimage.filters as filters\n",
    "\n",
    "def gaussian(x, amplitude, mean, stddev):\n",
    "    return amplitude * np.exp(-(x - mean) ** 2 / (2 * stddev ** 2))\n",
    "\n",
    "def measure_psf_fwhm(psf):\n",
    "    # Normalize the PSF to range [0, 1]\n",
    "    psf_norm = (psf - np.min(psf)) / (np.max(psf) - np.min(psf))\n",
    "    \n",
    "    # Find the center of the PSF using the maximum intensity\n",
    "    center = np.unravel_index(np.argmax(psf_norm), psf_norm.shape)\n",
    "    # Extract a 1D slice of the PSF along the z-axis passing through the center\n",
    "    z_slice = psf_norm[:, center[0]]\n",
    "    \n",
    "    # Estimate the initial parameters of the Gaussian fit\n",
    "    amplitude = np.max(z_slice) - np.min(z_slice)\n",
    "    mean = center[0]\n",
    "    stddev = 2\n",
    "    \n",
    "    # Fit the Gaussian to the 1D slice using least squares optimization\n",
    "    try:\n",
    "        popt, _ = opt.curve_fit(gaussian, np.arange(z_slice.size), z_slice, p0=[amplitude, mean, stddev])\n",
    "    except RuntimeError:\n",
    "        return np.inf\n",
    "    # Compute the FWHM of the Gaussian fit\n",
    "    fwhm = 2 * np.sqrt(2 * np.log(2)) * popt[2]\n",
    "    \n",
    "    return fwhm\n",
    "\n",
    "def determine_best_focus_slice(psf):\n",
    "    # Measure the FWHM of the PSF for each z-slice\n",
    "    fwhm_values = []\n",
    "    for i in range(psf.shape[0]):\n",
    "        fwhm = measure_psf_fwhm(psf[i])\n",
    "        fwhm_values.append(fwhm)\n",
    "    \n",
    "    # Find the index of the z-slice with the minimum FWHM value\n",
    "    best_slice_idx = np.argmin(fwhm_values)\n",
    "    plt.rcParams['figure.figsize'] = [2, 2]\n",
    "    print(f'best slice: {best_slice_idx}')\n",
    "    slices = psf[[best_slice_idx-5, best_slice_idx, best_slice_idx], :, :]\n",
    "    show_psf_axial(slices, '', 1)\n",
    "    return best_slice_idx\n",
    "\n",
    "def fwhm_offsets(psfs):\n",
    "    idxs = np.array([determine_best_focus_slice(psf) for psf in psfs]).astype(float)\n",
    "    idxs -= np.mean(idxs)\n",
    "    return idxs\n",
    "\n",
    "# measure_psf_fwhm(psfs[0][0])\n",
    "new_offsets = fwhm_offsets(psfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb4bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Alignment error (frames, 50nm z-step)')\n",
    "sns.scatterplot(data=df, x='x', y='y', hue=abs(df['roll']-new_offsets))\n",
    "plt.show()\n",
    "plt.scatter(df['roll'], abs(df['roll']+classic_offsets))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84433e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.utils.graph_shortest_path import graph_shortest_path\n",
    "import networkx as nx\n",
    "\n",
    "def get_center_point(df):\n",
    "    center_point = [[0, 0]]\n",
    "    df['dists'] = euclidean_distances(df[['x', 'y']], center_point)\n",
    "    idx = df['dists'].idxmin()\n",
    "    return df.iloc[idx]\n",
    "\n",
    "dists = euclidean_distances(df[['x', 'y']])\n",
    "\n",
    "G = nx.from_numpy_matrix(dists)\n",
    "G = nx.minimum_spanning_tree(G)\n",
    "center_point = get_center_point(df)\n",
    "\n",
    "from itertools import combinations\n",
    "for src, target in G.edges:\n",
    "    G[src][target]['weight'] = dists[src, target]\n",
    "\n",
    "nx.draw(G, pos=df[['x', 'y']].values, with_labels=True, node_size=100, node_color='lightgreen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc2ee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.utils.graph_shortest_path import graph_shortest_path\n",
    "from tqdm import trange\n",
    "import networkx as nx\n",
    "\n",
    "UPSCALE_RATIO = 10\n",
    "\n",
    "def pad_and_fit_spline(coords, psf, z, z_ups):\n",
    "    x, y = coords\n",
    "    zs = psf[:, x, y]\n",
    "    cs = UnivariateSpline(z, zs, k=1, s=1e-6)\n",
    "    if False:\n",
    "        plt.scatter(z, zs, label='raw')\n",
    "        plt.plot(z_ups, cs(z_ups), label='smooth')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return x, y, cs(z_ups)\n",
    "    \n",
    "def upsample_psf(psf, ratio=UPSCALE_RATIO):\n",
    "    pad_width = 10\n",
    "    z = np.arange(-pad_width, psf.shape[0] + pad_width)\n",
    "    z_ups = np.arange(0, psf.shape[0], 1/ratio)\n",
    "    upsampled_psf = np.zeros((z_ups.shape[0], *psf.shape[1:]))\n",
    "    \n",
    "    psf = np.pad(psf, ((pad_width, pad_width), (0, 0), (0, 0)), mode='edge')\n",
    "    xys = list(product(np.arange(psf.shape[1]), np.arange(psf.shape[2])))\n",
    "    func = partial(pad_and_fit_spline, psf=psf, z=z, z_ups=z_ups)\n",
    "    res = list(map(func, xys))\n",
    "    # with Pool(8) as p:\n",
    "    #     res = list(p.imap(func, xys))\n",
    "    for x, y, z_col in res:\n",
    "        upsampled_psf[:, x, y] = z_col\n",
    "\n",
    "    return upsampled_psf\n",
    "\n",
    "\n",
    "def plot_correction(target, img, psf_corrected, errors):\n",
    "    if True:\n",
    "        plt.plot(target.max(axis=(1,2)), label='target')\n",
    "        plt.plot(img.max(axis=(1,2)),  label='original')\n",
    "        plt.plot(psf_corrected.max(axis=(1,2)), label='corrected', )\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "mse = MeanSquaredError(reduction='sum')\n",
    "def loss_func(true_m, pred_m):\n",
    "    m = tf.math.abs(true_m-pred_m)\n",
    "    m = tf.math.square(m*true_m)\n",
    "    return tf.math.reduce_mean(m)\n",
    "\n",
    "def tf_find_optimal_roll(target, img, upscale_ratio=UPSCALE_RATIO):\n",
    "    ref_tf = tf.convert_to_tensor(target)\n",
    "    img_tf = tf.convert_to_tensor(img)\n",
    "    errors = []\n",
    "\n",
    "    for i in range(img.shape[0]):\n",
    "#         error = loss_func(ref_tf, img_tf)\n",
    "        error = mse(ref_tf, img_tf)\n",
    "        errors.append(error)\n",
    "        img_tf = tf.roll(img_tf, 1, axis=0)\n",
    "\n",
    "    best_i = tf.argmin(errors).numpy()\n",
    "    # Prefer small backwards roll to large forwards roll\n",
    "    if abs(best_i - img.shape[0]) < best_i:\n",
    "        best_i = best_i - img.shape[0]\n",
    "\n",
    "    psf_corrected = np.roll(img, int(best_i), axis=0)\n",
    "    plot_correction(target, img, psf_corrected, errors)\n",
    "\n",
    "    return best_i/upscale_ratio\n",
    "\n",
    "\n",
    "def prepare_psf(psf):\n",
    "#     psf = gaussian(psf, sigma=1)\n",
    "    psf = psf.copy()\n",
    "    psf = psf * psf\n",
    "    psf = norm_zero_one(psf.copy())\n",
    "    psf = upsample_psf(psf)\n",
    "    # psf = mask_img_stack(psf, 12)\n",
    "    return psf\n",
    "\n",
    "\n",
    "def align_psfs(psf, psf2):\n",
    "    psf = prepare_psf(psf)\n",
    "    psf2 = prepare_psf(psf2)\n",
    "    psf = match_histograms(psf, psf2)\n",
    "    offset = tf_find_optimal_roll(psf, psf2)\n",
    "    return offset * z_step\n",
    "\n",
    "offsets = np.zeros((df.shape[0], df.shape[0]))\n",
    "offsets[:] = None\n",
    "\n",
    "src_node = 0\n",
    "target_node = center_point.name\n",
    "all_offsets = []\n",
    "def get_path_offset(G, src_node, target_node):\n",
    "    spath = nx.shortest_path(G, src_node, target_node)\n",
    "    if not np.isnan(offsets[src, target]):\n",
    "        cumul = offsets[src, target]\n",
    "    else:\n",
    "        cumul = 0\n",
    "        for i in range(0, len(spath)-1):\n",
    "            a, b = spath[i], spath[i+1]\n",
    "            if not np.isnan(offsets[a, b]):\n",
    "                offset = offsets[a, b]\n",
    "            else:\n",
    "                offset = align_psfs(psfs[a], psfs[b])\n",
    "                offsets[a, b] = offset\n",
    "                offsets[b, a] = -offset\n",
    "                diff = (df['roll'][a] - df['roll'][b])\n",
    "                print(f'{a} -> {b}: {offsets[a, b]}, {diff}')\n",
    "            \n",
    "            cumul += offset\n",
    "        offsets[src_node, target_node] = cumul\n",
    "        offsets[target_node, src_node] = -cumul\n",
    "    all_offsets.append(cumul)\n",
    "    \n",
    "for i in trange(0, df.shape[0]):\n",
    "    if i == target_node:\n",
    "        all_offsets.append(0)\n",
    "        continue\n",
    "    get_path_offset(G, i, target_node)\n",
    "all_offsets = np.array(all_offsets)\n",
    "\n",
    "# def roll_psf(psf, roll):\n",
    "#     rolled_psf = np.roll(psf, shift=roll, axis=0)\n",
    "# #     if roll < 0:\n",
    "# #         rolled_psf[roll:] = 0\n",
    "# #     else:\n",
    "# #         rolled_psf[:roll] = 0\n",
    "#     return rolled_psf\n",
    "# rolls = df['roll']\n",
    "# print(rolls[5], rolls[6])\n",
    "# psf = psfs[5]\n",
    "# rolled_psf = roll_psf(psf, 10)\n",
    "# print(align_psfs(psf, rolled_psf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aaad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classic_offsets.shape)\n",
    "print(all_offsets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1f8763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "plt.scatter(df['roll'], -classic_offsets-150, label='classic')\n",
    "plt.scatter(df['roll'], all_offsets-150, label='new')\n",
    "plt.plot(df['roll'], df['roll'], c='red', label='1')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(round(mean_absolute_error(df['roll'], -classic_offsets-150), 5))\n",
    "print(round(mean_absolute_error(df['roll'], all_offsets-150), 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f7245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "co = -classic_offsets*50\n",
    "sns.scatterplot(data=df, x='x', y='y', hue=co-co.min())\n",
    "plt.title('Offsets [nm]')\n",
    "plt.xlabel('x [nm]')\n",
    "plt.ylabel('y [nm]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dc6593",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = df[['x', 'y']].to_numpy()\n",
    "target_node_coords = coords[69]\n",
    "dists = euclidean_distances(coords, [[0, 0]])\n",
    "plt.scatter(df['x'], co-co.min())\n",
    "plt.ylabel('offset [nm]')\n",
    "plt.xlabel('x [nm]')\n",
    "plt.show()\n",
    "plt.scatter(df['y'], co-co.min())\n",
    "plt.ylabel('offset [nm]')\n",
    "plt.xlabel('y [nm]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09bc225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "for path in tri.simplices:\n",
    "    nx.add_path(G, path)\n",
    "\n",
    "from itertools import combinations\n",
    "for src, target in G.edges:\n",
    "    G[src][target]['weight'] = dists[src, target]\n",
    "\n",
    "edge_weights = []\n",
    "for edge in G.edges():\n",
    "    src, target = edge\n",
    "    edge_weights.append(offsets[src-1, target-1])\n",
    "    \n",
    "nx.draw(G, pos=df[['x', 'y']].values, edge_color=edge_weights, width=edge_weights, with_labels=True, node_size=100, node_color='lightgreen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7199eb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_step = 50\n",
    "psf_z = np.arange(0, psfs.shape[1]) * z_step\n",
    "\n",
    "psf2_z = (np.arange(0, psfs.shape[1]) * z_step) + overall_align\n",
    "\n",
    "all_z = np.concatenate((psf_z, psf2_z))\n",
    "all_psfs = np.concatenate((psfs[src_node], psfs[target_node]))\n",
    "idx = np.argsort(all_z)\n",
    "all_psfs = all_psfs[idx]\n",
    "\n",
    "def norm_zero_one(psf):\n",
    "    return (psf - psf.min()) / (psf.max() - psf.min())\n",
    "\n",
    "all_psfs = np.stack([norm_zero_one(p) for p in all_psfs])\n",
    "\n",
    "from data.visualise import grid_psfs\n",
    "\n",
    "plt.imshow(grid_psfs(norm_zero_one(psfs[src_node])))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.imshow(grid_psfs(norm_zero_one(psfs[target_node])))\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(grid_psfs(all_psfs))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8034e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyotf\n",
    "from tifffile import imread, imwrite\n",
    "\n",
    "psf = imread('/home/miguel/Projects/uni/phd/smlm_z/test/psfs/20220506_Miguel_beads_zeiss_training_3_beads.tif')[0]\n",
    "\n",
    "from pyotf.otf import HanserPSF\n",
    "from pyotf.phaseretrieval import retrieve_phase\n",
    "from pyotf.zernike import zernike\n",
    "\n",
    "model_kwargs = dict(\n",
    "    wl=0.647,\n",
    "    na=1.3,\n",
    "    ni=1.33,\n",
    "    res=0.106,\n",
    "    size=psf.shape[1],\n",
    "    zsize=psf.shape[0],\n",
    "    zres=0.01,\n",
    "    vec_corr=\"none\",\n",
    "    condition=\"none\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f64dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(psf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130c5fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PR_result = retrieve_phase(\n",
    "    psf, model_kwargs, max_iters=200, pupil_tol=0, mse_tol=0, phase_only=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb87c264",
   "metadata": {},
   "outputs": [],
   "source": [
    "PR_result.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32c964a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PR_result.plot()\n",
    "PR_result.plot_convergence()\n",
    "PR_result.fit_to_zernikes(64)\n",
    "PR_result.zd_result.plot()\n",
    "PR_result.zd_result.plot_named_coefs()\n",
    "PR_result.zd_result.plot_coefs()\n",
    "\n",
    "from pyotf.otf import apply_aberration\n",
    "result_psf = HanserPSF(**model_kwargs)\n",
    "result_psf = apply_aberration(result_psf, PR_result.zd_result.mcoefs, PR_result.zd_result.pcoefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8bb569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data.visualise import show_psf_axial\n",
    "psfs = np.concatenate((psf, result_psf.PSFi), axis=2)\n",
    "show_psf_axial(psfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513fa384",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from data.datasets import TrainingPicassoDataset\n",
    "from config.datasets import dataset_configs\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "\n",
    "cfg = dataset_configs['paired_bead_stacks']['training_1']\n",
    "\n",
    "locs = cfg['bpath'] / cfg['locs']\n",
    "df = pd.read_hdf(locs, 'locs')\n",
    "center = df[['x', 'y']].mean(axis=0).to_numpy()\n",
    "coords = df[['x', 'y']].to_numpy()\n",
    "dists = euclidean_distances([center], coords).squeeze()\n",
    "df['dist_to_center'] = dists\n",
    "\n",
    "first_point = np.argmin(dists)\n",
    "df['source'] = False\n",
    "df.loc[first_point, 'source'] = True\n",
    "\n",
    "dists = euclidean_distances(coords, coords)\n",
    "m = csr_matrix(dists)\n",
    "tree = minimum_spanning_tree(m).toarray()\n",
    "tree[tree>0] = 1\n",
    "edges = np.where(tree>0)\n",
    "\n",
    "abs_offsets = np.zeros((df.shape[0]))\n",
    "abs_offsets[first_point] = 10\n",
    "\n",
    "tree += tree.T\n",
    "print(tree)\n",
    "print(first_point)\n",
    "alignable_psfs = set(np.argwhere(tree[first_point, :] > 0).squeeze())\n",
    "while len(alignable_psfs):\n",
    "    unaligned_psf = alignable_psfs.pop()\n",
    "    known_offsets = set(np.argwhere(abs_offsets>0).flatten())\n",
    "    connected_points = set(np.argwhere(tree[unaligned_psf, :] > 0).flatten())\n",
    "    \n",
    "    target_psf = known_offsets.intersection(connected_points).pop()\n",
    "    abs_offsets[unaligned_psf] += abs_offsets[target_psf] + 1\n",
    "    alignable_psfs = alignable_psfs.union(set(np.argwhere(tree[:, unaligned_psf]).flatten()))\n",
    "    alignable_psfs = alignable_psfs.difference(np.argwhere(abs_offsets>0).flatten())\n",
    "\n",
    "print(abs_offsets)\n",
    "\n",
    "edges = np.where(tree>0)\n",
    "for i in range(edges[0].shape[0]):\n",
    "    src, target = edges[0][i], edges[1][i]\n",
    "    x = [coords[src][0], coords[target][0]]\n",
    "    y = [coords[src][1], coords[target][1]]\n",
    "    plt.plot(x, y, color='0')\n",
    "sns.scatterplot(data=df, x='x', y='y', hue=abs_offsets.astype(int))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437d4cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pyotf.otf import HanserPSF, apply_aberration, apply_named_aberration\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "\n",
    "from data.visualise import show_psf_axial, grid_psfs\n",
    "from data.align_psfs import align_psfs, tf_find_optimal_roll, mask_img_stack, norm_zero_one\n",
    "\n",
    "align_psfs.debug = False\n",
    "\n",
    "kwargs = dict(\n",
    "    wl=647,\n",
    "    na=1.3,\n",
    "    ni=1.51,\n",
    "    res=106,\n",
    "    zres=50,\n",
    "    size=32,\n",
    "    zsize=200,\n",
    "    vec_corr=\"none\",\n",
    "    condition=\"none\",\n",
    ")\n",
    "psf = HanserPSF(**kwargs)\n",
    "psf = apply_aberration(psf, np.array([0, 0, 0, 0, 0]), np.array([0, 0, 0, 0, 1]))\n",
    "\n",
    "blank_psf = psf.PSFi\n",
    "\n",
    "blank_psf = norm_zero_one(blank_psf) * 255\n",
    "\n",
    "from experiments.noise.noise_psf import EMCCD\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "e = EMCCD(noise_background=125)\n",
    "e.add_noise(np.random.randint(0, 255, size=(32,32)))\n",
    "\n",
    "psf = blank_psf[blank_psf.shape[0]//2]\n",
    "plt.imshow(psf)\n",
    "plt.show()\n",
    "plt.imshow(e.add_noise(psf))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57185e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from experiments.noise.noise_psf import generate_noisy_psf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "\n",
    "offset = 5\n",
    "rolled_psf = np.roll(blank_psf, offset, axis=0)\n",
    "\n",
    "rolled_psf = generate_noisy_psf(rolled_psf)\n",
    "from skimage.exposure import match_histograms\n",
    "\n",
    "psfs = np.stack((blank_psf, rolled_psf))\n",
    "plt.plot(psfs[0].max(axis=(1,2)), label='target')\n",
    "plt.plot(psfs[1].max(axis=(1,2)), label='psf')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "res = tf_find_optimal_roll(rolled_psf, blank_psf, 1)\n",
    "\n",
    "offsets = [200, 200-res]\n",
    "\n",
    "print('offsets', offsets)\n",
    "assert res == -offset\n",
    "\n",
    "imgs = np.concatenate(psfs)\n",
    "zs = []\n",
    "labels = ['target', 'psf']\n",
    "\n",
    "for i, o in enumerate(offsets):\n",
    "    vals = (np.arange(0, psfs[0].shape[0])*10) - (o*10)\n",
    "    print(vals[0:5])\n",
    "    plt.plot(vals, np.max(psfs[i], axis=(1,2)), label=labels[i])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a7951b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.estimate_offset import get_peak_sharpness\n",
    "\n",
    "\n",
    "fake_stacks = []\n",
    "offsets = []\n",
    "for offset in [0, 5, 10]:\n",
    "    offsets.append(offset)\n",
    "    rolled_psf = np.roll(blank_psf, offset, axis=0)\n",
    "#     rolled_psf = generate_noisy_psf(rolled_psf)\n",
    "    fake_stacks.append(rolled_psf)\n",
    "\n",
    "fake_stacks = np.array(fake_stacks)\n",
    "fake_stacks[fake_stacks<0] = 0\n",
    "\n",
    "pos0 = (get_peak_sharpness(fake_stacks[0]) * 50) * 2\n",
    "print(pos0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d0919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_offsets = align_psfs(fake_stacks)\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "\n",
    "print(corr_offsets)\n",
    "voxel_size = 50\n",
    "zs = []\n",
    "for offset, psf in zip(corr_offsets, fake_stacks):\n",
    "    z = ((np.arange(0, psf.shape[0]) - offset)  * voxel_size) -  pos0\n",
    "    print(z.min(), z.max())\n",
    "    zs.append(z)\n",
    "    plt.plot(z, psf.max(axis=(1,2)))\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be016c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.datasets import stack_offset_to_z\n",
    "zs = []\n",
    "for psf, offset in zip(fake_stacks, corr_offsets):\n",
    "    z = stack_offset_to_z(offset, psf, 10)\n",
    "    print(z.min(), z.max())\n",
    "    zs.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cc76a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect training dataset\n",
    "from data.visualise import grid_psfs\n",
    "plt.rcParams['figure.figsize'] = [50, 50]\n",
    "\n",
    "zs = np.concatenate(zs)\n",
    "imgs = np.concatenate(fake_stacks)\n",
    "idx = np.argsort(zs)\n",
    "imgs = imgs[idx]\n",
    "\n",
    "plt.imshow(grid_psfs(imgs.squeeze()))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383557ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def read_spots(dirpath):\n",
    "    f = h5py.File(dirpath, 'r')\n",
    "    spots = np.array(f['spots'])[:, :, :, np.newaxis]\n",
    "    f.close()\n",
    "    return spots\n",
    "\n",
    "dirpath = '/home/miguel/Projects/uni/data/smlm_3d/picasso_sim/grid_pairs/grid_pairs_spots.hdf5'\n",
    "test_spots = read_spots(dirpath)\n",
    "\n",
    "dirpath = '/home/miguel/Projects/uni/data/smlm_3d/picasso_test/training/NPC-A647-3D-BEADS/0021_spots.hdf5'\n",
    "\n",
    "train_spots = read_spots(dirpath)\n",
    "\n",
    "print(train_spots.shape)\n",
    "print(test_spots.shape)\n",
    "for spots in [train_spots, test_spots]:\n",
    "    print(spots.min(), spots.mean(), spots.std(), spots.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da715eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skimage.exposure import match_histograms\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [3, 3]\n",
    "train_spot = train_spots[0]\n",
    "\n",
    "mean_img = np.mean(train_spots, axis=0)\n",
    "\n",
    "plt.imshow(train_spot)\n",
    "plt.show()\n",
    "plt.imshow(mean_img)\n",
    "plt.show()\n",
    "plt.imshow(match_histograms(train_spot, mean_img))\n",
    "plt.show()\n",
    "\n",
    "print(train_spot.min(), train_spot.max())\n",
    "\n",
    "print(mean_img.min(), mean_img.max())\n",
    "\n",
    "print(match_histograms(train_spot, mean_img).min(), match_histograms(train_spot, mean_img).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a99aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.exposure import equalize_hist\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "\n",
    "mean_img = np.mean(train_spots, axis=0)\n",
    "\n",
    "for i in range(0, 10):\n",
    "    show_imgs(train_spots[i], match_histograms(train_spots[i], mean_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04767c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from data.datasets import TrainingPicassoDataset\n",
    "from config.datasets import dataset_configs\n",
    "\n",
    "cfg = dataset_configs['paired_bead_stacks']['training_1']\n",
    "print(cfg)\n",
    "\n",
    "ds = TrainingPicassoDataset(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867c8874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "radius = 107\n",
    "n_elements = 8\n",
    "z_depths = [0, 200]\n",
    "offset = (0, 0)\n",
    "\n",
    "angles = [i*((np.pi*2)/n_elements) for i in range(n_elements)]\n",
    "x = np.array([np.cos(a)*radius for a in angles])\n",
    "y = np.array([np.sin(a)*radius for a in angles])\n",
    "\n",
    "x += offset[0]\n",
    "y += offset[0]\n",
    "\n",
    "xs = [round(n, 3) for n in np.concatenate((x, x))]\n",
    "ys = [round(n, 3) for n in np.concatenate((y, y))]\n",
    "zs = [round(n, 3) for n in (sorted(z_depths*n_elements))]\n",
    "\n",
    "print(len(xs), len(ys), len(zs))\n",
    "\n",
    "plt.scatter(xs, ys)\n",
    "plt.show()\n",
    "\n",
    "print(xs)\n",
    "print(ys)\n",
    "print(zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3463a9e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset ='picasso_test'\n",
    "BOUND = 31\n",
    "\n",
    "from model.model import load_trained_model\n",
    "\n",
    "\n",
    "model = load_trained_model(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860c14d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fea54b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "--------\n",
    "|    x |   <- 50nm deeper than other\n",
    "|      |\n",
    "| x    |\n",
    "--------\n",
    "\n",
    "5 structures, frame 16 px,\n",
    "structureX: 1000,3000\n",
    "structureY: 1000,3000\n",
    "\n",
    "structure3D: 0,50\n",
    "ExchangeLabels:1,1\n",
    "'''\n",
    "dirpath = '/home/miguel/Projects/uni/data/smlm_3d/picasso_sim/50nm/50nm'\n",
    "\n",
    "# dirpath = '/home/miguel/Projects/uni/data/smlm_3d/picasso_sim/grid_single/grid_single'\n",
    "# dirpath = '/home/miguel/Projects/uni/data/smlm_3d/picasso_sim/grid_pairs/grid_pairs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ada6b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "yaml_file = f'{dirpath}.yaml'\n",
    "\n",
    "class SafeLoaderIgnoreUnknown(yaml.SafeLoader):\n",
    "    def ignore_unknown(self, node):\n",
    "        return None \n",
    "\n",
    "SafeLoaderIgnoreUnknown.add_constructor(None, SafeLoaderIgnoreUnknown.ignore_unknown)\n",
    "\n",
    "with open(yaml_file, \"r\") as stream:\n",
    "    root = yaml.load(stream, Loader=SafeLoaderIgnoreUnknown)\n",
    "with open(yaml_file, \"w\") as stream:\n",
    "    yaml.dump(root, stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd55e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "\n",
    "def norm_zero_one(img):\n",
    "    img_max = img.max()\n",
    "    img_min = img.min()\n",
    "    return (img - img_min) / (img_max - img_min)\n",
    "\n",
    "def norm_one_one(img):\n",
    "    return (2 * norm_zero_one(img)) - 1 \n",
    "\n",
    "locs = f'{dirpath}_locs.hdf5'\n",
    "spots = f'{dirpath}_spots.hdf5'\n",
    "print(locs)\n",
    "print(spots)\n",
    "df = pd.read_hdf(locs, 'locs')\n",
    "\n",
    "f = h5py.File(spots, 'r')\n",
    "spots = np.array(f['spots'])[:, :, :, np.newaxis]\n",
    "\n",
    "df['id'] = np.arange(0, df.shape[0])\n",
    "print(spots.shape)\n",
    "print(df.shape, spots.shape)\n",
    "f.close()\n",
    "\n",
    "from data.visualise import grid_psfs\n",
    "# plt.imshow(grid_psfs(spots.squeeze()))\n",
    "# plt.show()\n",
    "\n",
    "assert df.shape[0] == spots.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "from data.datasets import norm_dataset_from_config, standardise, load_ref_img_and_norm\n",
    "\n",
    "print(spots.shape)\n",
    "print(spots.min(), spots.max())\n",
    "# spots = standardise(spots)\n",
    "# spots = load_ref_img_and_norm(spots)\n",
    "for s in spots[0:5]:\n",
    "    print(s.min(), s.max())\n",
    "    \n",
    "spots = np.stack([norm_one_one(img) for img in spots])\n",
    "\n",
    "print(spots.min(), spots.max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377d472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 20]\n",
    "plt.imshow(grid_psfs(spots[0:50].squeeze()).squeeze())\n",
    "plt.show()\n",
    "\n",
    "mean_img = spots[1]\n",
    "\n",
    "spots_matched_hist = np.stack([match_histograms(img, mean_img) for img in spots])\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "plt.imshow(mean_img)\n",
    "plt.show()\n",
    "plt.rcParams['figure.figsize'] = [20, 20]\n",
    "plt.imshow(grid_psfs(spots_matched_hist[0:50].squeeze()).squeeze())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec02f556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rescale to [-1, 1]\n",
    "# spots = np.stack([norm_one_one(img) for img in spots])\n",
    "\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "df['index'] = np.arange(0, df.shape[0])\n",
    "print(spots.shape)\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "sns.scatterplot(data=df, x='x', y='y', marker='+')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "coords = np.zeros((spots.shape[0], 2))\n",
    "\n",
    "pred = model.predict((spots_matched_hist, coords)).squeeze()\n",
    "print(np.std(pred))\n",
    "df['pred'] = pred\n",
    "print(pred.min(), pred.max())\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "sns.histplot(pred, bins=50)\n",
    "plt.xlabel('Z position (nm)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sub_imgs = spots[0:100]\n",
    "sub_preds = pred[0:100]\n",
    "plt.rcParams['figure.figsize'] = [100, 100]\n",
    "from data.visualise import grid_psfs\n",
    "print(np.sort(sub_preds.squeeze()))\n",
    "\n",
    "plt.imshow(grid_psfs(sub_imgs[np.argsort(sub_preds.squeeze())].squeeze()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3abc15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.std(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03026e87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from config.datasets import dataset_configs\n",
    "\n",
    "from data.datasets import TrainingDataSet, ExperimentalDataSet, GenericDataSet, MultiTrainingDataset, TrainingPicassoDataset\n",
    "\n",
    "dataset = 'picasso_test'\n",
    "version = ''\n",
    "cfg = dataset_configs[dataset]['training']\n",
    "print(cfg)\n",
    "\n",
    "train_dataset = TrainingPicassoDataset(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc0966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.data['train'][0][0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98e91fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "\n",
    "test_imgs = spots.flatten()\n",
    "\n",
    "plt.hist(test_imgs, label='exp', alpha=0.5)\n",
    "\n",
    "for k in train_dataset.data.keys():\n",
    "    imgs = train_dataset.data[k][0][0].flatten()\n",
    "    print(imgs.min(), imgs.max())\n",
    "    plt.hist(imgs.flatten(), label=k, alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a281fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pixel_vals = np.mean(spots, axis=(1,2))\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "sns.kdeplot(mean_pixel_vals, pred)\n",
    "plt.xlabel('Mean pixel value')\n",
    "plt.ylabel('Pred')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9340bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "# df['emitter'] = df['x'] < 60\n",
    "df['pred'] = pred.squeeze()\n",
    "# df['pred2'] = df['pred']>-250\n",
    "df['snr'] = [np.max(s)/np.median(s) for s in spots]\n",
    "df['error'] = df['pred']+130\n",
    "sns.scatterplot(data=df, x='snr', y='pred')\n",
    "plt.show()\n",
    "sns.scatterplot(data=df, x='snr', y='error')\n",
    "plt.show()\n",
    "# sns.histplot(daata=df, x='pred', hue='emitter')\n",
    "# plt.show()\n",
    "\n",
    "# sns.scatterplot(data=df, x='x', y='y', hue='pred2')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a59394",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checks influence of polar coords on Z localisation\n",
    "\"\"\"\n",
    "\n",
    "# from itertools import product\n",
    "\n",
    "# plt.rcParams['figure.figsize'] = [5, 5]\n",
    "\n",
    "# spot = spots[0:1][:, :, :, np.newaxis]\n",
    "# thetas = np.linspace(0, 1, 20)\n",
    "# rhos = np.linspace(0, 1, 20)\n",
    "\n",
    "# coords = np.array(list(product(thetas, rhos))).squeeze()\n",
    "# spot = np.repeat(spot, coords.shape[0], axis=0)\n",
    "\n",
    "# preds = model.predict((spot, coords))\n",
    "\n",
    "# tmp_df = pd.DataFrame.from_dict({'theta': coords[:, 0], 'rho': coords[:, 1], 'z': preds.squeeze()})\n",
    "# sns.scatterplot(data=tmp_df, x='theta', y='z')\n",
    "# plt.show()\n",
    "# sns.scatterplot(data=tmp_df, x='rho', y='z')\n",
    "# plt.show()\n",
    "# sns.scatterplot(data=tmp_df, x='theta', y='rho', hue='z')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedead60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# from config.datasets import dataset_configs\n",
    "# from data.datasets import StormDataset, ExperimentalDataSet\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# dataset ='picasso_test'\n",
    "\n",
    "# cfg = dataset_configs[dataset]['nucleopore']\n",
    "# ds = StormDataset(cfg, normalize_psf=True, lazy=True, apply_clustering=False)\n",
    "# # ds.neighbour_radius = 15\n",
    "# # ds.max_off_frames = 1000\n",
    "# ds.csv_data = ds.csv_data[(ds.csv_data['x [nm]'] > 8250) \n",
    "#                           & (ds.csv_data['x [nm]'] < 8425) \n",
    "#                           & (ds.csv_data['y [nm]'] > 9425) \n",
    "#                           & (ds.csv_data['y [nm]'] < 9575) \n",
    "#                          ]\n",
    "\n",
    "# plt.rcParams['figure.figsize'] = [5, 5]\n",
    "# sns.scatterplot(data=ds.csv_data, x='x [nm]', y='y [nm]', marker='.')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# ds.prepare_data()\n",
    "\n",
    "# from scipy.ndimage import median_filter\n",
    "# ds.data[0] = np.stack([median_filter(d, size=2) for d in ds.data[0]])\n",
    "\n",
    "# df = ds.csv_data\n",
    "# print(df.shape)\n",
    "# print(ds.data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8a65a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "\n",
    "coords = df[['x', 'y']].to_numpy() * 100\n",
    "distance_matrix = euclidean_distances(coords, coords)\n",
    "eps = 15\n",
    "min_count = 50\n",
    "\n",
    "distance_matrix = (distance_matrix < eps).astype(int).sum(axis=0)\n",
    "idx = np.argwhere(distance_matrix > min_count).squeeze()\n",
    "sub_coords = coords[idx]\n",
    "print(coords.shape)\n",
    "print(sub_coords.shape)\n",
    "\n",
    "cluster_ids = DBSCAN(eps=eps, min_samples=min_count).fit_predict(sub_coords)\n",
    "sns.scatterplot(x=sub_coords[:, 0], y=sub_coords[:, 1], hue=cluster_ids.astype(str))\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "sub_df = df.iloc[idx]\n",
    "sub_df['cluster_id'] = cluster_ids.squeeze().astype(str)\n",
    "sub_spots = spots[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c989dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(model.predict((spots, np.zeros((spots.shape[0], 2)))).squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297b85b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN, OPTICS, KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import norm\n",
    "from data.visualise import grid_psfs\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20, 5]\n",
    "# df['cluster_id'] = OPTICS().fit_predict(df[['x', 'y']].to_numpy()).squeeze().astype(str)\n",
    "plt.axis('equal')\n",
    "plt.title('Units: pixels (90nm each)')\n",
    "sns.scatterplot(data=sub_df, x='x', y='y', marker='.', hue='cluster_id', legend=False)\n",
    "plt.show()\n",
    "\n",
    "n_components = 2\n",
    "\n",
    "gm_mean_diffs = {}\n",
    "mean_img_diffs = {}\n",
    "\n",
    "\n",
    "def gm_min_bic(data, imgs):\n",
    "    gm_df = pd.DataFrame({'pred': data.squeeze()}, index=np.arange(0, data.squeeze().shape[0]))\n",
    "\n",
    "    best_gm = None\n",
    "    min_bic = np.inf\n",
    "    bics = []\n",
    "    cov_type = 'tied'\n",
    "    stdevs = []\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 6)\n",
    "    for n in range(1, 7):\n",
    "        gm = GaussianMixture(n_components=n, n_init=20, covariance_type=cov_type).fit(data)\n",
    "        bic = gm.bic(data)\n",
    "        \n",
    "        bics.append(round(bic, 3))\n",
    "        stdevs.append(round(np.std(gm.weights_), 3))\n",
    "        if bic < min_bic:\n",
    "            min_bic = bic\n",
    "            best_gm = gm\n",
    "        \n",
    "        ax = axes[n-1]\n",
    "        labels = gm.predict(data).squeeze()\n",
    "\n",
    "        gm_df['cluster_id'] = labels.astype(str)\n",
    "\n",
    "        weights = gm.weights_\n",
    "\n",
    "        sns.histplot(data=gm_df, x='pred', hue='cluster_id', stat='density', alpha=0.2, bins=20, ax=ax)\n",
    "\n",
    "        # create necessary things to plot\n",
    "        x_axis = np.linspace(data.min(), data.max(), 50)\n",
    "        ys = []\n",
    "        sub_df2 = pd.DataFrame.from_dict({'x': x_axis})\n",
    "        for i in range(0, best_gm.n_components):\n",
    "            if cov_type == 'tied':\n",
    "                cov = gm.covariances_.squeeze()\n",
    "            elif cov_type == 'full' or cov_type == None:\n",
    "                cov = gm.covariances_[i][0][0]\n",
    "            elif cov_type == 'spherical':\n",
    "                cov = gm.covariances_[i]\n",
    "            elif cov_type == 'diag':\n",
    "                cov_type = gm.covariances_[i]\n",
    "\n",
    "            sub_df2[f'y_{i}'] = norm.pdf(x_axis, float(gm.means_[i][0]), np.sqrt(cov))*gm.weights_[i]\n",
    "            sns.lineplot(data=sub_df2, x='x', y=f'y_{i}', ax=ax)\n",
    "    plt.show()        \n",
    "        \n",
    "    print(bics)\n",
    "    print(stdevs)\n",
    "\n",
    "    return best_gm.means_[:, 0]\n",
    "\n",
    "def apply_gm(data, imgs, cid):\n",
    "    data = data.reshape(-1, 1)\n",
    "    gm_df = pd.DataFrame({'pred': list(data)})\n",
    "    \n",
    "    return gm_min_bic(data, imgs)\n",
    "    \n",
    "\n",
    "all_coords = []\n",
    "for cid in set(sub_df['cluster_id']):\n",
    "    if cid == '-1':\n",
    "        continue\n",
    "    idx = np.argwhere(sub_df['cluster_id'].to_numpy()==cid).squeeze()\n",
    "    imgs = sub_spots[idx][:, :, :, np.newaxis]\n",
    "    coords = np.zeros((imgs.shape[0], 2))\n",
    "    preds = model.predict((imgs, coords)).squeeze()\n",
    "    preds -= preds.min()\n",
    "    preds += 0.00000001\n",
    "    preds = np.sqrt(preds)\n",
    "    z_pos = apply_gm(preds, imgs, cid)\n",
    "    x, y = sub_df.iloc[idx][['x', 'y']].mean(axis=0).to_numpy() * 106\n",
    "    coords = [[x, y, z, int(cid)] for z in z_pos]\n",
    "    all_coords.extend(coords)\n",
    "\n",
    "all_coords = np.array(all_coords)\n",
    "res = pd.DataFrame.from_dict({\n",
    "    k: all_coords[:, i] for k, i in zip(['x', 'y', 'z', 'cluster_id'], [0, 1, 2, 3])\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b656523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = gm_mean_diffs\n",
    "tops = [np.max(v) for k, v in d.items()]\n",
    "bottoms = [np.min(v) for k, v in d.items()]\n",
    "print(np.mean(tops))\n",
    "print(np.mean(bottoms))\n",
    "print(np.mean(tops) - np.mean(bottoms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713b6dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "res['cluster_id'] = res['cluster_id'].astype(int)\n",
    "fig = plt.figure()\n",
    "fig.tight_layout() \n",
    "\n",
    "plt.subplots_adjust(wspace = 0.4) \n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "\n",
    "ax.scatter(res['x'], res['y'], res['z'], marker='o', c=res['cluster_id'])\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "ax2.scatter(res['cluster_id'], res['z'], c=res['cluster_id'])\n",
    "plt.xlabel('Cluster ID')\n",
    "plt.title('All units in nm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78083955",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spots.shape)\n",
    "print(coords.shape)\n",
    "all_coords = np.zeros((spots.shape[0], 2))\n",
    "all_spots = spots[:, :, :, np.newaxis]\n",
    "\n",
    "pred = model.predict((all_spots, all_coords))\n",
    "df['z'] = pred.squeeze()\n",
    "# for c in list(df):\n",
    "#     sns.scatterplot(data=df, x=c, y='z')\n",
    "#     plt.show()\n",
    "sns.scatterplot(data=df.iloc[0:1000], x='frame', y='z')\n",
    "plt.show()\n",
    "sns.histplot(data=df, x='z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1bec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_img = spots[0]\n",
    "\n",
    "recs = []\n",
    "\n",
    "imgs = []\n",
    "coords = []\n",
    "\n",
    "for _ in range(1000):\n",
    "    noise_loc = np.random.uniform(0, 0.8)\n",
    "    noise_scale = np.random.uniform(0, 0.7)\n",
    "    new_img = np.random.normal(loc=noise_loc, scale=noise_scale, size=best_img.shape)\n",
    "    new_img += best_img\n",
    "    new_img = norm_zero_one(new_img)\n",
    "    \n",
    "    new_img = norm_zero_one(new_img)\n",
    "    \n",
    "    new_img = np.array([new_img])\n",
    "    imgs.append(new_img)\n",
    "    coords.append(np.zeros((1, 2)))\n",
    "    \n",
    "    recs.append({\n",
    "        'img_mean': np.mean(new_img),\n",
    "        'img_median': np.median(new_img),\n",
    "        'noise_loc': noise_loc,\n",
    "        'noise_scale': noise_scale,\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame.from_dict(recs)\n",
    "\n",
    "imgs = np.concatenate(imgs)\n",
    "print(imgs.shape)\n",
    "coords = np.concatenate(coords)\n",
    "print(coords.shape)\n",
    "pred = model.predict((imgs, coords)).squeeze()\n",
    "err = pred\n",
    "df['err'] = err\n",
    "df['pred'] = pred\n",
    "for col in list(df):\n",
    "    sns.scatterplot(data=df, x=col, y='err')\n",
    "    plt.show()\n",
    "sns.scatterplot(data=df, x='noise_loc', y='noise_scale', hue='err')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e093e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45bde13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352086ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
