{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dbaa4b-3599-46ff-bc68-9b5648f19e19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_hdf('/media/Backup/smlm_z_data/20240625_NUP_ifluor647/FOV1/storm_1/storm_1_MMStack_Default.ome_locs_undrift.hdf5', key='locs')\n",
    "\n",
    "with h5py.File('/media/Backup/smlm_z_data/20240625_NUP_ifluor647/FOV1/storm_1/storm_1_MMStack_Default.ome_spots.hdf5') as f:\n",
    "    spots = np.array(f['spots'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ab648d-4ad9-4d76-8fbe-a266dba08036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import erf\n",
    "from skimage.transform import resize\n",
    "\n",
    "def mean_squared_error(x, y):\n",
    "    err = np.mean((x-y)**2)\n",
    "    return err\n",
    "\n",
    "def reduce_img(stack):\n",
    "    return stack.max(axis=(1,2))\n",
    "    \n",
    "def get_lat_fwhm(image, px_size_xy, debug=False, mse_thres=0.001):\n",
    "    def gaussian_2d(xy, amplitude, xo, yo, sigma_x, sigma_y, theta, offset):\n",
    "        x, y = xy\n",
    "        xo = float(xo)\n",
    "        yo = float(yo)\n",
    "        a = (np.cos(theta)**2) / (2 * sigma_x**2) + (np.sin(theta)**2) / (2 * sigma_y**2)\n",
    "        b = -(np.sin(2 * theta)) / (4 * sigma_x**2) + (np.sin(2 * theta)) / (4 * sigma_y**2)\n",
    "        c = (np.sin(theta)**2) / (2 * sigma_x**2) + (np.cos(theta)**2) / (2 * sigma_y**2)\n",
    "        g = offset + amplitude * np.exp(- (a * ((x - xo)**2) + 2 * b * (x - xo) * (y - yo) + c * ((y - yo)**2)))\n",
    "        return g.ravel()\n",
    "\n",
    "    # Load and preprocess the image (e.g., convert to grayscale)\n",
    "    # For simplicity, let's generate a simple image for demonstration\n",
    "    image_size = image.shape[1]\n",
    "    x = np.linspace(0, image_size - 1, image_size)\n",
    "    y = np.linspace(0, image_size - 1, image_size)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    \n",
    "    image = image / image.max()\n",
    "\n",
    "    # Fit the Gaussian to the image data\n",
    "    p0 = [1, image_size / 2, image_size / 2, 2, 2, 0, 0]  # Initial guess for parameters\n",
    "    bounds = [\n",
    "        (0, np.inf),\n",
    "        (image_size * (1/5), image_size * (4/5)),\n",
    "        (image_size * (1/5), image_size * (4/5)),\n",
    "        (0, image_size/3),\n",
    "        (0, image_size/3),\n",
    "        (-np.inf, np.inf),\n",
    "        (0, np.inf),\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        popt, pcov = curve_fit(gaussian_2d, (x, y), image.ravel(), p0=p0, bounds=list(zip(*bounds)))\n",
    "    except RuntimeError as e:\n",
    "        popt = p0\n",
    "    render = gaussian_2d((x, y), *popt).reshape(image.shape)\n",
    "\n",
    "    error = mean_squared_error(render, image)\n",
    "    # if error > mse_thres:\n",
    "    #     fwhm_x, fwhm_y =  np.nan, np.nan\n",
    "    # else:\n",
    "    amplitude, xo, yo, sigma_x, sigma_y, theta, offset = popt\n",
    "    f = 2 * np.sqrt(2 * np.log(2))\n",
    "    fwhm_x = sigma_x * f * px_size_xy\n",
    "    fwhm_y = sigma_y * f * px_size_xy\n",
    "    fwhm_xy = np.mean([fwhm_x, fwhm_y])\n",
    "\n",
    "\n",
    "    if debug:\n",
    "        plt.figure(figsize=(2,2))\n",
    "        print('FWHM x:', round(fwhm_x, 3), 'nm')\n",
    "        print('FWHM y:', round(fwhm_y, 3), 'nm')\n",
    "        print('MSE   :', '{:.2e}'.format(error))\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "    return fwhm_xy, error\n",
    "\n",
    "from tqdm import tqdm\n",
    "spots_fwhm = [get_lat_fwhm(x, 106) for x in tqdm(spots)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ac800-34a8-4485-9e53-536ee3d4ebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df.groupby('frame').mean(), x='frame', y='z [nm]', alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1584c53-2f42-4799-8bd3-194b296c0dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, Callback\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Sequential, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cdd63b-f428-454a-909f-5941d1a60e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'image_size': 32,\n",
    "    'architecture': 'mobilenet',\n",
    "    'dense1': 32,\n",
    "    'dense2': 32\n",
    "}\n",
    "\n",
    "imshape = (args['image_size'], args['image_size'], 1)\n",
    "img_input = Input(shape=imshape, name='img_input')\n",
    "gray_to_rgb = layers.Lambda(tf.image.grayscale_to_rgb, name='gray_to_rgb', output_shape=(args['image_size'], args['image_size'], 3))\n",
    "img = gray_to_rgb(img_input)\n",
    "\n",
    "\n",
    "coords_input = layers.Input((2,))\n",
    "x_coords = layers.Dense(64)(coords_input)\n",
    "\n",
    "x_coords = layers.Dense(64)(x_coords)\n",
    "\n",
    "model_version = {\n",
    "    'mobilenet': keras.applications.MobileNetV3Small,\n",
    "    'mobilenet_large': keras.applications.MobileNetV3Large,\n",
    "    'vgg': keras.applications.VGG19,\n",
    "    'resnet': keras.applications.ResNet50V2,\n",
    "    'resnet_large': keras.applications.ResNet101V2,\n",
    "}[args['architecture']]\n",
    "\n",
    "if 'vit_' in args['architecture']:\n",
    "    feat_model = model_version(image_size=args['image_size'], \n",
    "                            activation='sigmoid',\n",
    "                            pretrained=True,\n",
    "                            include_top=False,\n",
    "                            pretrained_top=False)\n",
    "else:\n",
    "    feat_model = model_version(input_shape=(args['image_size'], args['image_size'], 3),\n",
    "                              weights='imagenet',\n",
    "                              include_top=False)\n",
    "\n",
    "x = feat_model(img)\n",
    "# Add additional layers for regression prediction\n",
    "x = Flatten()(x)\n",
    "x = tf.concat([x, x_coords], axis=-1)\n",
    "x = Dense(args['dense1'], activation='gelu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "if args['dense2'] != 0:\n",
    "    x = Dense(args['dense2'], activation='gelu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "regression_output = Dense(1, activation='tanh')(x)  # Linear activation for regression\n",
    "model = Model(inputs=[img_input, coords_input], outputs=regression_output)\n",
    "\n",
    "# aug_model = Model(inputs=img_input, outputs=img_aug_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf349762-f2a6-45f8-9611-8e4a7efadcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tmp = np.random.uniform(size=(5, 32, 32, 1))\n",
    "coords = np.random.uniform(size=(5, 2))\n",
    "\n",
    "model((tmp, coords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc49a667-df49-492c-9763-9e052fadad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0,'../publication')\n",
    "\n",
    "from util.util import _apply_img_norm, get_model_report, get_model_img_norm\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "model_path = '/home/miguel/Projects/smlm_z/autofocus/VIT_zeiss_mobilenet_fov_max/out/'\n",
    "model = tf.keras.models.load_model(model_path + '/latest_vit_model3')\n",
    "\n",
    "model_report = get_model_report(model_path)\n",
    "img_norm = get_model_img_norm(model_report)\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.load('/home/miguel/Projects/smlm_z/autofocus/VIT_zeiss_mobilenet_fov_max/out/test/')\n",
    "from tifffile import imread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4460dc-63e8-43a9-817b-674d8aa0e662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "class RandomGaussianNoise(keras.layers.Layer):\n",
    "    def __init__(self, mean_range=(-0.1, 0.1), std_range=(0.0, 0.5), perc_chance = 0.5, **kwargs):\n",
    "        super(RandomGaussianNoise, self).__init__(**kwargs)\n",
    "        self.mean_range = mean_range\n",
    "        self.std_range = std_range\n",
    "        self.perc = perc_chance\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            stack_noise = []\n",
    "            # Draw random mean and std values from the specified ranges\n",
    "            means = tf.random.uniform(shape=(tf.shape(inputs)[0],), minval=self.mean_range[0], maxval=self.mean_range[1])\n",
    "            stds = tf.random.uniform(shape=(tf.shape(inputs)[0],), minval=self.std_range[0], maxval=self.std_range[1])\n",
    "            prob = tf.random.uniform(shape=(tf.shape(inputs)[0],), minval=1, maxval=0)\n",
    "            for i in range(inputs.shape[0]):\n",
    "                if self.perc >= prob[i]:\n",
    "                    noise = tf.random.normal(shape=tf.shape(inputs)[1:], mean=means[i], stddev=stds[i])\n",
    "                    noise = tf.math.floor(noise)\n",
    "                else:\n",
    "                    noise = tf.zeros(shape=tf.shape(inputs)[1:])\n",
    "                stack_noise.append(noise)\n",
    "            \n",
    "            output = inputs + tf.stack(stack_noise)\n",
    "            maxs = tf.math.reduce_max(output, axis=(1,2,3), keepdims=True)\n",
    "            return tf.nn.relu(output / maxs)\n",
    "        else:\n",
    "            return inputs\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"mean_range\": self.mean_range,\n",
    "            \"std_range\": self.std_range,\n",
    "            'perc_chance': self.perc_chance\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "\n",
    "def tamper_imgs(imgs):\n",
    "    l = RandomGaussianNoise(mean_range=(0.5, 1), std_range=(0.1, 0.3), perc_chance=.5)\n",
    "    imgs2 = l(imgs, training=True).numpy()\n",
    "    return imgs2\n",
    "\n",
    "def eval_imgs(s):\n",
    "    # s = apply_img_norm(s).numpy()\n",
    "\n",
    "    snrs = str(round(np.array([snr(x) for x in s]).mean(), 3))\n",
    "    # print(s.min(axis=(1,2,3)))\n",
    "    # print(s.max(axis=(1,2,3)))\n",
    "\n",
    "    plt.imshow(grid_psfs(s.mean(axis=-1)).T)\n",
    "    plt.title(snrs)\n",
    "    plt.show()\n",
    "    \n",
    "for s in stacks:\n",
    "    idx = np.argmax(s.max(axis=(1,2)))\n",
    "    s = s[idx-100:idx+100, :, :, np.newaxis]\n",
    "    s = apply_img_norm(s).numpy()\n",
    "    eval_imgs(s)\n",
    "    for _ in range(5):\n",
    "        imgs = tamper_imgs(s)\n",
    "        print(imgs.sum())\n",
    "        eval_imgs(imgs)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0202d75-f372-4502-91ad-6d06871298cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from data.visualise import grid_psfs\n",
    "from tensorflow.keras import Sequential, layers\n",
    "\n",
    "import os\n",
    "\n",
    "def norm_imgs(imgs, img_norm):\n",
    "    imgs_xy, z = _apply_img_norm((imgs, _), None, img_norm)\n",
    "    return imgs_xy[0].numpy()\n",
    "\n",
    "\n",
    "zrange = 1000\n",
    "n_images = 200\n",
    "\n",
    "\n",
    "def snr(img):\n",
    "    return img.max() / img.mean()\n",
    "\n",
    "\n",
    "args = {\n",
    "    'aug_gauss': 0.5,\n",
    "    'aug_brightness': 0.1,\n",
    "    'aug_poisson_lam': 1000,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "def tamper_imgs(imgs):\n",
    "    imgs2 = np.array(imgs)\n",
    "    for i in range(imgs.shape[0]):\n",
    "        noise = np.random.normal(0.01, 0.05, size=(*imgs.shape[1:3], 1))\n",
    "        imgs2[i] += noise\n",
    "        imgs2[imgs2<0] = 0\n",
    "    return imgs2\n",
    "    \n",
    "def eval_images(model, img_norm, imgs, xy, zs):\n",
    "    imgs = norm_imgs(imgs, img_norm, )\n",
    "    print(imgs.min(), imgs.max())\n",
    "\n",
    "    snrs = str(round(np.mean([snr(x) for x in imgs]), 3))\n",
    "    fig, axs = plt.subplots(1, 3)\n",
    "    z_pred = model.predict((imgs, xy)) * zrange\n",
    "    err = str(round(mean_absolute_error(z_pred, zs), 3))\n",
    "    plt.title(f'mae: {err}, snr: {snrs}')\n",
    "    axs[0].scatter(zs, z_pred)\n",
    "    axs[0].set_xlabel('True z (nm)')\n",
    "    axs[0].set_ylabel('Pred z (nm)')\n",
    "    axs[1].imshow(grid_psfs(imgs.mean(axis=-1)))\n",
    "    axs[2].plot(imgs.max(axis=(1,2,3)))\n",
    "    axs[2].plot(imgs.min(axis=(1,2,3)))\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "dataset = tf.data.Dataset.load('/home/miguel/Projects/smlm_z/autofocus/VIT_zeiss_mobilenet_fov_max/out/test/')\n",
    "\n",
    "\n",
    "for ((imgs, xy), zs) in dataset.as_numpy_iterator():\n",
    "    zs = np.array(zs) * zrange\n",
    "    zs = zs[:n_images]\n",
    "    imgs = imgs[:n_images]\n",
    "    xy = xy[:n_images]\n",
    "    # eval_images(imgs, xy, zs)\n",
    "    tampered_imgs = [tamper_imgs(imgs) for _ in range(5)]\n",
    "\n",
    "    for _ in range(2):\n",
    "        imgs2 = tamper_imgs(imgs)\n",
    "\n",
    "        # eval_images(imgs2, xy, zs)\n",
    "    break\n",
    "\n",
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'../publication')\n",
    "\n",
    "import gc\n",
    "from util.util import _apply_img_norm, get_model_report, get_model_img_norm\n",
    "\n",
    "\n",
    "\n",
    "from glob import glob\n",
    "models = glob('/home/miguel/Projects/smlm_z/autofocus/VIT_zeiss_mobilenet/out_standard_aug*')\n",
    "for model_path in models:\n",
    "    imgs_model = np.array(imgs)\n",
    "    tamp_imgs_model = np.array(tampered_imgs)\n",
    "    \n",
    "    model = tf.keras.models.load_model(os.path.join(model_path,  'latest_vit_model3'))\n",
    "    \n",
    "    model_report = get_model_report(model_path)\n",
    "    img_norm = get_model_img_norm(model_report)\n",
    "    print(img_norm)\n",
    "    eval_images(model, img_norm, imgs, xy, zs)\n",
    "\n",
    "    for tamp_imgs in tamp_imgs_model:\n",
    "        eval_images(model, img_norm, tamp_imgs, xy, zs)\n",
    "    del model\n",
    "    gc.collect()\n",
    "    print('------------'*10)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a57b410-65c3-43a8-82a9-e4960037300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tifffile import imread\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "stacks = imread('/home/miguel/Projects/smlm_z/autofocus/VIT_zeiss_resnet2/stacks.ome.tif')\n",
    "\n",
    "with h5py.File('/media/Backup/smlm_z_data/20240606_bacteria_Miguel_zeiss/15min_timelapse_20nm_red_beads_every_1sec_1/15min_timelapse_20nm_red_beads_every_1sec_1_MMStack_Default.ome_spots.hdf5') as f:\n",
    "    timelapse_spots = np.array(f['spots'])\n",
    "\n",
    "timelapse_locs = pd.read_hdf('/media/Backup/smlm_z_data/20240606_bacteria_Miguel_zeiss/15min_timelapse_20nm_red_beads_every_1sec_1/15min_timelapse_20nm_red_beads_every_1sec_1_MMStack_Default.ome_locs.hdf5', key='locs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154c8f7a-4098-484a-a8a0-79f70d7fe6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "bead_imgs = []\n",
    "for s in stacks:\n",
    "    peak = np.argmax(s.max(axis=(1,2)))\n",
    "    bead_imgs.append(s[peak-20:peak+20])\n",
    "\n",
    "bead_imgs = np.stack(bead_imgs)\n",
    "import h5py\n",
    "\n",
    "\n",
    "args = {\n",
    "    'baseline': 100,\n",
    "    'sensitivity': 0.45,\n",
    "    'gain': 1\n",
    "}\n",
    "\n",
    "timelapse_spots = (timelapse_spots * args['gain'] / args['sensitivity']) + args['baseline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ddd5de-a767-40a4-9f77-ccca2d5553cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "aug_imgs = []\n",
    "train_imgs = np.concatenate(bead_imgs)\n",
    "n_aug_points = train_imgs.shape[0]\n",
    "idx = np.random.randint(0, train_imgs.shape[0], size=n_aug_points)\n",
    "for i in tqdm(idx):\n",
    "    img = train_imgs[i]\n",
    "    noise_mean = np.random.uniform(0, 1.2)\n",
    "    img_range = img.max() - img.min()\n",
    "    noise = np.random.normal(img.max() * noise_mean, img_range * np.random.uniform(0.1, 0.7), size=img.shape)\n",
    "    aug_imgs.append(img+noise)\n",
    "\n",
    "aug_imgs = np.stack(aug_imgs)\n",
    "print(aug_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8510deec-bd4c-4aac-8cb0-b9c06a5533e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snr(img):\n",
    "    return img.max() / img.mean()\n",
    "\n",
    "stack_snrs = np.array([snr(x) for x in np.concatenate(bead_imgs)])\n",
    "aug_stack_snrs = np.array([snr(x) for x in aug_imgs])\n",
    "timelapse_snrs = np.array([snr(x) for x in timelapse_spots])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dec4591-0631-4712-8b71-401266a1a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 3))\n",
    "frame_bins = np.linspace(0, timelapse_locs['frame'].max(), 5)\n",
    "for i in range(len(frame_bins)-1):\n",
    "    f_min = int(frame_bins[i])\n",
    "    f_max = int(frame_bins[i+1])\n",
    "    idx = np.argwhere((f_min <= timelapse_locs['frame']) & (timelapse_locs['frame'] <= f_max)).squeeze()\n",
    "    sns.histplot(timelapse_snrs[idx], label=f'timelapse [{f_min}, {f_max}]', stat=\"probability\", alpha=0.5)\n",
    "\n",
    "sns.histplot(stack_snrs, label='stack', stat=\"probability\", alpha=0.5)\n",
    "sns.histplot(aug_stack_snrs, label='stack_aug', stat=\"probability\", alpha=0.5)\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.xlabel('SNR (img_max  / img_mean)')\n",
    "plt.title('SNR of timelapse at various intervals of frames vs training data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b20166-1782-4e5d-a630-93eb14042002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../publication')\n",
    "import tensorflow as tf\n",
    "\n",
    "from util.util import get_model_report, get_model_img_norm, show_psf_axial, _apply_img_norm\n",
    "\n",
    "def norm_imgs(imgs, img_norm):\n",
    "    imgs_xy, z = _apply_img_norm((imgs, _), None, img_norm)\n",
    "    return imgs_xy[0].numpy()\n",
    "    \n",
    "def snr(x):\n",
    "    print(x.shape)\n",
    "    if x.ndim == 2:\n",
    "        return x.max() / x.mean()\n",
    "    else:\n",
    "        return np.mean(x.max(axis=(1,2,3)) / x.mean(axis=(1,2,3)))\n",
    "\n",
    "def add_noise(x, mean):\n",
    "    noise = np.random.normal(x.max() * mean, (x.max()-x.min()) * np.random.uniform(0.1, 0.6), size=x.shape)\n",
    "    return x + noise\n",
    "\n",
    "def plot_ax_max(x, label, img_norm):\n",
    "    x = norm_imgs(x, img_norm)\n",
    "    maxs = x.mean(axis=(1,2))\n",
    "    show_psf_axial(x.mean(axis=-1))\n",
    "    \n",
    "test_stack = bead_imgs[0, :, :, :, np.newaxis]\n",
    "for img_norm in ['standard']:\n",
    "    plot_ax_max(test_stack, 'orig', img_norm)\n",
    "    for mean in np.linspace(0, 1, 5):\n",
    "        noise_img = add_noise(test_stack, mean)\n",
    "        plot_ax_max(noise_img, str(round(mean, 3)), img_norm)\n",
    "        \n",
    "    plt.legend()\n",
    "    plt.title(img_norm)\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f23d302-6b0c-47e3-bc4f-5ff71e489911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../publication')\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from util.util import get_model_report, get_model_img_norm\n",
    "\n",
    "\n",
    "def _apply_img_norm(img_xy, z, img_norm):\n",
    "    img_xy = list(img_xy)\n",
    "    imgs = img_xy[0]\n",
    "    if img_norm == 'frame-mean':\n",
    "        means = tf.math.reduce_mean(imgs, axis=(1,2,3), keepdims=True)\n",
    "        imgs -= means\n",
    "        maxs = tf.math.reduce_max(imgs, axis=(1,2,3), keepdims=True)\n",
    "        imgs = tf.nn.relu(imgs / maxs)\n",
    "    elif img_norm == 'frame-min':\n",
    "        mins = tf.math.reduce_min(imgs, axis=(1,2,3), keepdims=True)\n",
    "        imgs -= mins\n",
    "        maxs = tf.math.reduce_max(imgs, axis=(1,2,3), keepdims=True)\n",
    "        imgs = tf.nn.relu(imgs / maxs)\n",
    "    elif img_norm == 'frame-max':\n",
    "        maxs = tf.math.reduce_max(imgs, axis=(1,2,3), keepdims=True)\n",
    "        imgs = imgs / maxs\n",
    "    elif img_norm == 'fov-max':\n",
    "        maxs = tf.math.reduce_max(imgs)\n",
    "        imgs = tf.nn.relu(imgs / maxs)\n",
    "    elif img_norm == 'fov-minmax':\n",
    "        maxs = tf.math.reduce_max(imgs)\n",
    "        mins = tf.math.reduce_min(imgs)\n",
    "        imgs = (imgs - mins) / (maxs-mins)\n",
    "    elif img_norm == 'standard':\n",
    "        mean = tf.math.reduce_mean(imgs)\n",
    "        std = tf.math.reduce_std(imgs)\n",
    "        imgs = (imgs - mean) / std\n",
    "    else:\n",
    "        print(f'img_norm: {img_norm} not supported')\n",
    "        raise NotImplementedError()\n",
    "    return (imgs, img_xy[1]), z\n",
    "    \n",
    "def norm_imgs(imgs, img_norm):\n",
    "    imgs_xy, z = _apply_img_norm((imgs, _), None, img_norm)\n",
    "    return imgs_xy[0].numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd58017-f1c8-4e92-aa25-bb7b0bdcc0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def sample(c):\n",
    "    idx = np.arange(c.shape[0])\n",
    "    idx = np.random.choice(idx, size=1000)\n",
    "    sample = c[idx, :, :, np.newaxis]\n",
    "    return sample\n",
    "\n",
    "for norm in ['frame-min', 'frame-mean', 'frame-max', 'fov-max', 'fov-minmax', 'standard']:\n",
    "    for label, c in (('beads', sample(bead_imgs)), ('timelapse', sample(timelapse_spots))):\n",
    "        norm_c = norm_imgs(np.array(c), norm)\n",
    "        sns.histplot(norm_c.flatten(), label=label, stat=\"probability\")\n",
    "    plt.title(norm)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bce72a0-064a-4b90-909d-86922315e93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for norm in reversed(['frame-min', 'frame-mean', 'frame-max', 'fov-max', 'fov-minmax', 'standard']):\n",
    "    outdir = f'./out_{norm}'\n",
    "    print(f'python3 ../../publication/train_model.py -o {outdir} --architecture=mobilenet --aug-brightness=0 --aug-gauss=0 --aug-poisson-lam=0 --batch_size=4096 --dataset=20240603_Miguel_Zeiss_Stacks --dense1=256 --dense2=32 --learning_rate=0.0001 --system=zeiss --project=autofocus --zrange=1000 --norm {norm};')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8413cec-ce75-42a3-a4e1-b23fc1c8b327",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    ('20230601_MQ_celltype_beads', 20, '/home/miguel/Projects/data/20230601_MQ_celltype/20230601_MQ_celltype_beads/combined/stacks.ome.tif'),\n",
    "    ('20231020_20nm_beads_10um_range_10nm_step', 10, '/home/miguel/Projects/data/20231020_20nm_beads_10um_range_10nm_step/combined/stacks.ome.tif'),\n",
    "    ('20231128_tubulin_miguel', 10, '/home/miguel/Projects/data/20231128_tubulin_miguel/combined/stacks.ome.tif'),\n",
    "    ('20231205_miguel_mitochondria', 10, '/home/miguel/Projects/data/all_openframe_beads/20231205_miguel_mitochondria/combined/stacks.ome.tif'),\n",
    "    ('20231212_miguel_openframe', 10, '/home/miguel/Projects/data/all_openframe_beads/20231212_miguel_openframe/combined/stacks.ome.tif'),\n",
    "    ('20240510_Miguel_beads_20nm', 10, '/media/Backup/smlm_z_data/20240510_Miguel_beads/zstacks/20nm_red/stacks/combined/stacks.ome.tif'),\n",
    "    ('20240510_Miguel_beads_100nm', 10, '/media/Backup/smlm_z_data/20240510_Miguel_beads/zstacks/100nm_tetraspeck/beads/combined/stacks.ome.tif'),\n",
    "]\n",
    "from tifffile import imread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3502d5d0-08c9-441b-a7cf-66aa60876738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "stacks = imread(datasets[-1][2])\n",
    "print(stacks.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3831e52a-43f5-4a71-93a0-7923e559f467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "def get_central_bead_idx(df):\n",
    "    df['dist'] = euclidean_distances(df[['x', 'y']].to_numpy(), [[df['x'].max()/2, df['y'].max()/2]])\n",
    "    ref_idx = np.argsort(df['dist'].to_numpy())[1]\n",
    "    return ref_idx\n",
    "\n",
    "stack_size = 400\n",
    "def subsample_stack(s):\n",
    "    inten = s.max(axis=(1,2))\n",
    "    peak = np.argmax(inten)\n",
    "    half_stack = int(stack_size//2)\n",
    "    return s[peak-half_stack:peak+half_stack]\n",
    "\n",
    "def resize_side_profile(img):\n",
    "    return resize(img, (img.shape[0]/4, 30))\n",
    "        \n",
    "def gen_xyz_profiles(s):\n",
    "\n",
    "    s = subsample_stack(s)\n",
    "\n",
    "    print('Stack shape', s.shape)\n",
    "    fig, axs = plt.subplots(1, 3, layout='constrained', figsize=(10, 4))\n",
    "    \n",
    "    # xy view\n",
    "    img = s.sum(axis=(0))\n",
    "    axs[0].imshow(img)\n",
    "    axs[0].set_title('XY')\n",
    "    \n",
    "    # zy\n",
    "    img = s.sum(axis=1)\n",
    "    axs[1].imshow(resize_side_profile(img))\n",
    "    axs[1].set_title('ZY')\n",
    "\n",
    "    # zx\n",
    "    img = s.sum(axis=2)\n",
    "    axs[2].imshow(resize_side_profile(img))\n",
    "    axs[2].set_title('ZX')\n",
    "    plt.show()\n",
    "\n",
    "for stack in [\n",
    "    '/home/miguel/Projects/data/all_openframe_beads/20231205_miguel_mitochondria/combined/stacks.ome.tif',\n",
    "    '/media/Backup/smlm_z_data/20240510_Miguel_beads/zstacks/20nm_red/stacks/combined/stacks.ome.tif'\n",
    "    ]:\n",
    "    print(stack)\n",
    "    locs = stack.replace('stacks.ome.tif', 'locs.hdf')\n",
    "    locs = pd.read_hdf(locs, key='locs')\n",
    "    idx = get_central_bead_idx(locs)\n",
    "    gen_xyz_profiles(imread(stack)[idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207ec651-e1f8-40c9-ba2a-3129d6df6612",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = stacks.max(axis=(2,3))\n",
    "print(s.shape)\n",
    "for x in s[0:50]:\n",
    "    plt.plot(norm_zero_one(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa79aeed-7ade-42df-a304-f9f6e851fcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "def get_fwhm(s):\n",
    "    profile = s.max(axis=(1,2))\n",
    "    profile -= min(profile)\n",
    "    max_val = max(profile)\n",
    "    half_max = max(profile) / 2\n",
    "\n",
    "    above_hm = np.argwhere(profile>half_max)\n",
    "    first, last = above_hm[0], above_hm[-1]\n",
    "    fwhm_px = (last - first)\n",
    "    # plt.plot(np.arange(profile.shape[0]) * 10, profile)\n",
    "    # plt.show()\n",
    "    # print(first*10, last*10, fwhm)\n",
    "    return fwhm_px\n",
    "    \n",
    "    \n",
    "\n",
    "data_fwhms = []\n",
    "for _, zstep, fpath in datasets:\n",
    "    ds = imread(fpath)\n",
    "    fwhms = []\n",
    "    for s in tqdm(ds):\n",
    "        fwhms.append(get_fwhm(s) * zstep)\n",
    "    fwhms = np.array(fwhms).squeeze()\n",
    "    data_fwhms.append(fwhms)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c4f848-8325-4131-89b9-47296f55ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_fwhms[0].shape)\n",
    "plt.boxplot(data_fwhms, labels=[x[0] for x in datasets])\n",
    "plt.title('FWHM measured on cut-out bead stacks from various datasets')\n",
    "plt.ylabel('FWHM (z) in nm')\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d2bb45-c2cd-4915-88d1-707a03e87c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tifffile import imread\n",
    "import pandas as pd\n",
    "from sklearn.metrics import euclidean_distances\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.metrics import mean_squared_error\n",
    "from tqdm import trange\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "import tensorflow as tf\n",
    "\n",
    "stacks = imread('/media/Backup/smlm_z_data/20240510_Miguel_beads/zstacks/20nm_red/stacks/combined/stacks.ome.tif')\n",
    "locs = pd.read_hdf('/media/Backup/smlm_z_data/20240510_Miguel_beads/zstacks/20nm_red/stacks/combined/locs.hdf', key='locs')\n",
    "\n",
    "def norm_zero_one(s):\n",
    "    max_s = s.max()\n",
    "    min_s = s.min()\n",
    "    return (s - min_s) / (max_s - min_s)\n",
    "\n",
    "\n",
    "def realign_beads(psfs, df, z_step, i=0):\n",
    "    from sklearn.metrics import euclidean_distances\n",
    "    ref_idx = get_central_bead_idx(df, i)\n",
    "    ref_offset = df.iloc[ref_idx]['offset']\n",
    "\n",
    "    ref_psf = norm_zero_one(psfs[ref_idx])\n",
    "    ref_tf = tf.convert_to_tensor(ref_psf)\n",
    "    rolls = []\n",
    "    for idx in trange(df.shape[0]):\n",
    "        if idx == ref_idx:\n",
    "            roll = 0\n",
    "        else:\n",
    "            psf2 = norm_zero_one(psfs[idx])\n",
    "            roll = -tf_find_optimal_roll(ref_tf, psf2)\n",
    "        rolls.append(roll)\n",
    "    rolls = np.array(rolls)\n",
    "    df['offset'] = rolls * z_step\n",
    "    df['offset'] += ref_offset\n",
    "    return psfs, df\n",
    "\n",
    "\n",
    "\n",
    "def tf_eval_roll(ref_psf, psf, roll):\n",
    "    return tf.reduce_mean(mean_squared_error(ref_psf, tf.roll(psf, roll, axis=0)))\n",
    "    \n",
    "def tf_find_optimal_roll(ref_tf, img):\n",
    "    \n",
    "    img_tf = tf.convert_to_tensor(img)\n",
    "\n",
    "    roll_range = ref_tf.shape[0]//4\n",
    "    rolls = np.arange(-roll_range, roll_range).astype(int)\n",
    "    errors = tf.map_fn(lambda roll: tf_eval_roll(ref_tf, img_tf, roll), rolls, dtype=tf.float64)\n",
    "    # idx = 0\n",
    "    # for roll in tqdm(rolls):\n",
    "    #     error = tf.eval_roll(ref_tf, img_tf, roll)\n",
    "    #     print(i, error)\n",
    "    #     errors[idx] = error\n",
    "    #     idx += 1\n",
    "\n",
    "    best_roll = rolls[tf.argmin(errors).numpy()]\n",
    "    # Prefer small backwards roll to large forwards roll\n",
    "    if abs(best_roll - img.shape[0]) < best_roll:\n",
    "        best_roll = best_roll - img.shape[0]\n",
    "\n",
    "    return best_roll\n",
    "\n",
    "def get_central_bead_idx(df, i):\n",
    "    df['dist'] = euclidean_distances(df[['x', 'y']].to_numpy(), [[df['x'].max()/2, df['y'].max()/2]])\n",
    "    ref_idx = np.argsort(df['dist'].to_numpy())[i]\n",
    "    print(ref_idx)\n",
    "    return ref_idx\n",
    "\n",
    "locs_sets = [realign_beads(stacks, locs.copy(deep=True), 10, i)[1] for i in range(5)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fed6b90-711e-4068-98ac-6cfe9ba8d375",
   "metadata": {},
   "outputs": [],
   "source": [
    "locs_sets2 = [realign_beads(stacks, locs.copy(deep=True), 10, i)[1] for i in range(5, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ff6ecc-fdca-4a38-8e03-e8ce7efec5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "locs_sets[0]['offset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b76f740-f7f1-4baa-8a18-9ca9da68bb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets = np.array([locs_set['offset'].to_numpy() for locs_set in locs_sets+locs_sets2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c76ebd-6719-4164-87bc-ff1202712834",
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094382a4-0280-4154-bf89-c3d4171665c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for o in offsets:\n",
    "    o2 = o[0:10]\n",
    "    o2 -= o2[0]\n",
    "    plt.scatter(np.arange(o2.shape[0]), o2, alpha=0.1)\n",
    "# plt.show()\n",
    "\n",
    "mean_offsets = np.mean(offsets, axis=0)\n",
    "mean_offsets -= mean_offsets[0]\n",
    "plt.scatter(np.arange(mean_offsets.shape[0]), mean_offsets, marker='+')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d5cf03-8705-4b69-b039-7857cedcb2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_offsets = np.mean(offsets, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9729fc9b-6971-4724-8a86-875d33bdf0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "locs['offset'] = mean_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db318696-6cb9-4f64-b47d-a94c9f4c0007",
   "metadata": {},
   "outputs": [],
   "source": [
    "locs.to_hdf('/media/Backup/smlm_z_data/20240510_Miguel_beads/zstacks/20nm_red/stacks/combined/locs.hdf', key='locs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815b06c7-3fe3-4103-a936-3cf67e76fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG19, MobileNetV3Small\n",
    "\n",
    "MobileNetV3Small(input_shape=(64,64,3), weights='imagenet',\n",
    "                                  include_top=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8712a3e4-8658-43df-8db9-1fc9b0afff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=''\n",
    "from tensorflow.keras import Sequential, layers\n",
    "import tensorflow as tf\n",
    "\n",
    "image_size = 64\n",
    "imshape = (image_size, image_size)\n",
    "img_preprocessing = Sequential([\n",
    "    layers.Resizing(*imshape),\n",
    "    layers.Lambda(tf.image.grayscale_to_rgb)\n",
    "])\n",
    "\n",
    "\n",
    "imgs = tf.convert_to_tensor(np.random.uniform(0, 1, size=(3, 15, 15, 1)))\n",
    "\n",
    "imgs2 = img_preprocessing(imgs)\n",
    "print(imgs2.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14bbb80-6892-419a-a464-6cd1cc2c8438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=''\n",
    "from tifffile import imread\n",
    "\n",
    "stacks = imread('/home/miguel/Projects/smlm_z/publication/VIT_openframe/stacks.ome.tif')\n",
    "stack = np.concatenate(stacks[0:1, 60:120]).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bebd43-e39e-43cd-bb1c-0ffc2d408cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/miguel/Projects/smlm_z/publication/')\n",
    "from util import util\n",
    "\n",
    "stack_c = stack[:, :, :, np.newaxis]\n",
    "\n",
    "stack_norm = util._apply_img_norm((stack_c, None), None, {'norm': 'frame-min'})[0][0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a5d5d1-5730-4dc2-a948-5e27de87ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stack_norm.min(axis=(1,2,3)))\n",
    "print(stack_norm.max(axis=(1,2,3)))\n",
    "\n",
    "args = {\n",
    "    'aug_gauss': 0,\n",
    "    'aug_brightness': 0.2,\n",
    "    'aug_poisson_lam': 0,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "extra_aug = Sequential([], name='extra_aug')\n",
    "\n",
    "if args['aug_gauss']:\n",
    "    extra_aug.add(layers.GaussianNoise(stddev=args['aug_gauss'], seed=args['seed']))\n",
    "\n",
    "if args['aug_brightness']:\n",
    "    extra_aug.add(layers.RandomBrightness(args['aug_brightness'], value_range=[0, 1], seed=args['seed']))\n",
    "    \n",
    "# layers.RandomTranslation(1/imshape[0], 1/imshape[0], seed=args['seed']),\n",
    "if args['aug_poisson_lam']:\n",
    "    extra_aug.add(RandomPoissonNoise(imshape, 1, args['aug_poisson_lam'], seed=args['seed']))\n",
    "\n",
    "stack_aug = extra_aug(stack_norm).numpy()\n",
    "print(stack_aug.min(axis=(1,2,3)))\n",
    "print(stack_aug.max(axis=(1,2,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d989b82e-c761-46a5-9416-c3691441ed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_img_norm(imgs):\n",
    "    print(imgs.dtype)\n",
    "    mins = tf.math.reduce_min(imgs, axis=(1,2), keepdims=True)\n",
    "    imgs -= mins\n",
    "    maxs = tf.math.reduce_max(imgs, axis=(1,2), keepdims=True)\n",
    "    imgs = tf.nn.relu(imgs / maxs)\n",
    "    return imgs\n",
    "\n",
    "data = data.map(_apply_img_norm)\n",
    "data = data.batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7500e2-d63e-44c1-a7ec-dcaec95758fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for batch in data.as_numpy_iterator():\n",
    "    imgs = batch.squeeze()\n",
    "\n",
    "for i in range(norm_stacks.shape[1]):\n",
    "    plt.figure(figsize=(1, 1), dpi=60)\n",
    "    plt.imshow(norm_stacks[0, i])\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(1, 1), dpi=60)\n",
    "    plt.imshow(imgs[i])\n",
    "    plt.show()\n",
    "    print('---------')\n",
    "\n",
    "\n",
    "print(d.shape)\n",
    "plt.plot(d.max(axis=(1,2)))\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfe920a-0adc-430c-8889-de7c54311114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "psfs = np.random.uniform(0, 1, size=(100, 10, 15, 15))\n",
    "\n",
    "i = 0\n",
    "psf_mins = psfs[i].min(axis=(1,2), keepdims=True)\n",
    "print(psf_mins.shape)\n",
    "psfs[i] -= psf_mins\n",
    "print(psf_mins.shape)\n",
    "psf_sums = psfs[i].sum(axis=(1,2), keepdims=True)\n",
    "psfs[i] /= psf_sums\n",
    "print(psfs[i].sum(axis=(1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c489898d-9392-4a76-97c0-34609c1161ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "\n",
    "run_dir = '/home/miguel/Projects/smlm_z/publication/VIT_Zeiss_gauss_aug5/out_roll_alignment/'\n",
    "train_data = tf.data.Dataset.load(run_dir + 'train')\n",
    "\n",
    "for (imgs, xy), z in train_data.as_numpy_iterator():\n",
    "    break\n",
    "\n",
    "img = imgs[500:502]\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers, Sequential\n",
    "\n",
    "class RandomPoissonNoise(layers.Layer):\n",
    "    def __init__(self, shape, lam_min, lam_max, rescale=65336, seed=42):\n",
    "        super(RandomPoissonNoise, self).__init__()\n",
    "        tf.random.set_seed(seed)\n",
    "\n",
    "        self.shape = shape\n",
    "        self.lam_min = lam_min\n",
    "        self.lam_max = lam_max\n",
    "        self.rescale = rescale\n",
    "\n",
    "    def call(self, input, training=False):\n",
    "        if training==False:\n",
    "            return input\n",
    "        lam = tf.random.uniform((1,), self.lam_min, self.lam_max)[0]\n",
    "        noise = tf.random.poisson(self.shape, lam, dtype=tf.float32) / self.rescale\n",
    "        return input + noise\n",
    "\n",
    "\n",
    "extra_aug = Sequential([\n",
    "    layers.GaussianNoise(stddev=0.005, seed=42),\n",
    "    layers.RandomTranslation(0.1, 0.1, seed=42),\n",
    "    layers.RandomBrightness(0.4, value_range=[0, 1], seed=42),\n",
    "    RandomPoissonNoise(img.shape, 100, 10000)\n",
    "], name='extra_aug')\n",
    "\n",
    "print(img.min(), img.max())\n",
    "plt.figure(figsize=(2, 2), dpi=60)\n",
    "plt.imshow(img[0].mean(axis=-1))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def norm_zero_one(x):\n",
    "    return (x-x.min()) / (x.max()-x.min())\n",
    "\n",
    "\n",
    "for _ in range(5):\n",
    "    aug_img = extra_aug(img, training=True).numpy()[0]\n",
    "    print(aug_img.min(), aug_img.max())\n",
    "    plt.figure(figsize=(2, 2), dpi=60)\n",
    "    plt.imshow(aug_img.mean(axis=-1))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e45a8a4-c071-4573-9139-8036000c879c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "\n",
    "run_dir = '/home/miguel/Projects/smlm_z/publication/VIT_Zeiss_gauss_aug10/out_roll_alignment/'\n",
    "train_data = tf.data.Dataset.load(run_dir + 'train')\n",
    "\n",
    "\n",
    "    \n",
    "import numpy as np\n",
    "train_img_mins = []\n",
    "train_img_maxs = []\n",
    "train_img_means = []\n",
    "for (imgs, xy), z in train_data.as_numpy_iterator():\n",
    "    train_img_mins.append(imgs.min(axis=(1,2,3)))\n",
    "    train_img_maxs.append(imgs.max(axis=(1,2,3)))\n",
    "    train_img_means.append(imgs.mean(axis=(1,2,3)))\n",
    "\n",
    "train_img_mins = np.concatenate(train_img_mins)\n",
    "train_img_maxs = np.concatenate(train_img_maxs)\n",
    "train_img_means = np.concatenate(train_img_means)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cd3880-2fb4-4e48-8187-20f4f50d42a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py \n",
    "\n",
    "datagen = joblib.load(run_dir + 'datagen.gz')\n",
    "\n",
    "exp_data = '/media/Data/smlm_z_data/20231121_nup_miguel_zeiss/FOV1/storm_1/old_locs/storm_1_MMStack_Default.ome_spots.hdf5'\n",
    "with h5py.File(exp_data, 'r') as f:\n",
    "    spots = np.array(f['spots']).astype(float)\n",
    "\n",
    "BASELINE = 100\n",
    "SENSITIVITY = 1\n",
    "GAIN = 1\n",
    "\n",
    "spots = (spots * GAIN / SENSITIVITY) + BASELINE\n",
    "\n",
    "spots = datagen.standardize(spots)\n",
    "spots_mins = spots.min(axis=(1,2))\n",
    "spots_maxs = spots.max(axis=(1,2))\n",
    "spots_mean = spots.mean(axis=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81dba47-0211-4753-b70a-1cfe0332e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.boxplot((spots_mins, train_img_mins))\n",
    "plt.show()\n",
    "plt.boxplot((spots_maxs, train_img_maxs))\n",
    "plt.show()\n",
    "plt.boxplot((spots_mean, train_img_means))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a80067a-7e32-4d35-8863-b348ab8a39b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from data.visualise import grid_psfs\n",
    "from tensorflow.keras import layers, Sequential\n",
    "import numpy as np\n",
    "    \n",
    "SEED = 42\n",
    "n = 200\n",
    "for (imgs, xy), z in test_data.as_numpy_iterator():\n",
    "    imgs = imgs[:n]\n",
    "    xy = xy[:n]\n",
    "    z = z[:n]\n",
    "    plt.imshow(grid_psfs(imgs.mean(axis=-1)).T)\n",
    "    plt.show()\n",
    "    z_pred = model.predict((imgs, xy), verbose=False).squeeze()\n",
    "    mae = mean_absolute_error(z_pred, z)\n",
    "\n",
    "    plt.title(str(round(mae, 2)))\n",
    "    plt.scatter(z, z_pred, marker='x')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    aug = Sequential([\n",
    "        layers.RandomBrightness(0.5, value_range=[0, 1], seed=SEED)\n",
    "    ])\n",
    "\n",
    "    aug_imgs = aug(imgs).numpy()\n",
    "\n",
    "    plt.imshow(grid_psfs(aug_imgs.mean(axis=-1)).T)\n",
    "    plt.show()\n",
    "    z_pred = model.predict((aug_imgs, xy), verbose=False).squeeze()\n",
    "\n",
    "    idx = np.argwhere(np.sum(aug_imgs, axis=(1,2,3))!=0).squeeze()\n",
    "    z = z[idx]\n",
    "    z_pred = z_pred[idx]\n",
    "    \n",
    "    mae = mean_absolute_error(z_pred, z)\n",
    "\n",
    "    plt.title(str(round(mae, 2)))\n",
    "    plt.scatter(z, z_pred, marker='x')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438a9e07-a70f-48e7-9d15-fb9661bb2de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/VIT_openframe_newaug5/out_roll_alignment/out_nup/locs_3d.hdf5', key='locs')\n",
    "picked_locs = pd.read_hdf('/home/miguel/Projects/data/20230601_MQ_celltype/nup/fov2/storm_1/storm_1_MMStack_Default.ome_locs_undrifted_picked_4.hdf5', key='locs')\n",
    "df = df.merge(picked_locs, on=['x', 'y', 'photons', 'bg', 'lpx', 'lpy', 'net_gradient', 'iterations', 'frame', 'likelihood', 'sx', 'sy'])\n",
    "df['clusterID'] = df['group']\n",
    "for i in range(100):\n",
    "    \n",
    "    data = df[df['group']==i][['z']].to_numpy()\n",
    "    sns.histplot(data=data, bins=40, stat='density')\n",
    "\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    \n",
    "    cov_type = 'full'\n",
    "    gm = GaussianMixture(n_components=2, n_init=20, covariance_type=cov_type).fit(data)\n",
    "    bic = gm.bic(data)\n",
    "    \n",
    "    labels = gm.predict(data).squeeze()\n",
    "    \n",
    "    weights = gm.weights_\n",
    "    \n",
    "    # sns.histplot(data=gm_df, x='pred', hue='cluster_id', stat='density', alpha=0.2, bins=20)\n",
    "    \n",
    "    # create necessary things to plot\n",
    "    x_axis = np.linspace(data.min(), data.max(), 50)\n",
    "    ys = []\n",
    "    sub_df2 = pd.DataFrame.from_dict({'x': x_axis})\n",
    "    for i in range(0, gm.n_components):\n",
    "        if cov_type == 'tied':\n",
    "            cov = gm.covariances_.squeeze()\n",
    "        elif cov_type == 'full' or cov_type == None:\n",
    "            cov = gm.covariances_[i][0][0]\n",
    "        elif cov_type == 'spherical':\n",
    "            cov = gm.covariances_[i]\n",
    "        elif cov_type == 'diag':\n",
    "            cov_type = gm.covariances_[i]\n",
    "    \n",
    "        sub_df2[f'y_{i}'] = norm.pdf(x_axis, float(gm.means_[i][0]), np.sqrt(cov))*gm.weights_[i]\n",
    "        sns.lineplot(data=sub_df2, x='x', y=f'y_{i}')\n",
    "\n",
    "    diff = gm.means_.max() - gm.means_.min()\n",
    "    if 40 < diff and diff < 60:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb13f8a-bacb-4aa4-bf8c-9330928f825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention map for VIT model\n",
    "import matplotlib.pyplot as plt\n",
    "from vit_keras import visualize\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "import joblib\n",
    "\n",
    "dataset = '/home/miguel/Projects/smlm_z/publication/VIT_openframe_newaug3/out_roll_alignment/test'\n",
    "model = '/home/miguel/Projects/smlm_z/publication/VIT_openframe_newaug3/out_roll_alignment/latest_vit_model3'\n",
    "model = tf.keras.models.load_model(model)\n",
    "model = model.layers[2]\n",
    "train_data = tf.data.Dataset.load(dataset)\n",
    "\n",
    "def norm_zero_one(x):\n",
    "    return (x-x.min()) / (x.max()-x.min())\n",
    "    \n",
    "for (imgs, xy), z in train_data.as_numpy_iterator():\n",
    "    for i in range(30, 50):\n",
    "        # for i in range(imgs.shape[0]):\n",
    "        #     plt.figure(figsize=(1, 1))\n",
    "        #     plt.title(str(i))\n",
    "        #     plt.imshow(norm_zero_one(imgs[i]))\n",
    "        #     plt.show()\n",
    "        image = imgs[i]\n",
    "        print(image.min(), image.max())\n",
    "        attention_map = visualize.attention_map(model=model, image=norm_zero_one(image)*255)\n",
    "        print(attention_map.min(), attention_map.max())\n",
    "        fig, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "        ax1.axis('off')\n",
    "        ax2.axis('off')\n",
    "        ax1.set_title('Original')\n",
    "        ax2.set_title('Attention Map')\n",
    "        _ = ax1.imshow(norm_zero_one(image))\n",
    "        _ = ax2.imshow(norm_zero_one(attention_map))\n",
    "        plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1151dcc8-8765-42ca-9a9a-2e5f98512e58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compare pixel vals between training data and exp data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "import joblib\n",
    "\n",
    "dataset = '/home/miguel/Projects/smlm_z/publication/VIT_openframe_no_imagenet2/out_roll_alignment/train'\n",
    "locs = '/home/miguel/Projects/data/20230601_MQ_celltype/nup/fov2/storm_1/storm_1_MMStack_Default.ome_spots.hdf5'\n",
    "datagen = joblib.load('/home/miguel/Projects/smlm_z/publication/VIT_openframe_no_imagenet2/out_roll_alignment/datagen.gz')\n",
    "\n",
    "train_data = tf.data.Dataset.load(dataset)\n",
    "\n",
    "psfs = []\n",
    "i = 0\n",
    "for (psf, xy), z in train_data.as_numpy_iterator():\n",
    "    psfs.append(psf)\n",
    "    i += 1\n",
    "    if i == 10:\n",
    "        break\n",
    "    \n",
    "psfs = np.concatenate(psfs)\n",
    "\n",
    "with h5py.File(locs, 'r') as f:\n",
    "    nup_spots = np.array(f['spots']).astype(np.uint16)\n",
    "nup_spots = datagen.standardize(nup_spots.astype(float))\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "def snr(x):\n",
    "    return x.max() / np.median(x)\n",
    "\n",
    "for op in [np.max, np.min, np.mean, snr]:\n",
    "    df = pd.DataFrame.from_dict({'val': [op(x) for x in psfs]})\n",
    "    df['ds'] = 'beads'\n",
    "    df2 = pd.DataFrame.from_dict({'val': [op(x) for x in nup_spots]})\n",
    "    df2['ds'] = 'locs' \n",
    "    df = pd.concat((df, df2))\n",
    "    fig = plt.figure(figsize=(5, 3))\n",
    "\n",
    "    plt.title(str(op.__name__))\n",
    "    sns.boxplot(data=df, x='ds', y='val')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da61f075-ea80-46f6-a374-c98bd8f39892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661c3958-6eb0-45e2-8c57-0ab755d819ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ea121d-5172-43c4-8718-030e201a7bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(psfs.mean(), np.std(psfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1176b14b-d02e-4304-a612-50f2eeaf3706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9464811-43ba-41a7-897d-cfe9ed70110e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tifffile import imread\n",
    "import h5py\n",
    "\n",
    "\n",
    "locs = '/home/miguel/Projects/smlm_z/publication/simul/fd-loco-simul/nup_spots.hdf5'\n",
    "with h5py.File(locs, 'r') as f:\n",
    "    simul_spots = np.array(f['spots']).astype(np.uint16)\n",
    "\n",
    "locs = '/home/miguel/Projects/data/fd-loco/roi_startpos_810_790_split.ome_spots.hdf5'\n",
    "with h5py.File(locs, 'r') as f:\n",
    "    real_spots = np.array(f['spots']).astype(np.uint16)\n",
    "\n",
    "print(simul_spots.min(), simul_spots.max(), simul_spots.mean())\n",
    "print(real_spots.min(), real_spots.max(), real_spots.mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cf9754-ac11-416a-8ea9-fa417f7d0d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.visualise import show_psf_axial\n",
    "keep_beads = []\n",
    "\n",
    "for i in range(len(beads)):\n",
    "    max_val = np.argmax(beads[i].max(axis=(1,2)))\n",
    "    keep_beads.append(beads[i][max_val-100:max_val+100])\n",
    "\n",
    "keep_beads = np.concatenate(keep_beads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a62f9ad-bb2e-4e42-8aaa-cffce4466f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_beads = keep_beads[:, :, :, np.newaxis].astype(float)\n",
    "spots = spots[:, :, :, np.newaxis].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763034bd-f04c-48a6-b18d-142bcc2fd41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_beads.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4313624f-72c8-49ec-a67b-9d09a2a44b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snr(x):\n",
    "    return x.max() / np.median(x)\n",
    "\n",
    "\n",
    "for op in [np.max, np.min, np.mean, snr]:\n",
    "    \n",
    "    df = pd.DataFrame.from_dict({'val': [op(x) for x in keep_beads]})\n",
    "    df['ds'] = 'beads'\n",
    "    df4 = pd.DataFrame.from_dict({'val': [op(x) for x in spots]})\n",
    "    df4['ds'] = 'locs' \n",
    "    df = pd.concat((df, df4))\n",
    "    fig = plt.figure(figsize=(5, 3))\n",
    "\n",
    "    plt.title(str(op.__name__))\n",
    "    sns.boxplot(data=df, x='ds', y='val')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8cf27a-06f0-4899-afe6-1feddd68514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x, maxval=1):\n",
    "    return ((x - x.min()) / (x.max() - x.min())) * maxval\n",
    "\n",
    "\n",
    "keep_beads = np.stack([norm(x) for x in keep_beads])\n",
    "spots = np.stack([norm(x) for x in spots])\n",
    "\n",
    "print(keep_beads.max(), spots.max())\n",
    "print(keep_beads.min(), spots.min())\n",
    "print(keep_beads.mean(), spots.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf0e49-acd7-4707-a2bd-4dcf8091bd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fake_images = np.random.uniform(0, 255, size=(4, 255, 255, 3))\n",
    "\n",
    "def norm_zero_one(imgs):\n",
    "    mins = imgs.min(axis=(1,2,3), keepdims=True)\n",
    "    maxs = imgs.max(axis=(1,2,3), keepdims=True)\n",
    "    return (imgs-mins) / (maxs-mins)\n",
    "\n",
    "norm_imgs = norm_zero_one(fake_images)\n",
    "print(norm_imgs.min(axis=(1,2,3)))\n",
    "print(norm_imgs.max(axis=(1,2,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c708d272-8aee-4ae5-acd2-c42ab969fde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, layers\n",
    "\n",
    "aug = Sequential([\n",
    "   layers.RandomBrightness([-0.2, 0.2], value_range=(0, 1)),\n",
    "   layers.GaussianNoise(0.5),\n",
    "   layers.RandomContrast(0.2)\n",
    "])\n",
    "# new_aug = aug(keep_beads.copy()).numpy()\n",
    "\n",
    "# for i in [0, 25, 50, 600]:\n",
    "#     plt.figure(figsize=(1,1))\n",
    "#     img1 = keep_beads[i]\n",
    "#     print(img1.min(), img1.max(), img1.dtype)\n",
    "#     img2 = aug(img1).numpy()\n",
    "#     print(img2.min(), img2.max(), img2.dtype)\n",
    "#     img = np.concatenate((norm(img1), norm(img2)), axis=1)\n",
    "#     plt.imshow(img)\n",
    "#     plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e73bab-3d05-4c6e-838f-88085ff8dcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, layers\n",
    "\n",
    "aug_pipeline = Sequential([\n",
    "    layers.GaussianNoise(stddev=0.001*keep_beads.max()),\n",
    "    # layers.RandomTranslation(MAX_TRANSLATION_PX/img_size, MAX_TRANSLATION_PX/img_size, seed=args['seed']),\n",
    "    layers.RandomBrightness(0.01, value_range=[0, keep_beads.max()]),\n",
    "])\n",
    "\n",
    "old_aug = aug_pipeline(keep_beads.copy()).numpy()[:, :, : np.newaxis]\n",
    "old_aug = np.concatenate((old_aug, keep_beads))\n",
    "\n",
    "# aug = Sequential([\n",
    "#    layers.RandomBrightness([-0.4, 0], value_range=(0, 1)),\n",
    "#    layers.GaussianNoise(0.5),\n",
    "#    layers.RandomContrast(0.5)\n",
    "# ])\n",
    "\n",
    "new_aug = (aug(keep_beads.copy()).numpy())[:, :, : :, np.newaxis]\n",
    "new_aug = np.concatenate((new_aug, keep_beads))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2389c76f-08ee-4d0c-8e43-72e7061def08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def snr(x):\n",
    "    return x.max() / np.median(x)\n",
    "\n",
    "\n",
    "for op in [np.max, np.min, np.mean, snr]:\n",
    "    \n",
    "    df = pd.DataFrame.from_dict({'val': [op(x) for x in keep_beads]})\n",
    "    df['ds'] = 'beads'\n",
    "    df2 = pd.DataFrame.from_dict({'val': [op(x) for x in old_aug]})\n",
    "    df2['ds'] = 'old_aug'\n",
    "    df3 = pd.DataFrame.from_dict({'val': [op(x) for x in new_aug]})\n",
    "    df3['ds'] = 'new_aug'\n",
    "    df4 = pd.DataFrame.from_dict({'val': [op(x) for x in spots]})\n",
    "    df4['ds'] = 'locs' \n",
    "    df = pd.concat((df, df2, df3, df4))\n",
    "    fig = plt.figure(figsize=(5, 3))\n",
    "\n",
    "    plt.title(str(op.__name__))\n",
    "    sns.boxplot(data=df, x='ds', y='val')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6164a638-c111-4326-b1c7-affcdc169aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0/65336.0,\n",
    "    samplewise_center=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    horizontal_flip=False)\n",
    "\n",
    "datagen.fit(keep_beads)\n",
    "std_beads = datagen.standardize(keep_beads.copy())\n",
    "std_spots = datagen.standardize(spots.copy())\n",
    "\n",
    "print(old_aug.shape, keep_beads.shape)\n",
    "old_aug_data = np.concatenate((old_aug, keep_beads))\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0/65336.0,\n",
    "    samplewise_center=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    horizontal_flip=False)\n",
    "datagen.fit(old_aug_data)\n",
    "old_aug_data_norm = datagen.standardize(old_aug_data.copy())\n",
    "old_aug_spots = datagen.standardize(spots.copy())\n",
    "\n",
    "new_aug_data = np.concatenate((new_aug, keep_beads))\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0/65336.0,\n",
    "    samplewise_center=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    horizontal_flip=False)\n",
    "datagen.fit(new_aug_data)\n",
    "new_aug_data_norm = datagen.standardize(new_aug_data.copy())\n",
    "new_aug_spots = datagen.standardize(spots.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4d89ea-99cf-4f58-86ba-18b815c702c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snr(x):\n",
    "    return x.max() / np.median(x)\n",
    "\n",
    "for op in [np.max, np.min, np.mean, snr]:\n",
    "    df = pd.DataFrame.from_dict({'val': [op(x) for x in old_aug_data_norm]})\n",
    "    df['ds'] = 'beads'\n",
    "    df2 = pd.DataFrame.from_dict({'val': [op(x) for x in old_aug_spots]})\n",
    "    df2['ds'] = 'locs' \n",
    "    df3 = pd.DataFrame.from_dict({'val': [op(x) for x in new_aug_data_norm]})\n",
    "    df3['ds'] = 'beads_2'\n",
    "    df4 = pd.DataFrame.from_dict({'val': [op(x) for x in new_aug_spots]})\n",
    "    df4['ds'] = 'locs_2' \n",
    "    df = pd.concat((df, df2, df3, df4))\n",
    "    fig = plt.figure(figsize=(5, 3))\n",
    "\n",
    "    plt.title(str(op.__name__))\n",
    "    sns.boxplot(data=df, x='ds', y='val')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40578bf2-a6be-4fe7-9cb2-1b8be730430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "snrs_beads = [max(x) for x in keep_beads]\n",
    "snrs_locs = [max(x) for x in spots]\n",
    "\n",
    "plt.hist(snrs_beads, label='beads')\n",
    "plt.hist(snrs_locs, label='locs')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "snrs_beads = [max(x) for x in keep_beads]\n",
    "snrs_locs = [max(x) for x in spots]\n",
    "\n",
    "plt.hist(snrs_beads, label='beads')\n",
    "plt.hist(snrs_locs, label='locs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da93dfe8-ce33-4020-b916-e3d31ad0ed73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=''\n",
    "import tensorflow as tf\n",
    "\n",
    "ds = tf.data.Dataset.load('/home/miguel/Projects/smlm_z/publication/VIT_Zeiss_gauss_aug/out_roll_alignment/train')\n",
    "\n",
    "imgs = []\n",
    "for (img, xy), z in ds.as_numpy_iterator():\n",
    "    imgs.append(img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce38dbd-0157-4285-9e4d-82c6ac80fd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = np.concatenate(imgs)\n",
    "print(imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2787906f-14a6-44b9-8c67-4c8be928898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spots.min(), spots.max(), spots.mean())\n",
    "print(imgs.min(), imgs.max(), imgs.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b73e71-b0c6-4b29-bada-99c6dc083ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# new_nup = pd.read_csv('/home/miguel/Projects/smlm_z/publication/VIT_openframe_newaug/out_roll_alignment/out_nup/nup_renders3/nup_report.csv')\n",
    "# old_nup = pd.read_csv('/home/miguel/Projects/smlm_z/publication/VIT_openframe/out_roll_alignment/out_nup/nup_renders3/nup_report.csv')\n",
    "\n",
    "\n",
    "new_nup = pd.read_csv('/home/miguel/Projects/smlm_z/publication/VIT_Zeiss_gauss_aug/out_roll_alignment/out_nup/nup_renders3/nup_report.csv')\n",
    "old_nup = pd.read_csv('/home/miguel/Projects/smlm_z/publication/VIT_Zeiss/out_roll_alignment/out_nup/nup_renders3/nup_report.csv')\n",
    "\n",
    "new_nup['dataset'] = 'new'\n",
    "old_nup['dataset'] = 'old'\n",
    "\n",
    "df = pd.concat((old_nup, new_nup))\n",
    "\n",
    "sns.boxplot(data=df, x='dataset', y='seperation')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "def count_valid(x):\n",
    "    return len(np.argwhere((x>40) & (x<60)))\n",
    "\n",
    "print(count_valid(new_nup['seperation']))\n",
    "print(count_valid(old_nup['seperation']))\n",
    "\n",
    "# new_locs = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/VIT_Zeiss_gauss_aug/out_roll_alignment/out_nup/locs_3d.hdf5', key='locs')\n",
    "# old_locs = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/VIT_Zeiss/out_roll_alignment/out_nup/locs_3d.hdf5', key='locs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9d3873-72d1-42e3-8d1a-292690dcadb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(new_locs['z [nm]'])\n",
    "sns.histplot(old_locs['z [nm]'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c5f3e-5b8a-4d2f-8a43-edf8a6d61bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys, os\n",
    "\n",
    "\n",
    "\n",
    "# # TODO remove this\n",
    "if not os.environ.get('CUDA_VISIBLE_DEVICES'):\n",
    "    os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "import shutil\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Resizing, Lambda\n",
    "from tensorflow.keras import Sequential\n",
    "import tensorflow as tf\n",
    "from picasso import io\n",
    "\n",
    "from data.visualise import grid_psfs\n",
    "\n",
    "\n",
    "\n",
    "N_GPUS = max(1, len(tf.config.experimental.list_physical_devices(\"GPU\")))\n",
    "\n",
    "\n",
    "VERSION = '0.1'\n",
    "\n",
    "\n",
    "\n",
    "# Picasso localisation parameters\n",
    "BASELINE = 100\n",
    "SENSITIVITY = 1\n",
    "GAIN = 1\n",
    "\n",
    "\n",
    "DEFAULT_LOCS = None\n",
    "DEFAULT_SPOTS = None\n",
    "DEFAULT_PIXEL_SIZE = None\n",
    "PICKED = None\n",
    "XLIM, YLIM = None, None\n",
    "\n",
    "# NUP FD-LOCO\n",
    "# DEFAULT_LOCS = '/home/miguel/Projects/data/fd-loco/roi_startpos_810_790_split.ome_locs.hdf5'\n",
    "# DEFAULT_SPOTS = '/home/miguel/Projects/data/fd-loco/roi_startpos_810_790_split.ome_spots.hdf5'\n",
    "# PICKED = '/home/miguel/Projects/data/fd-loco/roi_startpos_810_790_split.ome_locs_picked.hdf5'\n",
    "# DEFAULT_PIXEL_SIZE = 110\n",
    "# XLIM, YLIM = None, None\n",
    "\n",
    "\n",
    "\n",
    "# NUP OPENFRAME\n",
    "# DEFAULT_LOCS = '/home/miguel/Projects/data/20230601_MQ_celltype/nup/fov2/storm_1/storm_1_MMStack_Default.ome_locs_undrifted.hdf5'\n",
    "# DEFAULT_SPOTS = '/home/miguel/Projects/data/20230601_MQ_celltype/nup/fov2/storm_1/storm_1_MMStack_Default.ome_spots.hdf5'\n",
    "# PICKED = '/home/miguel/Projects/data/20230601_MQ_celltype/nup/fov2/storm_1/storm_1_MMStack_Default.ome_locs_undrifted_picked_4.hdf5'\n",
    "# DEFAULT_PIXEL_SIZE = 86\n",
    "# XLIM, YLIM = None, None\n",
    "\n",
    "\n",
    "\n",
    "# Zeiss\n",
    "# DEFAULT_LOCS = '/media/Data/smlm_z_data/20231121_nup_miguel_zeiss/FOV1/storm_1/storm_1_MMStack_Default.ome_locs_undrifted.hdf5'\n",
    "# DEFAULT_SPOTS = '/media/Data/smlm_z_data/20231121_nup_miguel_zeiss/FOV1/storm_1/storm_1_MMStack_Default.ome_spots.hdf5'\n",
    "# PICKED = '/media/Data/smlm_z_data/20231121_nup_miguel_zeiss/FOV1/storm_1/storm_1_MMStack_Default.ome_locs_picked.hdf5'\n",
    "# DEFAULT_PIXEL_SIZE = 106\n",
    "# XLIM, YLIM = None, None\n",
    "\n",
    "\n",
    "# Unused below\n",
    "\n",
    "# # Mitochondria (older)\n",
    "# DEFAULT_LOCS = '/home/miguel/Projects/data/20231205_miguel_mitochondria/mitochondria/FOV2/storm_1/storm_1_MMStack_Default.ome_locs_undrifted.hdf5'\n",
    "# DEFAULT_SPOTS = '/home/miguel/Projects/data/20231205_miguel_mitochondria/mitochondria/FOV2/storm_1/storm_1_MMStack_Default.ome_spots.hdf5'\n",
    "# DEFAULT_PIXEL_SIZE = 86\n",
    "# XLIM, YLIM = None, None\n",
    "\n",
    "# Mitochondria (newer) (still not clearly working)\n",
    "# DEFAULT_LOCS = '/media/Data/smlm_z_data/20231212_miguel_openframe/mitochondria/FOV2/storm_1/storm_1_MMStack_Default.ome_locs_undrift.hdf5'\n",
    "# DEFAULT_SPOTS = '/media/Data/smlm_z_data/20231212_miguel_openframe/mitochondria/FOV2/storm_1/storm_1_MMStack_Default.ome_spots.hdf5'\n",
    "# PICKED = None\n",
    "# DEFAULT_PIXEL_SIZE = 86\n",
    "# XLIM = 400, 600\n",
    "# YLIM = 700, 1000\n",
    "\n",
    "\n",
    "\n",
    "# Tubulin\n",
    "# DEFAULT_LOCS = '/media/Data/smlm_z_data/20231212_miguel_openframe/tubulin/FOV1/storm_1/storm_1_MMStack_Default.ome_locs_undrifted.hdf5'\n",
    "# DEFAULT_SPOTS = '/media/Data/smlm_z_data/20231212_miguel_openframe/tubulin/FOV1/storm_1/storm_1_MMStack_Default.ome_spots.hdf5'\n",
    "# PICKED = None\n",
    "# DEFAULT_PIXEL_SIZE = 86\n",
    "# XLIM = 200, 800\n",
    "# YLIM = 500, 1000\n",
    "\n",
    "def write_arg_log(args):\n",
    "    outfile = os.path.join(args['outdir'], 'config.json')\n",
    "    with open(outfile, 'w') as fp:\n",
    "        json_dumps_str = json.dumps(args, indent=4)\n",
    "        print(json_dumps_str, file=fp)\n",
    "\n",
    "\n",
    "def save_copy_script(outdir):\n",
    "    outpath = os.path.join(outdir, 'localise_exp_sample.py.bak')\n",
    "    shutil.copy(os.path.abspath(__file__), outpath)\n",
    "\n",
    "\n",
    "def gen_2d_plot(locs, outdir):\n",
    "    print('Gen 2d plot')\n",
    "    sns.scatterplot(data=locs, x='x', y='y', marker='.', alpha=0.1)\n",
    "    plt.axis('equal')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.savefig(os.path.join(outdir, '2d_scatterplot.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def gen_example_spots(spots, outdir):\n",
    "    print('Gen example splots')\n",
    "    plt.rcParams['figure.figsize'] = [10, 10]\n",
    "    plt.imshow(grid_psfs(spots[0:100]))\n",
    "    plt.savefig(os.path.join(outdir, 'example_spots.png'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def apply_normalisation(locs, spots, args):\n",
    "    print('Applying pre-processing')\n",
    "    scaler = joblib.load(args['coords_scaler'])\n",
    "    datagen = joblib.load(args['datagen'])\n",
    "\n",
    "    coords = scaler.transform(locs[['x', 'y']].to_numpy())\n",
    "    spots = datagen.standardize(spots.astype(np.float32))[:, :, :, np.newaxis]\n",
    "\n",
    "    return coords, spots\n",
    "\n",
    "\n",
    "\n",
    "def pred_z(model, spots, coords):\n",
    "\n",
    "    spots = spots.astype(np.float32)\n",
    "    print('Predicting z locs')\n",
    "\n",
    "    # exp_spots = tf.data.Dataset.from_generator(\n",
    "    #     generator=lambda: iter(spots),\n",
    "    #     output_signature=tf.TensorSpec(shape=spots.shape[1:], dtype=tf.float32)\n",
    "    # )\n",
    "    exp_spots = tf.data.Dataset.from_tensor_slices(spots)\n",
    "    exp_coords = tf.data.Dataset.from_tensor_slices(coords)\n",
    "\n",
    "    exp_X = tf.data.Dataset.zip((exp_spots, exp_coords))\n",
    "\n",
    "    fake_z = np.zeros((coords.shape[0],))\n",
    "    exp_z = tf.data.Dataset.from_tensor_slices(fake_z)\n",
    "\n",
    "    exp_data = tf.data.Dataset.zip((exp_X, exp_z))\n",
    "\n",
    "    image_size = 64\n",
    "    imshape = (image_size, image_size)\n",
    "    img_preprocessing = Sequential([\n",
    "        Resizing(*imshape),\n",
    "        Lambda(tf.image.grayscale_to_rgb)\n",
    "    ])\n",
    "\n",
    "    def apply_rescaling(x, y):\n",
    "        x = [x[0], x[1]]\n",
    "        x[0] = img_preprocessing(x[0])\n",
    "        return tuple(x), y\n",
    "\n",
    "    BATCH_SIZE = 2048\n",
    "    exp_data = exp_data.map(apply_rescaling, num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE)\n",
    "\n",
    "    pred_z = model.predict(exp_data, batch_size=BATCH_SIZE, workers=4)\n",
    "\n",
    "    \n",
    "\n",
    "    # sns.histplot(pred_z)\n",
    "    # plt.show()\n",
    "    # plt.savefig(os.path.join(outdir, 'z_histplot.png'))\n",
    "    # plt.close()\n",
    "    return pred_z\n",
    "\n",
    "def write_locs(locs, z_coords, args):\n",
    "    locs['z [nm]'] = z_coords\n",
    "    locs['z'] = locs['z [nm]']\n",
    "    # locs['z'] = z_coords / args['pixel_size']\n",
    "    locs['x [nm]'] = locs['x'] * args['pixel_size']\n",
    "    locs['y [nm]'] = locs['y'] * args['pixel_size']\n",
    "\n",
    "    locs_path = os.path.join(args['outdir'], 'locs_3d.hdf5')\n",
    "    with h5py.File(locs_path, \"w\") as locs_file:\n",
    "        locs_file.create_dataset(\"locs\", data=locs.to_records())\n",
    "\n",
    "    yaml_file = args['locs'].replace('.hdf5', '.yaml')\n",
    "    if os.path.exists(yaml_file):\n",
    "        dest_yaml = locs_path.replace('.hdf5', '.yaml')\n",
    "        shutil.copy(yaml_file, dest_yaml)\n",
    "    else:\n",
    "        dest_yaml = None\n",
    "        print('Could not write yaml file (original from 2D localisation not found)')\n",
    "    print('Wrote results to:')\n",
    "    print(f'\\t- {os.path.abspath(locs_path)}')\n",
    "    if dest_yaml:\n",
    "        print(f'\\t- {os.path.abspath(dest_yaml)}')\n",
    "\n",
    "\n",
    "def write_report_data(args):\n",
    "    report_data = {\n",
    "        'code_version': VERSION\n",
    "    }\n",
    "    report_data.update(args)\n",
    "    with open(os.path.join(args['outdir'], 'report.json'), 'w') as fp:\n",
    "        json_dumps_str = json.dumps(report_data, indent=4)\n",
    "        print(json_dumps_str, file=fp)\n",
    "\n",
    "\n",
    "def extract_fov(spots, locs):\n",
    "    print(locs.shape)\n",
    "    idx = np.argwhere((XLIM[0]<locs['x']) & (XLIM[1]>locs['x']) & (YLIM[0]<locs['y']) & (YLIM[1]>locs['y'])).squeeze()\n",
    "    spots = spots[idx]\n",
    "    locs = locs.iloc[idx]\n",
    "    return spots, locs\n",
    "\n",
    "def tmp_filter_locs(new_locs, spots, args):\n",
    "    old_locs = pd.read_hdf(args['picked_locs'], key='locs')\n",
    "\n",
    "    idx = np.argwhere(new_locs['x'].isin(old_locs['x'])).squeeze()\n",
    "    new_locs = new_locs.iloc[idx]\n",
    "    spots = spots[idx]\n",
    "    return new_locs, spots\n",
    "\n",
    "\n",
    "\n",
    "args = {\n",
    "    'model': '/home/miguel/Projects/smlm_z/publication/VIT_Zeiss/out_roll_alignment/latest_vit_model3',\n",
    "    'locs': '/media/Data/smlm_z_data/20231121_nup_miguel_zeiss/FOV1/storm_1/storm_1_MMStack_Default.ome_locs_undrifted.hdf5',\n",
    "    'spots': '/media/Data/smlm_z_data/20231121_nup_miguel_zeiss/FOV1/storm_1/storm_1_MMStack_Default.ome_spots.hdf5',\n",
    "    'coords_scaler': '/home/miguel/Projects/smlm_z/publication/VIT_Zeiss/out_roll_alignment/scaler.save',\n",
    "    'datagen': '/home/miguel/Projects/smlm_z/publication/VIT_Zeiss/out_roll_alignment/datagen.gz',\n",
    "    'picked_locs': '/media/Data/smlm_z_data/20231121_nup_miguel_zeiss/FOV1/storm_1/storm_1_MMStack_Default.ome_locs_picked.hdf5'\n",
    "}\n",
    "\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with mirrored_strategy.scope():\n",
    "    model = tf.keras.models.load_model(args['model'])\n",
    "\n",
    "locs, info = io.load_locs(args['locs'])\n",
    "locs = pd.DataFrame.from_records(locs)\n",
    "\n",
    "with h5py.File(args['spots'], 'r') as f:\n",
    "    spots = np.array(f['spots']).astype(np.uint16)\n",
    "\n",
    "spots = (spots * GAIN / SENSITIVITY) + BASELINE\n",
    "\n",
    "\n",
    "\n",
    "# # TODO remove temp subset of locs\n",
    "if args['picked_locs']:\n",
    "    locs, spots = tmp_filter_locs(locs, spots, args)\n",
    "\n",
    "assert locs.shape[0] == spots.shape[0]\n",
    "print(locs.shape)\n",
    "if XLIM or YLIM:\n",
    "    spots, locs = extract_fov(spots, locs)\n",
    "\n",
    "# gen_2d_plot(locs, args['outdir'])\n",
    "# gen_example_spots(spots, args['outdir'])\n",
    "coords, spots = apply_normalisation(locs, spots, args)\n",
    "print(coords.shape)\n",
    "\n",
    "z_coords = pred_z(model, spots, coords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4d6d82-3ffa-4742-8935-ee2b277b0f63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce50792-a0d2-4c7a-96c9-6b0d911667e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_coords2 = pred_z(model, spots, np.zeros(coords.shape))\n",
    "plt.scatter(z_coords, z_coords2)\n",
    "plt.show()\n",
    "print(np.abs(z_coords-z_coords2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d232cc4-06f2-4237-9b91-d0f25c395d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "im1 = grid_psfs(spots[0:100].mean(axis=-1))\n",
    "im2 = im1+np.random.normal(0, 0.05, size=im1.shape)\n",
    "plt.imshow(im1)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(im2)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(abs(im2-im1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a12ff15-10d1-4b66-a3ca-d1a5068ef7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "import os\n",
    "from glob import glob\n",
    "from natsort import natsorted\n",
    "import subprocess\n",
    "import h5py\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from tifffile import imread, imwrite\n",
    "from skimage.feature import match_template\n",
    "from skimage.filters import butterworth\n",
    "from skimage.filters import gaussian\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.special import erf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import json\n",
    "import shutil\n",
    "from data.visualise import grid_psfs\n",
    "import seaborn as sns\n",
    "\n",
    "def norm_zero_one(s):\n",
    "    max_s = s.max()\n",
    "    min_s = s.min()\n",
    "    return (s - min_s) / (max_s - min_s)\n",
    "\n",
    "def validate_args(args):\n",
    "    args['bead_stacks'] = [b for b in args['bead_stacks'] if 'ignored' not in b]\n",
    "    n_stacks = len(args['bead_stacks'])\n",
    "    print(f\"Found {n_stacks} bead stacks\")\n",
    "    if n_stacks == 0:\n",
    "        quit(1)\n",
    "    for f in natsorted(args['bead_stacks']):\n",
    "        print(f'\\t - {f}')\n",
    "\n",
    "\n",
    "def test_picasso_exec():\n",
    "    res = subprocess.run(['picasso', '-h'], capture_output=True, text=True)\n",
    "    if res.returncode != 0:\n",
    "        print(res.stdout)\n",
    "        print(res.stderr)\n",
    "        print('\\n')\n",
    "        raise EnvironmentError('Picasso not found/working (see above)')\n",
    "\n",
    "def transform_args(args):\n",
    "    fnames = glob(f\"{args['bead_stacks']}/**/*.tif\", recursive=True)\n",
    "\n",
    "    args['outpath'] = args['bead_stacks']\n",
    "    fnames = [os.path.abspath(f) for f in fnames if '_slice.ome.tif' not in f and os.path.basename(f) != 'stacks.ome.tif']\n",
    "    args['bead_stacks'] = fnames\n",
    "\n",
    "    args['gaussian_blur'] = list(map(int, args['gaussian_blur'].split(',')))\n",
    "    return args\n",
    "\n",
    "def get_or_create_slice(bead_stack, slice_path):\n",
    "    if not os.path.exists(slice_path):\n",
    "        im_slice = bead_stack[bead_stack.shape[0]//2]\n",
    "        # plt.imshow(im_slice)\n",
    "        # plt.show()\n",
    "        imwrite(slice_path, im_slice.astype(np.uint16))\n",
    "    return slice_path\n",
    "\n",
    "def get_or_create_locs(slice_path, args):\n",
    "    spots_path = slice_path.replace('.ome.tif', '.ome_spots.hdf5')\n",
    "    locs_path = slice_path.replace('.ome.tif', '.ome_locs.hdf5')\n",
    "\n",
    "    if not os.path.exists(spots_path) or not os.path.exists(locs_path) or args['regen']:\n",
    "        cmd = ['picasso', 'localize', slice_path, '-b', args['box_size_length'], '-g', args['gradient'], '-px', args['pixel_size']]\n",
    "        print(f'Running {\" \".join(list(map(str, cmd)))}')\n",
    "        for extra_arg in ['qe', 'sensitivity', 'gain', 'baseline', 'fit-method']:\n",
    "            if extra_arg in args and args[extra_arg]:\n",
    "                cmd.extend([f'-{extra_arg}', args[extra_arg]])\n",
    "        cmd = ' '.join(list(map(str, cmd)))\n",
    "        tqdm.write('Running picasso...', end='')\n",
    "        res = subprocess.run(cmd, capture_output=True, shell=True, text=True)\n",
    "        if res.returncode != 0:\n",
    "            print('Picasso error occured')\n",
    "            print(res.stdout)\n",
    "            print(res.stderr)\n",
    "            return\n",
    "        tqdm.write('finished!')\n",
    "\n",
    "    with h5py.File(spots_path) as f:\n",
    "        spots = np.array(f['spots'])\n",
    "\n",
    "    locs = pd.read_hdf(locs_path, key='locs')\n",
    "    locs['fname'] = '___'.join(slice_path.split('/')[-3:])\n",
    "    print(f'Found {locs.shape[0]} beads')\n",
    "    return locs, spots\n",
    "\n",
    "def remove_colocal_beads(locs, spots, args):\n",
    "    tqdm.write('Removing overlapping beads...')\n",
    "    coords = locs[['x', 'y']].to_numpy()\n",
    "    dists = euclidean_distances(coords, coords)\n",
    "    np.fill_diagonal(dists, np.inf)\n",
    "    min_dists = dists.min(axis=1)\n",
    "    \n",
    "    error_margin = 0.8\n",
    "    min_seperation = (np.sqrt(2)  * args['box_size_length']) * error_margin\n",
    "    idx = np.argwhere(min_dists > min_seperation).squeeze()\n",
    "    locs = locs.iloc[idx]\n",
    "    spots = spots[idx]\n",
    "\n",
    "    return locs, spots\n",
    "\n",
    "\n",
    "def extract_training_stacks(spots, bead_stack, args) -> np.array:\n",
    "    spot_size = args['box_size_length']\n",
    "    frame_idx = bead_stack.shape[0]//2\n",
    "    frame = bead_stack[frame_idx]\n",
    "    stacks = []\n",
    "    for spot in spots:\n",
    "        res = match_template(frame, spot)\n",
    "        i, j = np.unravel_index(np.argmax(res), res.shape)\n",
    "        stack = bead_stack[:, i:i+spot_size, j:j+spot_size]\n",
    "        stacks.append(stack)\n",
    "    return np.array(stacks)\n",
    "\n",
    "def snr(psf):\n",
    "    return psf.max() / np.median(psf)\n",
    "\n",
    "\n",
    "def has_fwhm(psf, args):\n",
    "    psf = butterworth(psf, cutoff_frequency_ratio=0.2, high_pass=False)\n",
    "    y = np.max(gaussian(psf), axis=(1,2))\n",
    "    max_val = np.max(y)\n",
    "    min_val = np.min(y)\n",
    "    half_max = min_val + ((max_val-min_val) / 2)\n",
    "    crossCount = np.sum((y[:-1]>half_max) != (y[1:]>half_max))\n",
    "    # if args['debug'] and crossCount < 2:\n",
    "    #     plt.plot(y, label='raw')\n",
    "    #     plt.plot([0, len(y)], [half_max, half_max])\n",
    "    #     plt.show()\n",
    "    # if not (crossCount >= 2):\n",
    "    #     fig = plt.figure(layout=\"constrained\", figsize=(20, 15), dpi=64)\n",
    "    #     gs = plt.GridSpec(1, 2, figure=fig)\n",
    "    #     ax1 = fig.add_subplot(gs[0, 0])\n",
    "    #     ax2 = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "    #     ax1.imshow(grid_psfs(psf, cols=20))\n",
    "    #     ax2.plot(psf)\n",
    "    #     plt.show()\n",
    "    return crossCount >= 2\n",
    "\n",
    "\n",
    "def filter_mse_zprofile(psf, args, i):\n",
    "    z_step = args['zstep']\n",
    "\n",
    "    # Define the skewed Gaussian function\n",
    "    def skewed_gaussian(x, A, x0, sigma, alpha, offset):\n",
    "        \"\"\"\n",
    "        A: Amplitude\n",
    "        x0: Center\n",
    "        sigma: Standard Deviation\n",
    "        alpha: Skewness parameter\n",
    "        offset: Vertical offset\n",
    "        \"\"\"\n",
    "        return A * np.exp(-(x - x0)**2 / (2 * sigma**2)) * (1 + erf(alpha * (x - x0))) + offset\n",
    "\n",
    "\n",
    "    # Fit the skewed Gaussian to the data\n",
    "    x_data = np.arange(psf.shape[0]) * z_step\n",
    "    y_data = psf.max(axis=(1,2))\n",
    "    y_data = norm_zero_one(y_data)\n",
    "    initial_guess = [1, psf.shape[0] * z_step / 2, psf.shape[0] * z_step/4, 0.0, np.median(y_data)]\n",
    "\n",
    "    bounds = [\n",
    "        (0.6, 1.2),\n",
    "        (psf.shape[0] * z_step/8, psf.shape[0] * z_step),\n",
    "        (psf.shape[0] * z_step/20, psf.shape[0] * z_step/4),\n",
    "        (-np.inf, np.inf),\n",
    "        (y_data.min(), y_data.max())\n",
    "    ]\n",
    "    try:\n",
    "        params, _ = curve_fit(skewed_gaussian, x_data, y_data, p0=initial_guess, bounds=list(zip(*bounds)))\n",
    "    except RuntimeError:\n",
    "        print('Failed to find Z fit')\n",
    "        params = initial_guess\n",
    "\n",
    "    y_fit = skewed_gaussian(x_data, *params)\n",
    "\n",
    "    mse = (y_fit - y_data) ** 2\n",
    "    avg_mse = np.mean(mse)\n",
    "    max_mse = np.max(mse)\n",
    "\n",
    "    permitted_avg_mse = 0.02\n",
    "    permitted_max_mse = 0.1\n",
    "    # if (avg_mse < permitted_avg_mse and max_mse < permitted_max_mse):\n",
    "    #     fig = plt.figure(layout=\"constrained\", figsize=(10, 8), dpi=64)\n",
    "    #     gs = plt.GridSpec(1, 2, figure=fig)\n",
    "    #     ax1 = fig.add_subplot(gs[0, 0])\n",
    "    #     ax2 = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "    #     ax1.imshow(grid_psfs(psf, cols=20))\n",
    "    #     print(avg_mse, max_mse)\n",
    "    #     ax2.plot(x_data, y_data)\n",
    "    #     ax2.plot(x_data, y_fit)\n",
    "    #     plt.show()\n",
    "    return avg_mse < permitted_avg_mse and max_mse < permitted_max_mse\n",
    "        \n",
    "\n",
    "def get_sharpness(array):\n",
    "    gy, gx = np.gradient(array)\n",
    "    gnorm = np.sqrt(gx**2 + gy**2)\n",
    "    sharpness = np.average(gnorm)\n",
    "    return sharpness\n",
    "\n",
    "\n",
    "def reduce_img(psf):\n",
    "    return np.stack([get_sharpness(x) for x in psf])\n",
    "\n",
    "\n",
    "def est_bead_offsets(psfs, locs, args):\n",
    "    UPSCALE_RATIO = 10\n",
    "\n",
    "    def denoise(img):\n",
    "        \n",
    "        sigmas = np.array(args['gaussian_blur'])\n",
    "        return gaussian_filter(img.copy(), sigma=sigmas)\n",
    "\n",
    "    def find_peak(psf):\n",
    "        if psf.ndim == 4:\n",
    "            psf = psf.mean(axis=-1)\n",
    "        x = np.arange(psf.shape[0]) * args['zstep']\n",
    "        psf = denoise(psf)\n",
    "        \n",
    "        inten = norm_zero_one(reduce_img(psf))\n",
    "\n",
    "        cs = UnivariateSpline(x, inten, k=3, s=0.2)\n",
    "\n",
    "        x_ups = np.linspace(0, psf.shape[0], len(x) * UPSCALE_RATIO) * args['zstep']\n",
    "\n",
    "        peak_xups = x_ups[np.argmax(cs(x_ups))] \n",
    "\n",
    "        return peak_xups\n",
    "    offsets = np.array(map(find_peak, psfs))\n",
    "\n",
    "    locs['offset'] = offsets\n",
    "\n",
    "\n",
    "def filter_mse_xy(stack, max_mse, i):\n",
    "    # Define a 2D Gaussian function\n",
    "    def gaussian_2d(xy, amplitude, xo, yo, sigma_x, sigma_y, theta, offset):\n",
    "        x, y = xy\n",
    "        xo = float(xo)\n",
    "        yo = float(yo)\n",
    "        a = (np.cos(theta)**2) / (2 * sigma_x**2) + (np.sin(theta)**2) / (2 * sigma_y**2)\n",
    "        b = -(np.sin(2 * theta)) / (4 * sigma_x**2) + (np.sin(2 * theta)) / (4 * sigma_y**2)\n",
    "        c = (np.sin(theta)**2) / (2 * sigma_x**2) + (np.cos(theta)**2) / (2 * sigma_y**2)\n",
    "        g = offset + amplitude * np.exp(- (a * ((x - xo)**2) + 2 * b * (x - xo) * (y - yo) + c * ((y - yo)**2)))\n",
    "        return g.ravel()\n",
    "\n",
    "    sharp = reduce_img(stack)\n",
    "    idx = np.argmax(sharp)\n",
    "    image = stack[idx]\n",
    "\n",
    "\n",
    "    # Load and preprocess the image (e.g., convert to grayscale)\n",
    "    # For simplicity, let's generate a simple image for demonstration\n",
    "    image_size = image.shape[1]\n",
    "    x = np.linspace(0, image_size - 1, image_size)\n",
    "    y = np.linspace(0, image_size - 1, image_size)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    \n",
    "    image = image / image.max()\n",
    "\n",
    "    # Fit the Gaussian to the image data\n",
    "    p0 = [1, image_size / 2, image_size / 2, 2, 2, 0, 0]  # Initial guess for parameters\n",
    "    bounds = [\n",
    "        (0, np.inf),\n",
    "        (image_size * (1/5), image_size * (4/5)),\n",
    "        (image_size * (1/5), image_size * (4/5)),\n",
    "        (0, image_size/3),\n",
    "        (0, image_size/3),\n",
    "        (-np.inf, np.inf),\n",
    "        (0, np.inf),\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        popt, pcov = curve_fit(gaussian_2d, (x, y), image.ravel(), p0=p0, bounds=list(zip(*bounds)))\n",
    "    except RuntimeError:\n",
    "        print('XY fit failed')\n",
    "        popt = p0\n",
    "    render = gaussian_2d((x, y), *popt).reshape(image.shape)\n",
    "\n",
    "    error = mean_squared_error(render, image)\n",
    "\n",
    "    # if error > max_mse:\n",
    "    #     print(i)\n",
    "    #     # Visualize the original image and the fitted Gaussian\n",
    "    #     plt.plot(sharp)\n",
    "    #     plt.show()\n",
    "    #     plt.figure(figsize=(5, 3))\n",
    "    #     plt.subplot(1, 2, 1)\n",
    "    #     plt.imshow(image)\n",
    "    #     plt.title('Original Image')\n",
    "        \n",
    "    #     plt.subplot(1, 2, 2)\n",
    "    #     plt.imshow(render)\n",
    "    #     plt.title('Fitted Gaussian')\n",
    "        \n",
    "    #     plt.tight_layout()\n",
    "    #     plt.show()\n",
    "    #     print(error, max_mse, error <= max_mse)\n",
    "\n",
    "    return error <= max_mse\n",
    "\n",
    "\n",
    "def filter_beads(spots, locs, stacks, args, rejected_outpath):\n",
    "    print('Removing poorly imaged beads...', end='')\n",
    "    # Filter by SNR threshold\n",
    "\n",
    "    for i in range(stacks.shape[0]):\n",
    "        psf = stacks[i]\n",
    "        plt.title(str(i))\n",
    "        plt.imshow(grid_psfs(psf))\n",
    "        plt.show()\n",
    "    mse_xy = np.array([filter_mse_xy(psf, 10000, i) for i, psf in enumerate(stacks)])\n",
    "    snrs = np.array([snr(psf) > args['min_snr'] for psf in stacks])\n",
    "    fwhms = np.array([has_fwhm(psf, args) for psf in stacks])\n",
    "    mse_z = np.array([filter_mse_zprofile(psf, args, i) for i, psf in enumerate(stacks)])\n",
    "    # TODO re-enable\n",
    "    # mse_xy[:] = True\n",
    "    \n",
    "    # snrs[:] = True\n",
    "    # mse_filters[:] = True\n",
    "\n",
    "    for i in range(stacks.shape[0]):\n",
    "        psf = stacks[i]\n",
    "        print(filter_mse_xy(psf, 10000, i))\n",
    "        print(snr(psf) > args['min_snr'])\n",
    "        print(has_fwhm(psf, args))\n",
    "        print(filter_mse_zprofile(psf, args, i))\n",
    "        plt.imshow(grid_psfs(psf))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    idx = np.argwhere(snrs & fwhms & mse_z & mse_xy).squeeze()\n",
    "    reasons = [''] * len(snrs)\n",
    "    for i in range(spots.shape[0]):\n",
    "        if not fwhms[i]:\n",
    "            reasons[i] += 'fwhm'\n",
    "        if not snrs[i]:\n",
    "            reasons[i] += f',snrs({round(snr(stacks[i]), 3)})' \n",
    "        if not mse_z[i]:\n",
    "            reasons[i] += ',mse_z'\n",
    "        if not mse_xy[i]:\n",
    "            reasons[i] += ',mse_xy'\n",
    "\n",
    "    locs['rejected'] = reasons\n",
    "\n",
    "\n",
    "    est_bead_offsets(stacks, locs, args)\n",
    "\n",
    "    if args['debug']:\n",
    "        rejected_idx = np.argwhere(np.invert(snrs & fwhms & mse_z & mse_xy))[:, 0]\n",
    "        print('\\n', 'Rejected: ', rejected_idx)\n",
    "\n",
    "        if len(rejected_idx):\n",
    "            print('Writing rejected figures...')\n",
    "\n",
    "            write_stack_figures(stacks[rejected_idx], locs.iloc[rejected_idx], rejected_outpath)\n",
    "\n",
    "    spots = spots[idx]\n",
    "    locs = locs.iloc[idx]\n",
    "    stacks = stacks[idx]\n",
    "\n",
    "    print('finished!')\n",
    "\n",
    "    return spots, locs, stacks\n",
    "\n",
    "\n",
    "def write_combined_data(stacks, locs, args):\n",
    "\n",
    "    outpath = os.path.join(args['outpath'], 'combined')\n",
    "    os.makedirs(outpath, exist_ok=True)\n",
    "\n",
    "    locs_outpath = os.path.join(outpath, 'locs.hdf')\n",
    "    stacks_outpath = os.path.join(outpath, 'stacks.ome.tif')\n",
    "\n",
    "    imwrite(stacks_outpath, stacks)\n",
    "    locs.to_hdf(locs_outpath, key='locs')\n",
    "\n",
    "    stacks_config = {\n",
    "        'zstep': args['zstep'],\n",
    "        'gen_args': args\n",
    "    }\n",
    "    \n",
    "    stacks_config_outpath = os.path.join(outpath, 'stacks_config.json')\n",
    "    with open(stacks_config_outpath, 'w') as fp:\n",
    "        json_dumps_str = json.dumps(stacks_config, indent=4)\n",
    "        print(json_dumps_str, file=fp)\n",
    "\n",
    "    figpath = os.path.join(outpath, 'offsets.png')\n",
    "    sns.scatterplot(data=locs, x='x', y='y', hue='offset')\n",
    "    plt.savefig(figpath)\n",
    "    plt.close()\n",
    "\n",
    "    print('Saved results to:')\n",
    "    print(f'\\t{locs_outpath}')\n",
    "    print(f'\\t{stacks_outpath}')\n",
    "    print(f'\\t{stacks_config_outpath}')\n",
    "    print(f'Total beads: {locs.shape[0]}')\n",
    "\n",
    "\n",
    "def write_stack_figure(i, stacks, locs, outpath, fname):\n",
    "    stack = stacks[i]\n",
    "    loc = locs.iloc[i].to_dict()\n",
    "\n",
    "    fig = plt.figure(layout=\"constrained\", figsize=(20, 15), dpi=64)\n",
    "    gs = plt.GridSpec(2, 3, figure=fig)\n",
    "    ax1 = fig.add_subplot(gs[0:, 0])\n",
    "    ax2 = fig.add_subplot(gs[0, 1:])\n",
    "    ax3 = fig.add_subplot(gs[1, 1:])\n",
    "\n",
    "\n",
    "    fig.suptitle(f'Bead: {i}')\n",
    "\n",
    "    ax1.imshow(grid_psfs(stack, cols=20))\n",
    "    ax1.set_title('Ordered by frame')\n",
    "\n",
    "    intensity = stack.max(axis=(1,2))\n",
    "    min_val = min(intensity)\n",
    "    max_val = max(intensity)\n",
    "    frame_zpos = (np.arange(len(intensity)) * args['zstep']) - loc['offset']\n",
    "    ax2.plot(frame_zpos, intensity)\n",
    "    ax2.vlines(0, min_val, max_val, colors='orange')\n",
    "    ax2.set_title('Max normalised pixel intensity over z')\n",
    "    ax2.set_xlabel('z (nm)')\n",
    "    ax2.set_ylabel('pixel intensity')    \n",
    "\n",
    "\n",
    "    for k, v in loc.items():\n",
    "        if isinstance(v, float):\n",
    "            loc[k] = round(v, 5)\n",
    "    text = json.dumps(loc, indent=4)\n",
    "    ax3.axis((0, 10, 0, 10))\n",
    "    ax3.text(0,0, text, fontsize=18, wrap=True)\n",
    "    outfpath = os.path.join(outpath, f'{fname}_bead_{i}.png')\n",
    "    plt.savefig(outfpath)\n",
    "    plt.close()\n",
    "    print(f'Wrote {outfpath}')\n",
    "\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from itertools import repeat\n",
    "\n",
    "\n",
    "def write_stack_figures(stacks, locs, outpath):\n",
    "    fname = set(locs['fname']).pop().replace('.ome.tif', '')\n",
    "    os.makedirs(outpath, exist_ok=True)\n",
    "\n",
    "    idx = np.arange(stacks.shape[0])\n",
    "    with Pool(8) as pool:\n",
    "        res = pool.starmap(write_stack_figure, zip(idx, repeat(stacks), repeat(locs), repeat(outpath), repeat(fname)))\n",
    "\n",
    "# def filter_by_tmp_locs(locs, spots):\n",
    "#     original_locs = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/original_locs.hdf', key='locs')\n",
    "#     x_coords = set(original_locs['x'])\n",
    "#     idx = np.argwhere([x in x_coords for x in locs['x']]).squeeze()\n",
    "#     locs = locs.iloc[idx]\n",
    "#     spots = spots[idx]\n",
    "#     print(locs.shape)\n",
    "#     return locs, spots\n",
    "\n",
    "\n",
    "\n",
    "stackss = None\n",
    "def main(args):\n",
    "    all_stacks = []\n",
    "    all_spots = []\n",
    "    all_locs = []\n",
    "\n",
    "    found_beads = 0\n",
    "    retained_beads = 0\n",
    "\n",
    "    rejected_outpath = os.path.join(args['outpath'], 'combined', 'rejected')\n",
    "    shutil.rmtree(rejected_outpath, ignore_errors=True)\n",
    "\n",
    "    if args['debug']:\n",
    "        os.makedirs(rejected_outpath, exist_ok=True)\n",
    "\n",
    "    for bead_stack_path in tqdm(natsorted(args['bead_stacks'])):\n",
    "        if 'stack__2_' not in bead_stack_path:\n",
    "            continue\n",
    "        tqdm.write(f'Preparing {os.path.basename(bead_stack_path)}')\n",
    "\n",
    "        bead_stack = imread(bead_stack_path)\n",
    "        slice_path = bead_stack_path.replace('.ome', '_slice.ome')\n",
    "        slice_path = get_or_create_slice(bead_stack, slice_path)\n",
    "\n",
    "        raw_locs, spots = get_or_create_locs(slice_path, args)\n",
    "        \n",
    "        # raw_locs, spots = filter_by_tmp_locs(raw_locs, spots)\n",
    "        found_beads += raw_locs.shape[0]\n",
    "        locs, spots = remove_colocal_beads(raw_locs, spots, args)\n",
    "        perc_removed = round(100*(1-(locs.shape[0]/raw_locs.shape[0])), 2)\n",
    "        print(f'Removed {perc_removed}% due to co-location')\n",
    "\n",
    "        stacks = extract_training_stacks(spots, bead_stack, args)\n",
    "        stackss = stacks\n",
    "        raise EnvironmentError\n",
    "        spots, locs, stacks = filter_beads(spots, locs, stacks, args, rejected_outpath)\n",
    "        retained_beads += locs.shape[0]\n",
    "        tqdm.write(f'Retained {stacks.shape[0]} beads')\n",
    "        all_stacks.append(stacks)\n",
    "        all_spots.append(spots)\n",
    "        all_locs.append(locs)\n",
    "\n",
    "    print(f'Found {found_beads} total beads')\n",
    "    min_stack_length = min(list(map(lambda s: s.shape[1], all_stacks)))\n",
    "    stacks = [s[:, :min_stack_length] for s in all_stacks]\n",
    "    locs = pd.concat(all_locs)\n",
    "    stacks = np.concatenate(stacks)\n",
    "\n",
    "    # original_locs = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/original_locs.hdf', key='locs')\n",
    "    # x_coords = set(original_locs['x'])\n",
    "    # print(len(set(locs['x'])), len(set(original_locs['x'])))\n",
    "    # print(len(set(locs['x']).intersection(set(original_locs['x']))))\n",
    "    # locs = pd.concat((locs, locs))\n",
    "    # stacks = np.concatenate((stacks, stacks))\n",
    "\n",
    "    print(locs.shape)\n",
    "    print(stacks.shape)\n",
    "    print(f'Kept {locs.shape[0]} total beads')\n",
    "\n",
    "    write_combined_data(stacks, locs, args)\n",
    "\n",
    "    if args['debug']:\n",
    "        outpath = os.path.join(args['outpath'], 'combined', 'debug')\n",
    "        write_stack_figures(stacks, locs, outpath)\n",
    "        \n",
    "\n",
    "def parse_args():\n",
    "    parser = ArgumentParser(description='')\n",
    "    parser.add_argument('bead_stacks', help='Path to TIFF bead stacks / directory containing bead stacks.')\n",
    "    parser.add_argument('-z', '--zstep', help='Pixel size (nm)', default=10, type=int)\n",
    "    parser.add_argument('-px', '--pixel_size', help='Pixel size (nm)', default=86, type=int)\n",
    "    parser.add_argument('-g', '--gradient', help='Min. net gradient', default=1000, type=int)\n",
    "    parser.add_argument('-b', '--box-size-length', help='Box size', default=15, type=int)\n",
    "    parser.add_argument('-qe', '--qe', help='Quantum efficiency', type=float)\n",
    "    parser.add_argument('-s', '--sensitivity', help='Sensitivity', type=float)\n",
    "    parser.add_argument('-ga', '--gain', help='Gain', type=float)\n",
    "    parser.add_argument('-bl', '--baseline', help='Baseline', type=int)\n",
    "    parser.add_argument('-a', '--fit-method', help='Fit method', choices=['mle', 'lq', 'avg'])\n",
    "    parser.add_argument('--regen', action='store_true')\n",
    "    parser.add_argument('-snr', '--min-snr', type=float, default=2.0)\n",
    "    parser.add_argument('-gb', '--gaussian-blur', default='3,2,2', help='Gaussian pixel-blur in Z/Y/X for bead offset estimation')\n",
    "    parser.add_argument('--debug', action='store_true')\n",
    "\n",
    "    args = vars(parser.parse_args())\n",
    "    return args\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaf7af9-ea82-47c9-80c1-bf2c44cc8036",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'zstep': 10,\n",
    "    'pixel_size': 86,\n",
    "    'bead_stacks': '/media/Data/smlm_z_data/20231121_nup_miguel_zeiss/stacks/',\n",
    "    'gaussian_blur': '3,2,2',\n",
    "    'debug': False,\n",
    "    'regen': False,\n",
    "    'box_size_length': 15\n",
    "}\n",
    "\n",
    "args = transform_args(args)\n",
    "validate_args(args)\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707d337f-8d07-49ea-8546-5999f6985fd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bead_stack_path = '/media/Data/smlm_z_data/20231121_nup_miguel_zeiss/stacks/stack__2/stack__2_MMStack_Default.ome.tif'\n",
    "\n",
    "bead_stack = imread(bead_stack_path)\n",
    "slice_path = bead_stack_path.replace('.ome', '_slice.ome')\n",
    "slice_path = get_or_create_slice(bead_stack, slice_path)\n",
    "\n",
    "raw_locs, spots = get_or_create_locs(slice_path, args)\n",
    "\n",
    "# raw_locs, spots = filter_by_tmp_locs(raw_locs, spots)\n",
    "locs, spots = remove_colocal_beads(raw_locs, spots, args)\n",
    "perc_removed = round(100*(1-(locs.shape[0]/raw_locs.shape[0])), 2)\n",
    "print(f'Removed {perc_removed}% due to co-location')\n",
    "\n",
    "stacks = extract_training_stacks(spots, bead_stack, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f90d53-fc5f-4c53-b5c1-1243383aea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse_xy = np.array([filter_mse_xy(psf, 10000, i) for i, psf in enumerate(stacks)])\n",
    "snrs = np.array([snr(psf) > args['min_snr'] for psf in stacks])\n",
    "fwhms = np.array([has_fwhm(psf, args) for psf in stacks])\n",
    "mse_z = np.array([filter_mse_zprofile(psf, args, i) for i, psf in enumerate(stacks)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ade36c-2e5b-420a-bca6-b9d02c800593",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(stacks.shape[0]):\n",
    "    plt.title(str(i))\n",
    "    plt.imshow(grid_psfs(stacks[i]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b057b10-a32d-4f06-95ca-87ff18e9447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "df = pd.read_csv('~/Projects/smlm_z/publication/comparisons/inspr/reconstruction.csv')\n",
    "df['x'] = df['x [nm]'] / 86\n",
    "df['y'] = df['y [nm]'] / 86\n",
    "df['z'] = df['z [nm]'] / 86\n",
    "\n",
    "df['photons'] = 1000\n",
    "df['sx'] = 1\n",
    "df['sy'] = 1\n",
    "df['bg'] = 0\n",
    "df['lpx'] = 0.1\n",
    "df['lpy'] = 0.1\n",
    "df['frame'] = df['Frame-ID']\n",
    "\n",
    "locs_path = '/home/miguel/Projects/smlm_z/publication/comparisons/inspr/reconstruction.hdf5'\n",
    "with h5py.File(locs_path, \"w\") as locs_file:\n",
    "    locs_file.create_dataset(\"locs\", data=df.to_records())\n",
    "        \n",
    "print(list(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df53868-53c6-4bc5-97f9-361ac6da8172",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['x [nm]'], df['y [nm]'], marker='.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4aa7ea-849a-483c-820a-862215c617ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(df['z [nm]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52dd868-035b-4c73-a216-13a6b4784e36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mat73\n",
    "import scipy\n",
    "import pandas as pd\n",
    "data_dict = mat73.loadmat('/home/miguel/Projects/smlm_z/publication/comparisons/spline/nup_11_sml.mat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8877a6-a3dc-42ef-9cc1-17dec96abfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['saveloc']['file']['info']['cam_pixelsize_um']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e61c5a4-37d2-4873-83f6-55c9d972387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "df = pd.DataFrame.from_records(data_dict['saveloc']['loc'])\n",
    "\n",
    "df2 = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/VIT_openframe/out_roll_alignment/out_nup/locs_3d.hdf5', key='locs')\n",
    "\n",
    "PIXEL_SIZE = 86\n",
    "\n",
    "map_cols = [\n",
    "    ('xnm', 'x [nm]'),\n",
    "    ('ynm', 'y [nm]'),\n",
    "    ('znm', 'z [nm]'),\n",
    "    ('xpix', 'x'),\n",
    "    ('ypix', 'y'),\n",
    "    ('PSFxnm', 'sx'),\n",
    "    ('PSFynm', 'sy'),\n",
    "    ('phot', 'photons'),\n",
    "    ('locprecnm', 'lpx'),\n",
    "    ('locprecnm', 'lpy'),\n",
    "    ('znm', 'z'),\n",
    "    ('zerr', 'lpz'),\n",
    "]\n",
    "\n",
    "for src, trgt in map_cols:\n",
    "    df[trgt] = df[src].copy()\n",
    "\n",
    "src_cols = list(set([x[0] for x in map_cols]))\n",
    "for col in src_cols:\n",
    "    del df[col]\n",
    "\n",
    "rescale_cols = ['lpx', 'lpy', 'sx', 'sy', 'lpz']\n",
    "for col in rescale_cols:\n",
    "    df[col] = df[col] / PIXEL_SIZE\n",
    "\n",
    "df = df[[c for c in list(df) if c in list(df2)] + ['lpz']]\n",
    "df['frame'] = df['frame'].astype(int)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(list(df))\n",
    "import h5py\n",
    "\n",
    "locs_path = '/home/miguel/Projects/smlm_z/publication/comparisons/spline/locs_3d.hdf5'\n",
    "with h5py.File(locs_path, \"w\") as locs_file:\n",
    "    locs_file.create_dataset(\"locs\", data=df.to_records())\n",
    "\n",
    "# shutil.copy('/home/miguel/Projects/smlm_z/publication/VIT_openframe/out_roll_alignment/out_nup/locs_3d.yaml', locs_path.replace('hdf5', 'yaml'))\n",
    "# frame=frame\n",
    "# xnm=x\n",
    "# ynm=y\n",
    "# phot=photons\n",
    "# PSFxnm=sx\n",
    "# PSFynm=sy\n",
    "# bg=bg\n",
    "# locprecnm=lpx\n",
    "# znm=z\n",
    "# cam_pixelsize_um=100\n",
    "# factor=100    1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930af224-6301-41be-af0e-79e950d5ea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import euclidean_distances\n",
    "df_new = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/comparisons/spline/locs_3d_undrift.hdf5', key='locs')\n",
    "df_orig = pd.read_hdf('/home/miguel/Projects/data/20230601_MQ_celltype/nup/fov2/storm_1/storm_1_MMStack_Default.ome_locs_undrifted_picked_4.hdf5', key='locs')\n",
    "print(df_new.shape)\n",
    "sns.scatterplot(data=df_new, x='x', y='y', alpha=0.01)\n",
    "sns.scatterplot(data=df_orig, x='x', y='y', alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35235484-ca04-438c-816d-1a13959c1759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import tensorflow as tf\n",
    "\n",
    "def _euclidean_distances(coords1, coords2, reduce_min=None):\n",
    "    \"\"\"\n",
    "    Compute the Euclidean distances between two sets of 2D coordinates.\n",
    "\n",
    "    Args:\n",
    "        coords1 (tf.Tensor): A tensor of shape (N, 2) containing N 2D coordinates.\n",
    "        coords2 (tf.Tensor): A tensor of shape (M, 2) containing M 2D coordinates.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: A tensor of shape (N, M) containing the Euclidean distances\n",
    "                   between each pair of coordinates from coords1 and coords2.\n",
    "    \"\"\"\n",
    "    # Ensure inputs are TensorFlow tensors\n",
    "    coords1 = tf.convert_to_tensor(coords1, dtype=tf.float32)\n",
    "    coords2 = tf.convert_to_tensor(coords2, dtype=tf.float32)\n",
    "\n",
    "    t1 = tf.reshape(coords1, (1, *coords1.shape))\n",
    "    t2 = tf.reshape(coords2, (coords2.shape[0],1,coords2.shape[1]))\n",
    "    result = tf.norm(t1-t2, ord='euclidean', axis=2,)\n",
    "    if reduce_min is not None:\n",
    "        result = tf.math.reduce_min(result, axis=reduce_min, keepdims=False)\n",
    "    res = result.numpy()\n",
    "    del result\n",
    "    return res\n",
    "    \n",
    "def batched_euclidean_distance(coords1, coords2, batch_size, reduce_min=None):\n",
    "    min_dists = np.zeros((coords1.shape[0],))\n",
    "    from tqdm import trange\n",
    "    for i in trange(0, coords1.shape[0]-1, batch_size):\n",
    "        start, end = i, i+batch_size\n",
    "        _coords1 = coords1[start:end]\n",
    "        _min_dists = _euclidean_distances(_coords1, coords2, reduce_min)\n",
    "        min_dists[start:end] = _min_dists\n",
    "    return min_dists\n",
    "\n",
    "xy_new = df_new[['x', 'y']].to_numpy()\n",
    "xy_orig = df_orig[['x', 'y']].to_numpy()\n",
    "\n",
    "# # Adjust batch size to fit in GPU memory\n",
    "BATCH_SIZE = 2**14\n",
    "\n",
    "# print(xy_new.shape)\n",
    "# print(BATCH_SIZE)\n",
    "\n",
    "min_dists = batched_euclidean_distance(xy_new, xy_orig, BATCH_SIZE, 0)\n",
    "\n",
    "print(min_dists.min(), min_dists.mean(), min_dists.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55402cac-a8d4-4697-9a73-50c661b575ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import euclidean_distances\n",
    "import matplotlib.pyplot as plt\n",
    "cutoff = (120/2) / 86\n",
    "sub_df = df_new[min_dists<cutoff]\n",
    "\n",
    "sub_df['lpz'] /= 10\n",
    "# sns.scatterplot(data=df_orig, x='x', y='y', alpha=0.01)\n",
    "# plt.show()\n",
    "# sns.scatterplot(data=sub_df, x='x', y='y', alpha=0.01)\n",
    "# plt.show()\n",
    "\n",
    "nearest_neighbour = np.zeros((sub_df.shape[0]), dtype=object)\n",
    "xy_new = sub_df[['x', 'y']].to_numpy()\n",
    "\n",
    "df_orig_groups = df_orig.groupby('group').mean()\n",
    "\n",
    "df_orig_groups.reset_index(inplace=True, drop=False)\n",
    "\n",
    "group_centers = df_orig_groups[['x', 'y']].to_numpy()\n",
    "\n",
    "print(xy_new.shape, group_centers.shape)\n",
    "dists = euclidean_distances(xy_new, group_centers)\n",
    "\n",
    "\n",
    "dists_idx = np.argmin(dists, axis=1)\n",
    "sub_df['clusterID'] = df_orig_groups['group'][dists_idx].to_numpy().astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b047e8-1d29-4349-9a49-217c7fab6db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "if 'index' in list(sub_df):\n",
    "    del sub_df['index']\n",
    "sub_df['group'] = sub_df['clusterID']\n",
    "locs_path = '/home/miguel/Projects/smlm_z/publication/comparisons/spline/locs_3d_undrift.hdf5'\n",
    "with h5py.File(locs_path, \"w\") as locs_file:\n",
    "    locs_file.create_dataset(\"locs\", data=sub_df.to_records())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b400c3f5-ff9f-4d4e-a765-18552cdb8638",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/miguel/Projects/smlm_z/publication/spline_comparison/nup_renders3/*/nup_88_gaussian.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694f3d5f-4fb5-4612-97a0-886ff4ab1ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "from natsort import natsorted\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "dir1 = '/home/miguel/Projects/smlm_z/publication/comparisons/spline/nup_renders3/*/'\n",
    "dir2 = '/home/miguel/Projects/smlm_z/publication/VIT_openframe/out*/out_nup/nup_renders3/*/'\n",
    "\n",
    "imgs = [set(list(map(os.path.basename, glob.glob(f'{dirname}/*.png')))) for dirname in (dir1, dir2)]\n",
    "imnames = imgs[0].intersection(imgs[1])\n",
    "print(len(imnames))\n",
    "\n",
    "for imname in natsorted(imnames):\n",
    "    impath = glob.glob(dir1+imname)[0]\n",
    "    img1 = mpimg.imread(impath)\n",
    "\n",
    "    impath2 = glob.glob(dir2+imname)[0]\n",
    "    if 'good' not in impath2:\n",
    "        continue\n",
    "    print(imname)\n",
    "\n",
    "    img2 = mpimg.imread(impath2)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(30, 15))\n",
    "    # Display the first image\n",
    "    ax1.imshow(img1)\n",
    "    ax1.set_title('MLE')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Display the second image\n",
    "    ax2.imshow(img2)\n",
    "    ax2.set_title('Vision transformer')\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    # Adjust the spacing between subplots\n",
    "    plt.subplots_adjust(wspace=0)\n",
    "    \n",
    "    # Display the figure\n",
    "    plt.show()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203d2c0-24a0-41bb-a6e5-aeed0e24b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in sorted(set(sub_df['group'])):\n",
    "    tmp_df = sub_df[sub_df['group']==c]\n",
    "    print(tmp_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fbd2ec-7d0c-4f0c-b0f4-1de3c3b3ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "new_df = '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333cef1e-4a87-4762-b4f6-25e3e81539cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from picasso import io\n",
    "from picasso.render import render\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from scipy.signal import find_peaks\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "import matplotlib.font_manager as fm\n",
    "fontprops = fm.FontProperties(size=18)\n",
    "\n",
    "min_sigma = 0\n",
    "max_sigma = 3\n",
    "\n",
    "z_min = 400\n",
    "z_max = 600\n",
    "min_log_likelihood = -100\n",
    "# min_kde = np.log(0.007)\n",
    "min_kde = 0.05\n",
    "\n",
    "cmap_min_z = -600\n",
    "cmap_max_z = -300\n",
    "BLUR = 'gaussian'\n",
    "color_by_depth = False\n",
    "\n",
    "MIN_BLUR=0.001\n",
    "\n",
    "records = []\n",
    "\n",
    "def filter_locs(l):\n",
    "    n_points = l.shape[0]\n",
    "    print(f'From {n_points} points')\n",
    "\n",
    "    l = l[(min_sigma < l['sx']) & (l['sx'] < max_sigma)]\n",
    "    l = l[(min_sigma < l['sy']) & (l['sy'] < max_sigma)]\n",
    "    # print(f'{n_points-l.shape[0]} removed by sx/sy')\n",
    "\n",
    "\n",
    "    X = l[['z']]\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(X)\n",
    "    l['kde'] = kde.score_samples(X)\n",
    "\n",
    "    # l = l[l['z [nm]'] > z_min]\n",
    "    # l = l[l['z [nm]'] < z_max]\n",
    "    # sns.scatterplot(data=l, x='z', y='kde')\n",
    "    # plt.show()\n",
    "    \n",
    "    l = l[np.power(10, l['kde']) > min_kde]\n",
    "    # print(f'{n_points-l.shape[0]} removed by kde')\n",
    "\n",
    "    # l = l[l['likelihood']>min_log_likelihood]\n",
    "    \n",
    "    n_points2 = l.shape[0]\n",
    "    # print(f'Removed {n_points-n_points2} pts')\n",
    "    # print(f'{n_points2} remaining')\n",
    "    print(f'N points: {n_points2}')\n",
    "\n",
    "    return l\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [18, 6]\n",
    "\n",
    "\n",
    "def apply_cmap_img(img, cmap_min_coord, cmap_max_coord, img_min_coord, img_max_coord, cmap='gist_rainbow', brightness_factor = 20):\n",
    "    img = img.squeeze()\n",
    "    \n",
    "    cmap_zrange = cmap_max_coord - cmap_min_coord\n",
    "    \n",
    "    def map_z_to_cbar(z_val):\n",
    "        return (z_val - cmap_min_coord) / cmap_zrange\n",
    "        \n",
    "    min_coord_color = map_z_to_cbar(img_min_coord)\n",
    "    max_coord_color = map_z_to_cbar(img_max_coord)\n",
    "    \n",
    "    cmap = plt.get_cmap('gist_rainbow')\n",
    "    \n",
    "    gradient = np.repeat(np.linspace(min_coord_color, max_coord_color, img.shape[1])[np.newaxis, :], img.shape[0], 0)\n",
    "    \n",
    "    base = cmap(gradient)\n",
    "    img = img[:, :, np.newaxis]\n",
    "    cmap_img = img * base\n",
    "    # cmap_img /= 2\n",
    "    # Black background\n",
    "    cmap_img = (cmap_img / cmap_img.max()) * 255\n",
    "    cmap_img *= brightness_factor\n",
    "\n",
    "    cmap_img[:, :, 3] = 255 \n",
    "    \n",
    "    cmap_img = cmap_img.astype(int)\n",
    "\n",
    "    return cmap_img\n",
    "    \n",
    "def color_histplot(barplot, cmap_min_z, cmap_max_z):\n",
    "    from matplotlib.colors import rgb2hex\n",
    "    cmap = plt.get_cmap('gist_rainbow')\n",
    "    \n",
    "    bar_centres = [bar._x0 + bar._width/2 for bar in barplot.patches]\n",
    "    bar_centres = np.array(list(map(lambda x: (x-cmap_min_z) / (cmap_max_z-cmap_min_z), bar_centres)))\n",
    "    rgb_colors = cmap(bar_centres)\n",
    "    hex_colors = [rgb2hex(x) for x in rgb_colors]\n",
    "    \n",
    "    for bar, hex_color in zip(barplot.patches, hex_colors):\n",
    "        bar.set_facecolor(hex_color)\n",
    "        \n",
    "\n",
    "def center_view(locs, zrange=200):\n",
    "    zs = locs['z [nm]']\n",
    "    bin_width = 25\n",
    "    hist, bins = np.histogram(zs, bins=np.arange(zs.min(), zs.max(), bin_width))\n",
    "    try:\n",
    "        max_bin_idx = np.argmax(hist)\n",
    "        bin_val = bins[max_bin_idx] + (bin_width // 2)\n",
    "    except ValueError:\n",
    "        bin_val = np.mean(zs)\n",
    "\n",
    "    locs = locs[(bin_val-zrange <=locs['z [nm]']) & (locs['z [nm]'] <= bin_val+zrange)]\n",
    "\n",
    "    return locs\n",
    "\n",
    "def get_viewport(locs, axes, margin=1):\n",
    "    mins = np.array([locs[ax].min()-margin for ax in axes])\n",
    "    maxs = np.array([locs[ax].max()+margin for ax in axes])\n",
    "    # mins[:] = min(mins)\n",
    "    # maxs[:] = max(maxs)\n",
    "    return np.array([mins, maxs])\n",
    "\n",
    "def disable_axis_ticks():\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "def get_extent(viewport, pixel_size):\n",
    "    mins, maxs = viewport\n",
    "    return np.array([mins[1], maxs[1], mins[0], maxs[0]]) * pixel_size\n",
    "\n",
    "\n",
    "def render_locs(locs, args, ang_xyz=(0,0,0), barsize=None, ax=None):\n",
    "    \n",
    "    locs = locs.copy()\n",
    "    locs['lpz'] = np.mean(locs[['lpx', 'lpy']].to_numpy()) / 2\n",
    "    locs['sz'] = np.mean(locs[['sx', 'sy']].to_numpy()) / 3\n",
    "    # locs['lpx'] = 0.1\n",
    "    # locs['sx'] = 0.1\n",
    "    # locs['lpy'] = 0.1\n",
    "    # locs['sy'] = 0.1\n",
    "    disable_axis_ticks()\n",
    "    locs['x [nm]'] -= locs['x [nm]'].mean()\n",
    "    locs['y [nm]'] -= locs['y [nm]'].mean()\n",
    "    locs['z [nm]'] -= locs['z [nm]'].mean()\n",
    "    locs['x'] -= locs['x'].mean()\n",
    "    locs['y'] -= locs['y'].mean()\n",
    "    locs['z'] -= locs['z'].mean()\n",
    "\n",
    "    viewport = get_viewport(locs, ('y', 'x'))\n",
    "\n",
    "    _, img = render(locs.to_records(), blur_method=args['blur_method'], viewport=viewport, min_blur_width=args['min_blur'], ang=ang_xyz, oversampling=args['oversample'])\n",
    "    if ang_xyz == (0, 0, 0):\n",
    "        plt.xlabel('x [nm]')\n",
    "        plt.ylabel('y [nm]')\n",
    "    elif ang_xyz == (np.pi/2, 0, 0):\n",
    "        plt.xlabel('z [nm]')\n",
    "        plt.ylabel('x [nm]')\n",
    "        img = img.T\n",
    "        viewport = np.fliplr(viewport)\n",
    "\n",
    "    elif ang_xyz == (0, np.pi/2, 0):\n",
    "        plt.xlabel('z [nm]')\n",
    "        plt.ylabel('y [nm]')\n",
    "    else:\n",
    "        print('Axis labels uncertain due to rotation angle')\n",
    "\n",
    "    extent = get_extent(viewport, args['pixel_size'])\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    img_plot = plt.imshow(img, extent=extent)\n",
    "    plt.colorbar(img_plot)\n",
    "\n",
    "    if barsize is not None:\n",
    "        scalebar = AnchoredSizeBar(ax.transData,\n",
    "                            barsize, f'{barsize} nm', 'lower center', \n",
    "                            pad=0.1,\n",
    "                            color='white',\n",
    "                            frameon=False,\n",
    "                            size_vertical=1,\n",
    "                            fontproperties=fontprops)\n",
    "        ax.add_artist(scalebar)\n",
    "\n",
    "def write_nup_plots(locs, args, good_dir, other_dir):\n",
    "    for cid in set(locs['clusterID']):\n",
    "        # if not cid in [1, 6, 18, 19, 21, 22]:\n",
    "        if not cid in [6]:\n",
    "            continue\n",
    "        print('Cluster ID', cid)\n",
    "\n",
    "        df = locs[locs['clusterID']==cid]\n",
    "        df = filter_locs(df)\n",
    "\n",
    "        if df.shape[0] == 0:\n",
    "            continue\n",
    "        df = center_view(df)\n",
    "\n",
    "        try:\n",
    "            del df['index']\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        if df.shape[0] < 5:\n",
    "            print('No remaining localisations, continuing...')\n",
    "            continue\n",
    "\n",
    "        fig = plt.figure()\n",
    "        gs = fig.add_gridspec(1, 4)\n",
    "        plt.subplots_adjust(wspace=0.3, hspace=0)\n",
    "        \n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        render_locs(df, args, (0,0,0), barsize=110, ax=ax1)\n",
    "\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        render_locs(df, args, (np.pi/2,0,0), barsize=50, ax=ax2)\n",
    "\n",
    "        ax3 = fig.add_subplot(gs[0, 2])\n",
    "        render_locs(df, args, (0, np.pi/2,0), barsize=50, ax=ax3)\n",
    "\n",
    "        ax4 = fig.add_subplot(gs[0, 3])\n",
    "        \n",
    "        histplot = sns.histplot(data=df, x='z [nm]', ax=ax4, stat='density', legend=False)\n",
    "        if color_by_depth:\n",
    "            color_histplot(histplot, cmap_min_z, cmap_max_z)\n",
    "        sns.kdeplot(data=df, x='z [nm]', ax=ax4, bw_adjust=0.5, color='black', bw_method='silverman')\n",
    "\n",
    "        x = ax4.lines[0].get_xdata()\n",
    "        y = ax4.lines[0].get_ydata()\n",
    "        peaks, _ = find_peaks(y)\n",
    "\n",
    "        sorted_peaks = sorted(peaks, key=lambda peak_index: y[peak_index], reverse=True)\n",
    "        peak_vals = y[peaks]\n",
    "        if len(peak_vals) == 1:\n",
    "            n_peaks = 1\n",
    "        else:\n",
    "            n_peaks = 2\n",
    "            \n",
    "        sorted_peaks = sorted_peaks[:n_peaks]\n",
    "\n",
    "        peak_x = x[sorted_peaks]\n",
    "        peak_y = y[sorted_peaks]\n",
    "        for x, y in zip(peak_x, peak_y):\n",
    "            ax4.vlines(x, 0, y, label=str(round(x)), color='black')\n",
    "\n",
    "        sep = abs(max(peak_x) - min(peak_x))\n",
    "\n",
    "        septxt = 'Sep: '+ str(round(sep))+ 'nm'\n",
    "\n",
    "        records.append({\n",
    "            'id': cid,\n",
    "            'seperation': sep,\n",
    "        })\n",
    "\n",
    "        margin=10\n",
    "        if 50-margin <= sep and sep <= 50+margin:\n",
    "            cluster_outdir = good_dir\n",
    "        else:\n",
    "            cluster_outdir = other_dir\n",
    "        plt.suptitle(f'Nup ID: {cid}, N points: {df.shape[0]}, {septxt}')\n",
    "        plt.show()\n",
    "\n",
    "def load_and_filter_locs(args):\n",
    "    locs, info = io.load_locs(args['locs'])\n",
    "    locs = pd.DataFrame.from_records(locs)\n",
    "    try:\n",
    "        assert info[1]['Pixelsize'] == args['pixel_size']\n",
    "    except AssertionError:\n",
    "        print('Pixel size mismatch', info[1]['Pixelsize'],  args['pixel_size'])\n",
    "        quit(1)\n",
    "\n",
    "    if args['picked_locs']:\n",
    "        picked_locs, old_info = io.load_locs(args['picked_locs'])\n",
    "        picked_locs = pd.DataFrame.from_records(picked_locs)\n",
    "        locs = locs.merge(picked_locs, on=['x', 'y', 'photons', 'bg', 'lpx', 'lpy', 'net_gradient', 'iterations', 'frame', 'likelihood', 'sx', 'sy'])\n",
    "    locs['clusterID'] = locs['group']\n",
    "    locs['z'] = locs['z [nm]'] / args['pixel_size']\n",
    "    return locs\n",
    "\n",
    "locs = '/home/miguel/Projects/smlm_z/publication/VIT_openframe/out_roll_alignment/out_nup/locs_3d.hdf5'\n",
    "picked_locs = '/home/miguel/Projects/data/20230601_MQ_celltype/nup/fov2/storm_1/storm_1_MMStack_Default.ome_locs_undrifted_picked_4.hdf5'\n",
    "\n",
    "args = {\n",
    "    'locs': locs,\n",
    "    'picked_locs': picked_locs,\n",
    "    'pixel_size': 86,\n",
    "    'blur_method': 'gaussian',\n",
    "    'min_blur': 0.001,\n",
    "    'oversample': 20,\n",
    "}\n",
    "\n",
    "locs = load_and_filter_locs(args)\n",
    "\n",
    "write_nup_plots(locs, args, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445bae89-e921-4cb2-95fc-c808bce62617",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "\n",
    "for cid in [6, 18, 19, 21, 22]:\n",
    "    cid_locs = locs[locs['clusterID']==cid]\n",
    "    df2 = filter_locs(cid_locs).copy()\n",
    "    del df2['index']\n",
    "    \n",
    "    kde = gaussian_kde(df2['z [nm]'].to_numpy())\n",
    "    kde.set_bandwidth(bw_method='silverman')\n",
    "    kde.set_bandwidth(kde.factor * 0.75)\n",
    "    \n",
    "    zvals = np.linspace(df2['z [nm]'].min()-25, df2['z [nm]'].max()+25, 50)\n",
    "    \n",
    "    score = kde(zvals)\n",
    "    zvals = zvals.squeeze()\n",
    "    \n",
    "    peaks, _ = find_peaks(score)\n",
    "    \n",
    "    sorted_peaks = sorted(peaks, key=lambda peak_index: zvals[peak_index], reverse=False)\n",
    "    peak_vals = zvals[peaks]\n",
    "    \n",
    "    if len(peak_vals) == 1:\n",
    "        n_peaks = 1\n",
    "    else:\n",
    "        n_peaks = 2\n",
    "        \n",
    "    sorted_peaks = sorted_peaks[-2:]\n",
    "    z_peaks = zvals[sorted_peaks]\n",
    "\n",
    "    seperation = np.diff(z_peaks)[0]\n",
    "\n",
    "    z_between_peaks = np.linspace(min(z_peaks), max(z_peaks), 50)\n",
    "    scores = kde(z_between_peaks)\n",
    "    \n",
    "    density_cutoff = min(scores) * 1.05\n",
    "    print('Cutoff', density_cutoff)\n",
    "    \n",
    "    df2['density'] = kde(df2['z [nm]'].to_numpy())\n",
    "    \n",
    "    df3 = df2[df2['density']>=density_cutoff]\n",
    "    \n",
    "    \n",
    "    for _df, title in zip([df2, df3], ['raw', 'w/ min density']):\n",
    "        fig = plt.figure()\n",
    "        plt.axis('off')\n",
    "        plt.subplots_adjust(wspace=0.3, hspace=0)\n",
    "        plt.title(title)\n",
    "        gs = fig.add_gridspec(1, 3)\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        render_locs(_df, args, (np.pi/2,0,0), barsize=50, ax=ax1)\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        render_locs(_df, args, (0,np.pi/2,0), barsize=50, ax=ax2)\n",
    "\n",
    "        ax3 = fig.add_subplot(gs[0,2])\n",
    "        histplot = sns.histplot(data=_df, x='z [nm]', stat='density', legend=False)\n",
    "        if title == 'raw':\n",
    "            # Plot KDE\n",
    "            ax3.plot(zvals, score)\n",
    "            # PLOT cutoff line\n",
    "            x = [_df['z [nm]'].min(), _df['z [nm]'].max()]\n",
    "            y = [density_cutoff, density_cutoff]\n",
    "            ax3.plot(x, y, 'r--')\n",
    "\n",
    "            # Plot peaks\n",
    "            for peak in z_peaks:\n",
    "                x = [peak, peak]\n",
    "                y = [0, kde(peak).squeeze()]\n",
    "                ax3.plot(x, y, 'r--')\n",
    "                ax3.set_title(f'Sep: {round(seperation, 2)}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75fa74f-1366-4823-81a6-46a570d865f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaborn._statistics import KDE\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [3, 3]\n",
    "\n",
    "locs = locs[locs['clusterID']==6]\n",
    "df2 = filter_locs(locs).copy()\n",
    "del df2['index']\n",
    "fig = plt.figure()\n",
    "ax2 = plt.gca()\n",
    "histplot = sns.histplot(data=df2, x='z [nm]', ax=ax2, stat='density', legend=False)\n",
    "sns.kdeplot(data=df2, x='z [nm]', ax=ax2, bw_adjust=0.5, color='black', bw_method='silverman')\n",
    "\n",
    "kde = KDE(bw_method='silverman', bw_adjust=0.5).fit(df2[['z [nm]']])\n",
    "\n",
    "zvals = np.linspace(df2['z [nm]'].min()-25, df2['z [nm]'].max()+25, 50).reshape(-1, 1)\n",
    "\n",
    "score = kde.score_samples(zvals)\n",
    "score = np.exp(score)\n",
    "print(score)\n",
    "zvals = zvals.squeeze()\n",
    "plt.plot(zvals, score, c='red')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# peaks, _ = find_peaks(y)\n",
    "\n",
    "# sorted_peaks = sorted(peaks, key=lambda peak_index: y[peak_index], reverse=True)\n",
    "# peak_vals = y[peaks]\n",
    "# if len(peak_vals) == 1:\n",
    "#     n_peaks = 1\n",
    "# else:\n",
    "#     n_peaks = 2\n",
    "    \n",
    "# sorted_peaks = sorted_peaks[:n_peaks]\n",
    "\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = plt.gca()\n",
    "# render_locs(df2, args, (np.pi/2,0,0), barsize=50, ax=ax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ee4268-4701-48ac-abc4-506d7e02e282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14584ede-a5cb-4fe5-bcda-32d9bddae89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import euclidean_distances\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def norm_zero_one(s):\n",
    "    min_val = s.min()\n",
    "    max_val = s.max()\n",
    "    return (s - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.metrics import mean_squared_error\n",
    "from tqdm import tqdm, trange\n",
    "from tifffile import imread\n",
    "\n",
    "UPSCALE_RATIO = 1\n",
    "\n",
    "def norm_sum_imgs(psf):\n",
    "    psf_sums = psf.sum(axis=(1,2))\n",
    "    psf = psf / psf_sums[:, np.newaxis, np.newaxis]\n",
    "    return psf\n",
    "\n",
    "\n",
    "def tf_eval_roll(ref_psf, psf, roll):\n",
    "    return tf.reduce_mean(mean_squared_error(ref_psf, tf.roll(psf, roll, axis=0)))\n",
    "    \n",
    "def tf_find_optimal_roll(ref_tf, img, upscale_ratio=UPSCALE_RATIO):\n",
    "    \n",
    "    img_tf = tf.convert_to_tensor(img)\n",
    "\n",
    "    roll_range = ref_tf.shape[0]//4\n",
    "    rolls = np.arange(-roll_range, roll_range).astype(int)\n",
    "    errors = tf.map_fn(lambda roll: tf_eval_roll(ref_tf, img_tf, roll), rolls, dtype=tf.float64)\n",
    "    # idx = 0\n",
    "    # for roll in tqdm(rolls):\n",
    "    #     error = tf.eval_roll(ref_tf, img_tf, roll)\n",
    "    #     print(i, error)\n",
    "    #     errors[idx] = error\n",
    "    #     idx += 1\n",
    "    best_roll = rolls[tf.argmin(errors).numpy()]\n",
    "    # Prefer small backwards roll to large forwards roll\n",
    "    if abs(best_roll - img.shape[0]) < best_roll:\n",
    "        best_roll = best_roll - img.shape[0]\n",
    "    return best_roll/upscale_ratio\n",
    "\n",
    "\n",
    "\n",
    "def realign_beads(psfs, df, z_step, i):\n",
    "    from sklearn.metrics import euclidean_distances\n",
    "    df['dist'] = euclidean_distances(df[['x', 'y']].to_numpy(), [[df['x'].max()/2, df['y'].max()/2]])\n",
    "    ref_idx = np.argsort(df['dist'].to_numpy())[i]\n",
    "    print(ref_idx)\n",
    "    ref_offset = df.iloc[ref_idx]['offset']\n",
    "\n",
    "    ref_psf = norm_zero_one(psfs[ref_idx])\n",
    "    ref_tf = tf.convert_to_tensor(ref_psf)\n",
    "    rolls = []\n",
    "    for idx in trange(df.shape[0]):\n",
    "        if idx == ref_idx:\n",
    "            roll = 0\n",
    "        else:\n",
    "            psf2 = norm_zero_one(psfs[idx])\n",
    "            roll = -tf_find_optimal_roll(ref_tf, psf2)\n",
    "        print(roll)\n",
    "        rolls.append(roll)\n",
    "    rolls = np.array(rolls)\n",
    "    return rolls * z_step\n",
    "\n",
    "\n",
    "psfs = imread('/home/miguel/Projects/smlm_z/publication/VIT_openframe/stacks.ome.tif')\n",
    "locs = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/VIT_openframe/locs.hdf', key='locs')\n",
    "z_step = 10\n",
    "\n",
    "psfs = psfs[::5]\n",
    "locs = locs.iloc[::5]\n",
    "\n",
    "for i in range(5):\n",
    "    locs[f'offset_{i}'] = realign_beads(psfs, locs, z_step, i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dec8a6-9c11-44f0-90db-b7123f951106",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_psf = np.zeros(psfs.shape[1:])\n",
    "from data.visualise import show_psf_axial\n",
    "for p, offset in zip(psfs, locs['offset_0'].to_numpy()):\n",
    "    p = norm_zero_one(p)\n",
    "    rolled = np.roll(p, shift=-int(offset//z_step), axis=0)\n",
    "    empty_psf += rolled\n",
    "\n",
    "ref_psf = norm_zero_one(empty_psf)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80621172-fc9d-4d69-a287-46f52e4dc602",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(locs['offset_avg_ref'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cae89e-0031-4b65-908d-6bbc2c77f2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(ref_psf.max(axis=(1,2))) * z_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b5792c-ba7b-4f01-8342-2a1739d0eb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def realign_beads2(psfs, df, z_step, ref_psf):\n",
    "    ref_psf = norm_zero_one(ref_psf)\n",
    "    ref_tf = tf.convert_to_tensor(ref_psf)\n",
    "    rolls = []\n",
    "    for idx in trange(df.shape[0]):\n",
    "        if idx == ref_idx:\n",
    "            roll = 0\n",
    "        else:\n",
    "            psf2 = norm_zero_one(psfs[idx])\n",
    "            roll = -tf_find_optimal_roll(ref_tf, psf2)\n",
    "        rolls.append(roll)\n",
    "    rolls = np.array(rolls)\n",
    "    return rolls * z_step\n",
    "\n",
    "locs['offset_avg_ref'] = realign_beads2(psfs, locs, 10, ref_psf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3cc6bb-53d9-4b46-b06d-958b2e0971f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in list(locs):\n",
    "    if 'offset' in c:\n",
    "        plt.scatter(x, (locs[c]-locs[c].mean())-locs['offset_avg_ref'], label=c)\n",
    "        \n",
    "# plt.scatter(x, locs['offset_avg_ref'], label='avg')\n",
    "# for i in range(5):\n",
    "#     plt.scatter(x, locs[f'offset_{i}'], label=str(i))\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a31ed29-957b-4e19-a186-f91fe6101507",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(locs.shape[0])\n",
    "\n",
    "ds = []\n",
    "for i in range(5):\n",
    "    y = abs(locs[f'offset_{i}'] - locs['offset_0'].to_numpy())\n",
    "    plt.scatter(x, y)\n",
    "    print(np.mean(y))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2c855b-ad9e-4a0d-b751-3383e87a71fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871f6a41-83a4-4345-b1af-d4052372ff47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1488e74-9a32-401e-8624-8cd17c709e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22401917-2baf-4c47-bc4b-c41185234184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "old_df = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/VIT_013/locs.hdf', key='locs')\n",
    "new_df = pd.read_hdf('/media/Data/smlm_z_data/20231121_nup_miguel_zeiss/stacks/combined/locs.hdf', key='locs')\n",
    "\n",
    "print(old_df.shape, new_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d97e85-3dfd-4e34-8d34-7c59b8664172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tifffile import imread\n",
    "\n",
    "d = imread('/home/miguel/Projects/smlm_z/publication/tmp/tmp.tif')\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f48000-4155-4aa2-a7b6-1a49dec20744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def snr(p):\n",
    "    return p.max() / p.mean()\n",
    "\n",
    "snrs = np.array([snr(p) for p in d])\n",
    "min_snr = 2.0\n",
    "idx = np.argwhere(snrs>min_snr).squeeze()\n",
    "print(len(idx))\n",
    "\n",
    "from data.visualise import show_psf_axial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2211bcd-7a13-4a24-bb52-dd6563aeca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_sharpness(array):\n",
    "    gy, gx = np.gradient(array)\n",
    "    gnorm = np.sqrt(gx**2 + gy**2)\n",
    "    sharpness = np.average(gnorm)\n",
    "    return sharpness\n",
    "\n",
    "\n",
    "def reduce_img(psf):\n",
    "    return np.stack([get_sharpness(x) for x in psf])\n",
    "\n",
    "\n",
    "def filter_mse_xy(stack, max_mse, i):\n",
    "    # Define a 2D Gaussian function\n",
    "    def gaussian_2d(xy, amplitude, xo, yo, sigma_x, sigma_y, theta, offset):\n",
    "        x, y = xy\n",
    "        xo = float(xo)\n",
    "        yo = float(yo)\n",
    "        a = (np.cos(theta)**2) / (2 * sigma_x**2) + (np.sin(theta)**2) / (2 * sigma_y**2)\n",
    "        b = -(np.sin(2 * theta)) / (4 * sigma_x**2) + (np.sin(2 * theta)) / (4 * sigma_y**2)\n",
    "        c = (np.sin(theta)**2) / (2 * sigma_x**2) + (np.cos(theta)**2) / (2 * sigma_y**2)\n",
    "        g = offset + amplitude * np.exp(- (a * ((x - xo)**2) + 2 * b * (x - xo) * (y - yo) + c * ((y - yo)**2)))\n",
    "        return g.ravel()\n",
    "\n",
    "    sharp = reduce_img(stack)\n",
    "    idx = np.argmax(sharp)\n",
    "    image = stack[idx]\n",
    "\n",
    "\n",
    "    # Load and preprocess the image (e.g., convert to grayscale)\n",
    "    # For simplicity, let's generate a simple image for demonstration\n",
    "    image_size = image.shape[1]\n",
    "    x = np.linspace(0, image_size - 1, image_size)\n",
    "    y = np.linspace(0, image_size - 1, image_size)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    \n",
    "    # Fit the Gaussian to the image data\n",
    "    p0 = [1, image_size / 2, image_size / 2, 2, 2, 0, 0]  # Initial guess for parameters\n",
    "    bounds = [\n",
    "        (0, np.inf),\n",
    "        (image_size * (1/5), image_size * (4/5)),\n",
    "        (image_size * (1/5), image_size * (4/5)),\n",
    "        (0, image_size/3),\n",
    "        (0, image_size/3),\n",
    "        (-np.inf, np.inf),\n",
    "        (0, np.inf),\n",
    "    ]\n",
    "\n",
    "    image = image / image.max()\n",
    "\n",
    "    try:\n",
    "        popt, pcov = curve_fit(gaussian_2d, (x, y), image.ravel(), p0=p0, bounds=list(zip(*bounds)))\n",
    "    except RuntimeError:\n",
    "        print('XY fit failed')\n",
    "        popt = p0\n",
    "    render = gaussian_2d((x, y), *popt).reshape(image.shape)\n",
    "\n",
    "    render0 = gaussian_2d((x, y), *p0).reshape(image.shape)\n",
    "    error = mean_squared_error(render, image)\n",
    "\n",
    "    if error > max_mse:\n",
    "        d = 'bad'\n",
    "    else:\n",
    "        d = 'good'\n",
    "\n",
    "    \n",
    "    # show_psf_axial(stack, f'{i} {d} ' + '{:.3E}'.format(error), 30)\n",
    "    # Visualize the original image and the fitted Gaussian\n",
    "    print(p0)\n",
    "    print(popt)\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.title('Error' +  '{:.8E}'.format(error))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title('Original Image')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(render0)\n",
    "\n",
    "    plt.title('Initial Gaussian')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(render)\n",
    "    plt.title('Fitted Gaussian')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # plt.savefig(f'./{d}/{i}.png')\n",
    "    # plt.close()\n",
    "    print(error, max_mse, error <= max_mse)\n",
    "\n",
    "    return error <= max_mse\n",
    "\n",
    "# problem_idx = \n",
    "for i in [1548]:\n",
    "    filter_mse_xy(d[i], 0.005, i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4be75a-a3a7-4645-b1b9-360148210b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "model_dir = '/home/miguel/Projects/smlm_z/publication/VIT_035/out_roll_alignment'\n",
    "\n",
    "args = {\n",
    "    'outdir': model_dir\n",
    "}\n",
    "\n",
    "model = keras.models.load_model(os.path.join(args['outdir'], './latest_vit_model3'))\n",
    "test_data = tf.data.Dataset.load(os.path.join(args['outdir'], 'test'))\n",
    "pred_zs = model.predict(test_data, batch_size=4096)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7d0385-f3eb-4fd9-bd04-4f3eca45757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_z_coordinates(dataset):\n",
    "    zs = []\n",
    "    xys = []\n",
    "    for (_, xy), z in dataset.as_numpy_iterator():\n",
    "        zs.append(z)\n",
    "        xys.append(xy)\n",
    "\n",
    "    return np.concatenate(xys).squeeze(), np.concatenate(zs).squeeze()\n",
    "\n",
    "xys, zs = get_z_coordinates(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51686bfd-4d19-4a23-9da0-1455e19c37ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_zs = pred_zs.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c862aa1-a800-4eb4-9791-f273ff8aff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize as opt\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "def bestfit_error(z_true, z_pred):\n",
    "    def linfit(x, c):\n",
    "        return x + c\n",
    "\n",
    "    x = z_true\n",
    "    y = z_pred\n",
    "    popt, _ = opt.curve_fit(linfit, x, y, p0=[0])\n",
    "\n",
    "    x = np.linspace(z_true.min(), z_true.max(), len(y))\n",
    "    y_fit = linfit(x, popt[0])\n",
    "    error = root_mean_squared_error(y_fit, y)\n",
    "    return error, popt[0], y_fit, abs(y_fit-y)\n",
    "\n",
    "def remove_constant_error(xys, zs, pred_zs):\n",
    "    coords2 = ['_'.join(x.astype(str)) for x in xys]\n",
    "    all_errors = []\n",
    "    for num, c in enumerate(set(coords2)):\n",
    "        idx = [i for i, val in enumerate(coords2) if val==c]\n",
    "        _, _, _, errors = bestfit_error(zs[idx], pred_zs[idx])\n",
    "        all_errors.append(errors)\n",
    "    all_errors = np.concatenate(all_errors)\n",
    "    return all_errors\n",
    "\n",
    "corrected_errors = remove_constant_error(xys, zs, pred_zs)\n",
    "errors = abs(pred_zs-zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41f7500-8b45-4199-8d7a-083ba42260df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ries_data = pd.read_csv('/home/miguel/Projects/smlm_z/publication/ries_comparison_data.csv')\n",
    "cols = list(ries_data)\n",
    "ries_deeploc = ries_data[[c for c in cols if ('DeepLoc' in c) or ('z(nm)' in c)]].dropna().set_index('z(nm)')\n",
    "crlb_deeploc = ries_data[[c for c in cols if ('CRLB' in c) or ('z(nm)' in c)]].dropna().set_index('z(nm)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af60de4-e28c-45dd-80cc-5da5072a9080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "pred_zs = pred_zs.squeeze()\n",
    "zs = zs.squeeze()\n",
    "\n",
    "sns.regplot(x=pred_zs, y=corrected_errors, scatter=True, ci=95, order=5, x_bins=np.arange(-1000, 1000, 50), label='Our method')\n",
    "sns.lineplot(data=ries_deeploc)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sns.regplot(x=pred_zs, y=corrected_errors, scatter=True, ci=95, order=5, x_bins=np.arange(-1000, 1000, 50), label='Our method')\n",
    "sns.lineplot(data=crlb_deeploc)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bbe62c-0693-4363-99c1-c79a545d92b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad1edb-c77c-4129-80f2-45bb59f3d6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c6c7fd-562e-432a-aa8e-df4814002211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd69d53-9431-456f-825d-dc903a2b350f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf46ceb-0b72-416c-9d6c-f7940ca71520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22608d12-d843-4415-8196-0402ee184332",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stacks = imread('/media/Data/smlm_z_data/20231121_nup_miguel_zeiss/stacks/tmp.tif')\n",
    "print(stacks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc40522-4573-4165-893e-c919abbb7bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tifffile import imread\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def get_sharpness(array):\n",
    "    gy, gx = np.gradient(array)\n",
    "    gnorm = np.sqrt(gx**2 + gy**2)\n",
    "    sharpness = np.average(gnorm)\n",
    "    return sharpness\n",
    "\n",
    "\n",
    "def reduce_img(psf):\n",
    "    return np.stack([get_sharpness(x) for x in psf])\n",
    "    \n",
    "def filter_mse_xy(stack, max_mse, i):\n",
    "    # Define a 2D Gaussian function\n",
    "    def gaussian_2d(xy, amplitude, xo, yo, sigma_x, sigma_y, theta, offset):\n",
    "        x, y = xy\n",
    "        xo = float(xo)\n",
    "        yo = float(yo)\n",
    "        a = (np.cos(theta)**2) / (2 * sigma_x**2) + (np.sin(theta)**2) / (2 * sigma_y**2)\n",
    "        b = -(np.sin(2 * theta)) / (4 * sigma_x**2) + (np.sin(2 * theta)) / (4 * sigma_y**2)\n",
    "        c = (np.sin(theta)**2) / (2 * sigma_x**2) + (np.cos(theta)**2) / (2 * sigma_y**2)\n",
    "        g = offset + amplitude * np.exp(- (a * ((x - xo)**2) + 2 * b * (x - xo) * (y - yo) + c * ((y - yo)**2)))\n",
    "        return g.ravel()\n",
    "\n",
    "    sharp = reduce_img(stack)\n",
    "    idx = np.argmax(sharp)\n",
    "    image = stack[idx]\n",
    "\n",
    "    # Load and preprocess the image (e.g., convert to grayscale)\n",
    "    # For simplicity, let's generate a simple image for demonstration\n",
    "    image_size = image.shape[1]\n",
    "    x = np.linspace(0, image_size - 1, image_size)\n",
    "    y = np.linspace(0, image_size - 1, image_size)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "\n",
    "\n",
    "\n",
    "    # Fit the Gaussian to the image data\n",
    "    p0 = [1, image_size / 2, image_size / 2, 5, 5, 0, 0]  # Initial guess for parameters\n",
    "    bounds = [\n",
    "        (-np.inf, np.inf),\n",
    "        (image_size * (2/5), image_size * (3/5)),\n",
    "        (image_size * (2/5), image_size * (3/5)),\n",
    "        (0, np.inf),\n",
    "        (0, np.inf),\n",
    "        (-np.inf, np.inf),\n",
    "        (0, np.inf),\n",
    "    ]\n",
    "\n",
    "    fit_failed = False\n",
    "    try:\n",
    "        popt, pcov = curve_fit(gaussian_2d, (x, y), image.ravel(), p0=p0, bounds=list(zip(*bounds)))\n",
    "    except RuntimeError:\n",
    "        print('XY fit failed')\n",
    "        fit_failed = True\n",
    "        popt = p0\n",
    "\n",
    "    render = gaussian_2d((x, y), *popt).reshape(image.shape)\n",
    "\n",
    "    error = mean_squared_error(render, image)\n",
    "\n",
    "    # Visualize the original image and the fitted Gaussian\n",
    "    if error >= max_mse or fit_failed:\n",
    "        print(i)\n",
    "\n",
    "        for limits, val in zip(bounds, popt):\n",
    "            print(limits, val)\n",
    "    \n",
    "        plt.plot(sharp)\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(5, 3))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title('Original Image')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(render)\n",
    "        plt.title('Fitted Gaussian')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(error, max_mse, error <= max_mse)\n",
    "        show_psf_axial(stack, '', 30)\n",
    "\n",
    "\n",
    "    return error <= max_mse\n",
    "from data.visualise import show_psf_axial\n",
    "for i in range(stacks.shape[0]):\n",
    "    print(i)\n",
    "    filter_mse_xy(stacks[i], 10000, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85324714-17e1-4342-9bd3-8a0117eb53dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plane alignment for beads\n",
    "\n",
    "\n",
    "from numpy.linalg import lstsq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import linear_model\n",
    "def realign_beads(psfs, df):\n",
    "        \n",
    "    X_data = df[['x', 'y']].to_numpy()\n",
    "    Y_data = df['offset'].to_numpy()\n",
    "    \n",
    "    reg = linear_model.LinearRegression().fit(X_data, Y_data)\n",
    "    \n",
    "    print(\"coefficients of equation of plane, (a1, a2): \", reg.coef_)\n",
    "    \n",
    "    print(\"value of intercept, c:\", reg.intercept_)\n",
    "    \n",
    "    z_fit = reg.predict(X_data)\n",
    "    \n",
    "    error = abs(z_fit-Y_data)\n",
    "    perc_cutoff = np.percentile(error, 95)\n",
    "    \n",
    "    idx = np.argwhere(error<=perc_cutoff).squeeze()\n",
    "    X_data = X_data[idx]\n",
    "    Y_data = Y_data[idx]\n",
    "    \n",
    "    psfs = psfs[idx]\n",
    "    df = df.iloc[idx]\n",
    "    \n",
    "    reg = linear_model.LinearRegression().fit(X_data, Y_data)\n",
    "    print(\"coefficients of equation of plane, (a1, a2): \", reg.coef_)\n",
    "    \n",
    "    print(\"value of intercept, c:\", reg.intercept_)\n",
    "    \n",
    "    z_fit = reg.predict(X_data)\n",
    "    df['offset'] = z_fit\n",
    "    \n",
    "    return psfs, df\n",
    "\n",
    "df = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/VIT_031_redo2/locs.hdf', key='locs')\n",
    "\n",
    "realign_beads(np.zeros(df.shape), df)\n",
    "plt.hist(error)\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(data=df, x='x', y='y', hue='offset')\n",
    "plt.show()\n",
    "sns.scatterplot(data=df.iloc[idx], x='x', y='y', hue=z_fit)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f32e075-d8f8-4af5-ba24-3a8c56458f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "for n2 in glob.glob('/home/miguel/Projects/smlm_z/publication/VIT_03*/out*'):\n",
    "    n = n2.replace('/home/miguel/Projects/smlm_z/publication/', '')\n",
    "    if 'subset' in n or '035' in n:\n",
    "        continue\n",
    "    dirname, outdir = n.split('/')\n",
    "\n",
    "    nup_path = os.path.join(n2, 'out_nup', 'nup_renders2')\n",
    "    model_path = os.path.join(n2, 'latest_vit_model3')\n",
    "    if not os.path.exists(nup_path) and os.path.exists(model_path):\n",
    "        print(f'cd /home/miguel/Projects/smlm_z/publication/VIT_031_redo/{outdir} && python3 ../../localise_exp_sample.py -mo . -o out_nup && cd out_nup && python3 ../../../render_nup.py && cd /home/miguel/Projects/smlm_z/publication/VIT_031_redo;')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e21fb36-41d2-4c68-88ed-e40e8ff84f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "records = []\n",
    "\n",
    "reports = glob.glob('/home/miguel/Projects/smlm_z/publication/VIT_03*/out*/out_nup/nup_renders2/*.csv')\n",
    "\n",
    "def within_n(seps, sep):\n",
    "    return sum((50-sep<=seps) & (seps<=50+sep))\n",
    "\n",
    "for r in reports:\n",
    "    if 'plane_alignment' in r and not 'plane_alignment_3' in r:\n",
    "        continue\n",
    "    outdir = '/'.join(r.split('/')[0:8])\n",
    "    name = '_'.join(outdir.split('/')[-2:])\n",
    "    training_report = os.path.join(outdir, 'results', 'report.json')\n",
    "    with open(training_report) as f:\n",
    "        training_report_data = json.load(f)\n",
    "\n",
    "    nup_report = pd.read_csv(r)\n",
    "\n",
    "    bead_report = os.path.join(outdir, 'results', 'results.csv')\n",
    "    bead_report_data = pd.read_csv(bead_report)\n",
    "    bead_report_data = bead_report_data[bead_report_data['dataset']=='test']\n",
    "    \n",
    "    args = training_report_data['args']\n",
    "    gen_args = args['gen_args']\n",
    "    records.append({\n",
    "        'name': name,\n",
    "        'train_mae': training_report_data['train_mae'],\n",
    "        'val_mae': training_report_data['val_mae'],\n",
    "        'test_mae': training_report_data['test_mae'],\n",
    "        'aug_ratio': args['aug_ratio'],\n",
    "        'brightness': args['brightness'],\n",
    "        'gauss': args['gauss'],\n",
    "        'min_snr': gen_args['min_snr'],\n",
    "        'mean_seperation': np.mean(nup_report['seperation']),\n",
    "        'stdev_seperation': np.std(nup_report['seperation']),\n",
    "        'sep_count_40-60': within_n(nup_report['seperation'], 10),\n",
    "        'sep_count_45-55': within_n(nup_report['seperation'], 5),\n",
    "        'mean_bead_offset': np.mean(np.abs(bead_report_data['offset']))\n",
    "    })\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "df = pd.DataFrame.from_records(records)\n",
    "df.sort_values(['sep_count_45-55'], ascending=False, inplace=True)\n",
    "print(df.shape)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd5d62f-8071-47cd-a90f-8e63bfc29161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.regplot(data=df, x='mean_bead_offset', y='test_mae')\n",
    "plt.show()\n",
    "\n",
    "for c in ['train_mae', 'val_mae', 'test_mae', 'min_snr', 'aug_ratio', 'gauss', 'brightness', 'stdev_seperation', 'mean_seperation', 'mean_bead_offset']:\n",
    "    sns.scatterplot(data=df, x=c, y='sep_count_40-60')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bde2fc9-5707-4cad-bb6c-baabae29be3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tifffile import imread\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "args = {\n",
    "    'stacks': '/home/miguel/Projects/smlm_z/publication/VIT_031_redo_subset/stacks.ome.tif',\n",
    "    'locs': '/home/miguel/Projects/smlm_z/publication/VIT_031_redo_subset/locs.hdf',\n",
    "    'zstep': 10,\n",
    "    'zrange': 1000\n",
    "}\n",
    "psfs = imread(args['stacks'])[:, :, :, :, np.newaxis]\n",
    "locs = pd.read_hdf(args['locs'], key='locs')\n",
    "locs['idx'] = np.arange(locs.shape[0])\n",
    "# idx = (xlim[0] < all_locs['x']) & (all_locs['x'] < xlim[1]) & (ylim[0] < all_locs['y']) & (all_locs['y'] < ylim[1])\n",
    "# locs = all_locs[idx]\n",
    "# psfs = all_psfs[locs['idx']]\n",
    "\n",
    "def filter_zrange(X, zs):\n",
    "    psfs = X\n",
    "    valid_ids = np.argwhere(abs(zs.squeeze()) < args['zrange']).squeeze()\n",
    "    return psfs[valid_ids], zs[valid_ids]\n",
    "        \n",
    "ys = []\n",
    "for offset in locs['offset']:\n",
    "    zs = ((np.arange(psfs.shape[1])) * args['zstep']) - offset\n",
    "    ys.append(zs)\n",
    "\n",
    "ys = np.array(ys)\n",
    "\n",
    "psfs = np.concatenate(psfs)\n",
    "ys = np.concatenate(ys)\n",
    "\n",
    "psfs, ys = filter_zrange(psfs, ys)\n",
    "\n",
    "print(psfs.shape, ys.shape)\n",
    "\n",
    "print(ys.min(), ys.max())\n",
    "\n",
    "train = psfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd182945-1dc4-4371-a41c-792edce2a75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "spots_path = '/home/miguel/Projects/data/20230601_MQ_celltype/nup/fov2/storm_1/storm_1_MMStack_Default.ome_spots.hdf5'\n",
    "with h5py.File(spots_path, \"r\") as f:\n",
    "    test = np.array(f['spots'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d5a03c-ad48-4dd4-9d6e-3ab7f4f6237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensitivity = 0.45, quantum efficiency = 0.9\n",
    "\n",
    "# em gain (1) baseline (100)\n",
    "# new_spots = (spots - baseline) * sensitivity / (gain)\n",
    "\n",
    "baseline = 100\n",
    "sensitivity = 0.45\n",
    "gain = 1\n",
    "\n",
    "test2 = (raw_test * gain / sensitivity) + baseline\n",
    "\n",
    "test2 = test2.astype(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ded300-849e-4ddf-bc2a-9885a2cc1ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train = imread('/home/miguel/Projects/data/all_openframe_beads/20231205_miguel_mitochondria/stack__10_MMStack_Default.ome.tif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2334fb0a-6b58-4f0f-bfd9-75007473558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test = imread('/home/miguel/Projects/data/20230601_MQ_celltype/nup/fov2/storm_1_MMStack_Default_2.ome.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da839ac0-1305-4110-8bd9-a91330206686",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.min(), train.max(), train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aba8240-d453-47b0-af45-01baa8c70f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.min(), test.max(), test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceb28aa-96e0-4b47-a348-b1703a00bc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train.min(), raw_train.max(), raw_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed35892a-b0f7-4dbd-9392-a0b6ab8ffd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test.min(), raw_test.max(), raw_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bbda9b-0829-41eb-988a-7eb98f7abdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2.min(), test2.max(), test2.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7112b284-6f95-44b4-95fc-e05bf354fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras \n",
    "\n",
    "# from keras import layers\n",
    "# from keras.layers.pre\n",
    "\n",
    "# class NoClipRandomContrast(layers.RandomContrast):\n",
    "#     def __init__(self, factor, seed=None, clip=False, **kwargs):\n",
    "#         super().__init__(factor, seed, **kwargs)\n",
    "#         self.clip = clip\n",
    "\n",
    "#     def call(self, inputs, training=True):\n",
    "#         # inputs = self.backend.cast(inputs, self.compute_dtype)\n",
    "#         if training:\n",
    "#             seed_generator = self._get_seed_generator(self.backend._backend)\n",
    "#             factor = self.backend.random.uniform(\n",
    "#                 shape=(),\n",
    "#                 minval=1.0 - self.lower,\n",
    "#                 maxval=1.0 + self.upper,\n",
    "#                 seed=seed_generator,\n",
    "#                 dtype=self.compute_dtype,\n",
    "#             )\n",
    "\n",
    "#             outputs = self._adjust_constrast(inputs, factor)\n",
    "#             if self.clip:\n",
    "#                 outputs = self.backend.numpy.clip(outputs, 0, 255)\n",
    "#             self.backend.numpy.reshape(outputs, self.backend.shape(inputs))\n",
    "#             return outputs\n",
    "#         else:\n",
    "#             return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6f07b4-2940-4266-8358-21eb3b9b756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential, layers\n",
    "MAX_GAUSS_NOISE = 0.001\n",
    "MAX_TRANSLATION_PX = 0\n",
    "BRIGHTNESS = 0.2\n",
    "aug_pipeline = Sequential([\n",
    "    # layers.GaussianNoise(stddev=MAX_GAUSS_NOISE*X_train[0].max(), seed=args['seed']),\n",
    "    # layers.RandomTranslation(MAX_TRANSLATION_PX/img_size, MAX_TRANSLATION_PX/img_size, seed=args['seed']),\n",
    "    layers.RandomBrightness(BRIGHTNESS, value_range=[0, psfs.max()], seed=42),\n",
    "    NoClipRandomContrast(0.2, seed=42),\n",
    "    layers.RandomContrast(0.2, seed=42)\n",
    "])\n",
    "\n",
    "new_ds = aug_pipeline(train.astype(float), training=True).numpy()\n",
    "print(new_ds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bb43d0-bc97-4c5c-827d-43ac4e1e7488",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in [train, test, new_ds]:\n",
    "    print(ds.min(), ds.std(), ds.mean(), ds.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7373d51b-10c8-4016-a700-9bf70ee48f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_hdf('/home/miguel/Projects/data/20231212_miguel_openframe/tubulin/FOV1/storm_1/storm_1_MMStack_Default.ome_locs_undrifted.hdf5', key='locs')\n",
    "\n",
    "\n",
    "sns.scatterplot(data=df, x='x', y='y', alpha=0.1)\n",
    "plt.xlim((400, 600))\n",
    "plt.ylim((700, 1000))\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82790dc6-222c-4bae-803a-0552984bad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "old_locs = '/home/miguel/Projects/data/20230601_MQ_celltype/nup/fov2/storm_1/storm_1_MMStack_Default.ome_locs_undrifted_picked_4.hdf5'\n",
    "old_locs = pd.read_hdf(old_locs, key='locs')\n",
    "for c in set(old_locs['group']):\n",
    "    if c != 10:\n",
    "        continue\n",
    "    sub_df = old_locs[old_locs['group']==c]\n",
    "    print(sub_df['x'].max()-sub_df['x'].min())\n",
    "    # sns.scatterplot(data=sub_df, x='x', y='y')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4d9a82-9f62-485a-829c-2412587dab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_locs = pd.read_hdf('/home/miguel/Projects/smlm_z/publication/VIT_031_redo/out6/out_nup/locs_3d.hdf5', key='locs')\n",
    "new_locs = new_locs[new_locs['x'].isin(old_locs['x'])]\n",
    "\n",
    "locs = new_locs.merge(old_locs, on=['x', 'y', 'photons', 'bg', 'lpx', 'lpy', 'net_gradient', 'iterations', 'frame', 'likelihood', 'sx', 'sy'])\n",
    "locs['clusterID'] = locs['group']\n",
    "print(list(locs))\n",
    "sns.scatterplot(data=old_locs[old_locs['group']==10], x='x', y='y')\n",
    "plt.show()\n",
    "sns.scatterplot(data=locs[locs['group']==10], x='x', y='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abf3dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyotf.otf import HanserPSF, apply_aberration, apply_named_aberration\n",
    "from pyotf.phaseretrieval import retrieve_phase\n",
    "from pyotf.utils import prep_data_for_PR, center_data\n",
    "import sys\n",
    "sys.path.append('/home/miguel/Projects/uni/phd/smlm_z')\n",
    "from data.estimate_offset import get_peak_sharpness\n",
    "from data.visualise import grid_psfs, show_psf_axial\n",
    "\n",
    "def mse(y, y_pred):\n",
    "    return np.mean((y - y_pred) ** 2).sum()\n",
    "\n",
    "psf = X[0]\n",
    "model_kwargs = dict(\n",
    "    wl=0.665,\n",
    "    na=0.9,\n",
    "    ni=1.34,\n",
    "    res=0.09,\n",
    "    zres=0.01,\n",
    "    size=psf.shape[1],\n",
    "    zsize=psf.shape[0],\n",
    "    vec_corr=\"none\",\n",
    "    condition=\"none\",\n",
    ")\n",
    "print(model_kwargs)\n",
    "psf = prep_data_for_PR(psf, multiplier=1.0)\n",
    "\n",
    "# Retrieve phase for experimental PSF\n",
    "PR_result = retrieve_phase(psf, model_kwargs, 100, 1e-4, 1e-4)\n",
    "\n",
    "PR_result.fit_to_zernikes(16)\n",
    "PR_result.plot()\n",
    "PR_result.zd_result.plot()\n",
    "PR_result.zd_result.plot_named_coefs()\n",
    "PR_result.plot_convergence()\n",
    "\n",
    "# Simulate HanserPSF with parameters\n",
    "\n",
    "result_psf = PR_result.generate_zd_psf(sphase=slice(None))\n",
    "\n",
    "# this part is very kludgy\n",
    "PR_result.model.PSFi = psf\n",
    "\n",
    "PR_result.model.PSFi = result_psf\n",
    "\n",
    "psf = psf / psf.max()\n",
    "result_psf = result_psf / result_psf.max()\n",
    "\n",
    "print(mse(psf, result_psf))\n",
    "\n",
    "plt.show()\n",
    "print('Experimental')\n",
    "show_psf_axial(psf / psf.max(), '', 15)\n",
    "print('Simulated')\n",
    "show_psf_axial(result_psf / result_psf.max(), '', 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11d3a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_img = np.zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94bb91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(PR_result.model.OTFa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bae29e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "xy = np.random.uniform(-1, 1, size=(50, 2))\n",
    "vals = np.random.uniform(0, 500, size=(50, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfef597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyotf\n",
    "import pyotf.zernike\n",
    "pyotf.zernike.name2noll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aba325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import euclidean_distances\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "s = np.linspace(-1, 1, 25)\n",
    "x, y = np.meshgrid(s, s)\n",
    "x = x.flatten()\n",
    "y = y.flatten()\n",
    "xy = np.stack((x,y)).T\n",
    "\n",
    "dists = euclidean_distances([[0, 0]], xy).squeeze()\n",
    "dists = np.power(dists, 3)\n",
    "dists /= dists.max()\n",
    "sns.scatterplot(x=x, y=y, hue=dists)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ef1fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/miguel/Projects/uni/phd/smlm_z')\n",
    "from data.visualise import grid_psfs, show_psf_axial\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import euclidean_distances\n",
    "import seaborn as sns\n",
    "model_kwargs = dict(\n",
    "    wl=647,\n",
    "    na=1.3,\n",
    "    ni=1.51,\n",
    "    res=90,\n",
    "    zres=50,\n",
    "    size=32,\n",
    "    zsize=100,\n",
    "    vec_corr=\"none\",\n",
    "    condition=\"none\",\n",
    ")\n",
    "from pyotf.otf import HanserPSF, apply_aberration, apply_named_aberration\n",
    "\n",
    "def get_ab(a1, a2):\n",
    "    base_args = np.array([0, 0, 0, 0, 0, a1, a2])\n",
    "    return base_args\n",
    "    \n",
    "def gen_fake_psf(model_kwargs, ab):\n",
    "    model = HanserPSF(**model_kwargs)\n",
    "    # model = apply_named_aberration(model, 'oblique astigmatism', 2)\n",
    "    model = apply_aberration(model, np.zeros(ab.shape), ab)\n",
    "\n",
    "    psf = model.PSFi\n",
    "    psf = psf.astype(float)\n",
    "    # psf = psf / psf.max()\n",
    "    return psf\n",
    "\n",
    "def add_noise(psf):\n",
    "    return psf + np.random.normal(0, 1e-2, size=psf.shape)\n",
    "\n",
    "coords = []\n",
    "psfs = []\n",
    "\n",
    "n_points = 50\n",
    "\n",
    "lim = 1\n",
    "xy = np.random.uniform(-1, 1, size=(n_points, 2))\n",
    "# xy = np.stack([np.linspace(-lim, lim, n_points), np.linspace(-lim, lim, n_points)]).T\n",
    "center = [[0, 0]]\n",
    "dists = euclidean_distances(xy, center).squeeze()\n",
    "# dists = 2**dists\n",
    "dists = np.power(dists, 3)\n",
    "dists /= dists.max()\n",
    "\n",
    "a1s = []\n",
    "a2s = []\n",
    "\n",
    "for i in range(n_points):\n",
    "    x, y = xy[i]\n",
    "    dist = dists[i]\n",
    "\n",
    "    a1 = (1 if x>0 else -1) * dist\n",
    "    a2 = (1 if y>0 else -1) * dist\n",
    "    a1s.append(a1)\n",
    "    a2s.append(a2)\n",
    "    ab = get_ab(a1, a2)\n",
    "    psf = gen_fake_psf(model_kwargs, ab*2)\n",
    "#     psf = add_noise(psf)\n",
    "    coords.append([x, y])\n",
    "    psfs.append(psf)\n",
    "#     plt.imshow(grid_psfs(psf[::7]).squeeze())\n",
    "#     plt.show()\n",
    "\n",
    "psfs = np.array(psfs)\n",
    "df = pd.DataFrame(coords, columns=['x', 'y'])\n",
    "sns.scatterplot(x=xy[:, 0], y=xy[:, 1], hue=a1s)\n",
    "plt.show()\n",
    "sns.scatterplot(x=xy[:, 0], y=xy[:, 1], hue=a2s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f76e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(psfs.shape)\n",
    "psfs.reshape((psfs.shape[0], -1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732760d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "pca = PCA().fit(psfs.reshape((psfs.shape[0], -1)))\n",
    "\n",
    "d = pca.transform(psfs.reshape((psfs.shape[0], -1)))\n",
    "sns.scatterplot(x=d[:, 0], y=d[:, 1], hue=dists)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac52e8e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# randomly roll psfs\n",
    "import seaborn as sns\n",
    "\n",
    "z_step = 50\n",
    "\n",
    "# subsample psfs\n",
    "# subsample = 5\n",
    "# psfs = psfs[:, ::subsample, :, :]\n",
    "# z_step *= subsample\n",
    "\n",
    "def roll_psf(psf, roll):\n",
    "    rolled_psf = np.roll(psfs[i], shift=roll, axis=0)\n",
    "#     if roll < 0:\n",
    "#         rolled_psf[roll:] = 0\n",
    "#     else:\n",
    "#         rolled_psf[:roll] = 0\n",
    "    return rolled_psf\n",
    "#     show_psf_axial(psfs[i], '', 7)\n",
    "#     show_psf_axial(rolled_psf, roll, 7)\n",
    "\n",
    "rolls = []\n",
    "for i in range(psfs.shape[0]):\n",
    "    roll = np.random.randint(-5, 5)\n",
    "    psfs[i] = roll_psf(psfs[i], roll)\n",
    "    rolls.append(roll)\n",
    "df['roll'] = np.array(rolls) * z_step\n",
    "# df['roll'] *= 10\n",
    "\n",
    "\n",
    "\n",
    "sns.scatterplot(data=df, x='x', y='y', hue='roll')\n",
    "plt.show()\n",
    "for i in range(psfs.shape[0]):\n",
    "    show_psf_axial(psfs[i], i, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0836608a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pca.transform(psfs.reshape((psfs.shape[0], -1)))\n",
    "sns.scatterplot(x=d[:, 0], y=d[:, 1], hue=dists)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3936f083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load real data\n",
    "# import pandas as pd\n",
    "# from tifffile import imread\n",
    "# import matplotlib.pyplot as plt\n",
    "# df = pd.read_csv('/home/miguel/Projects/uni/phd/smlm_z/smlm-z/data/05_model_input/coords.csv')\n",
    "# psfs = imread('/home/miguel/Projects/uni/phd/smlm_z/smlm-z/data/05_model_input/spots.tif')\n",
    "# df['roll'] = 0\n",
    "# assert psfs.shape[0] == df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295417e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.exposure import match_histograms\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "from multiprocessing import Pool\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "import tqdm\n",
    "from data.estimate_offset import get_peak_sharpness\n",
    "from data.visualise import show_psf_axial\n",
    "from keras.losses import MeanSquaredError\n",
    "from multiprocessing.spawn import prepare\n",
    "import numpy as np\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from tqdm import trange\n",
    "import tensorflow as tf\n",
    "\n",
    "mse = MeanSquaredError()\n",
    "DEBUG = False\n",
    "UPSCALE_RATIO = 10\n",
    "def norm_zero_one(s):\n",
    "    max_s = s.max()\n",
    "    min_s = s.min()\n",
    "    return (s - min_s) / (max_s - min_s)\n",
    "\n",
    "\n",
    "def pad_and_fit_spline(coords, psf, z, z_ups):\n",
    "    x, y = coords\n",
    "    zs = psf[:, x, y]\n",
    "    cs = UnivariateSpline(z, zs, k=1, s=1e-3)\n",
    "    if False:\n",
    "        plt.scatter(z, zs, label='raw')\n",
    "        plt.plot(z_ups, cs(z_ups), label='smooth')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return x, y, cs(z_ups)\n",
    "    \n",
    "def upsample_psf(psf, ratio=UPSCALE_RATIO):\n",
    "    pad_width = 10\n",
    "    z = np.arange(-pad_width, psf.shape[0] + pad_width)\n",
    "    z_ups = np.arange(0, psf.shape[0], 1/ratio)\n",
    "    upsampled_psf = np.zeros((z_ups.shape[0], *psf.shape[1:]))\n",
    "    \n",
    "    psf = np.pad(psf, ((pad_width, pad_width), (0, 0), (0, 0)), mode='edge')\n",
    "    xys = list(product(np.arange(psf.shape[1]), np.arange(psf.shape[2])))\n",
    "    func = partial(pad_and_fit_spline, psf=psf, z=z, z_ups=z_ups)\n",
    "    res = list(map(func, xys))\n",
    "    # with Pool(8) as p:\n",
    "    #     res = list(p.imap(func, xys))\n",
    "    for x, y, z_col in res:\n",
    "        upsampled_psf[:, x, y] = z_col\n",
    "\n",
    "    return upsampled_psf\n",
    "\n",
    "\n",
    "def plot_correction(target, img, psf_corrected, errors):\n",
    "    if True:\n",
    "        plt.plot(target.max(axis=(1,2)), label='target')\n",
    "        plt.plot(img.max(axis=(1,2)),  label='original')\n",
    "        plt.plot(psf_corrected.max(axis=(1,2)), label='corrected', )\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "mse = MeanSquaredError(reduction='sum')\n",
    "def loss_func(true_m, pred_m):\n",
    "    m = tf.math.abs(true_m-pred_m)\n",
    "    m = tf.math.square(m*pred_m)\n",
    "    return tf.math.reduce_mean(m)\n",
    "\n",
    "def tf_find_optimal_roll(target, img, upscale_ratio=UPSCALE_RATIO):\n",
    "    ref_tf = tf.convert_to_tensor(target)\n",
    "    img_tf = tf.convert_to_tensor(img)\n",
    "    errors = []\n",
    "\n",
    "    for i in range(img.shape[0]):\n",
    "#         error = loss_func(ref_tf, img_tf)\n",
    "        error = mse(ref_tf, img_tf)\n",
    "        errors.append(error)\n",
    "        img_tf = tf.roll(img_tf, 1, axis=0)\n",
    "\n",
    "    best_i = tf.argmin(errors).numpy()\n",
    "    # Prefer small backwards roll to large forwards roll\n",
    "    if abs(best_i - img.shape[0]) < best_i:\n",
    "        best_i = best_i - img.shape[0]\n",
    "\n",
    "    psf_corrected = np.roll(img, int(best_i), axis=0)\n",
    "    plot_correction(target, img, psf_corrected, errors)\n",
    "\n",
    "    return best_i/upscale_ratio\n",
    "\n",
    "def prepare_psf(psf):\n",
    "#     psf = gaussian(psf, sigma=1)\n",
    "    psf = psf.copy()\n",
    "    psf = np.square(psf)\n",
    "    psf = norm_zero_one(psf)\n",
    "    psf = upsample_psf(psf)\n",
    "#     psf = mask_img_stack(psf, 12)\n",
    "    return psf\n",
    "\n",
    "\n",
    "def align_psfs(psf, psf2):\n",
    "    psf = prepare_psf(psf)\n",
    "    psf2 = prepare_psf(psf2)\n",
    "    psf = match_histograms(psf, psf2)\n",
    "    offset = tf_find_optimal_roll(psf, psf2)\n",
    "    return offset * z_step\n",
    "\n",
    "def find_seed_psf(df):\n",
    "    # Seed PSF - most centered PSF in FOV\n",
    "    center = df[['x', 'y']].mean(axis=0).to_numpy()\n",
    "    coords = df[['x', 'y']].to_numpy()\n",
    "    dists = euclidean_distances([center], coords).squeeze()\n",
    "    first_point = np.argmin(dists)\n",
    "    print(first_point)\n",
    "    return first_point\n",
    "\n",
    "def get_or_prepare_psf(prepped_psfs, raw_psfs, idx):\n",
    "    if idx not in prepped_psfs:\n",
    "        prepped_psfs[idx] = prepare_psf(raw_psfs[idx])\n",
    "    return prepped_psfs[idx]\n",
    "\n",
    "xys = df[['x', 'y']].to_numpy()\n",
    "\n",
    "errors = []\n",
    "def classic_align_psfs(psfs, df):\n",
    "    print(f'Aligning {psfs.shape} psfs...')\n",
    "\n",
    "    seed_psf = find_seed_psf(df)\n",
    "    ref_psf = prepare_psf(psfs[seed_psf])\n",
    "    offsets = np.zeros((psfs.shape[0]))\n",
    "\n",
    "    ref_0 = get_peak_sharpness(psfs[seed_psf])\n",
    "\n",
    "    for i in trange(0, psfs.shape[0]):\n",
    "        if i == seed_psf:\n",
    "            offsets[i] = 0\n",
    "            errors.append(0)\n",
    "            continue\n",
    "        psf = psfs[i]\n",
    "        psf = prepare_psf(psf)\n",
    "#         psf = match_histograms(psf, ref_psf)\n",
    "        offset = tf_find_optimal_roll(ref_psf, psf) * z_step\n",
    "        offsets[i] = offset\n",
    "        correct_dist = df['roll'][seed_psf] - df['roll'][i]\n",
    "        euc_dist = euclidean_distances([xys[i]], [xys[seed_psf]]).squeeze()\n",
    "        errors.append(correct_dist)\n",
    "        print(f\"{seed_psf} -> {i}, {offset}, {correct_dist}\")\n",
    "        if DEBUG:\n",
    "            offset_psf = np.roll(psf, shift=-int(offset), axis=0)\n",
    "            imgs = np.concatenate((ref_psf, offset_psf), axis=2)\n",
    "            show_psf_axial(imgs, subsample_n=30)\n",
    "            \n",
    "#         plt.imshow(grid_psfs(psf[::5]).T)\n",
    "#         plt.show()\n",
    "#         plt.imshow(grid_psfs(ref_psf[::5]).T)\n",
    "#         plt.show()\n",
    "\n",
    "#     offsets -= ref_0\n",
    "\n",
    "    return offsets\n",
    "\n",
    "classic_offsets = classic_align_psfs(psfs, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f51f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import skimage.filters as filters\n",
    "\n",
    "def gaussian(x, amplitude, mean, stddev):\n",
    "    return amplitude * np.exp(-(x - mean) ** 2 / (2 * stddev ** 2))\n",
    "\n",
    "def measure_psf_fwhm(psf):\n",
    "    # Normalize the PSF to range [0, 1]\n",
    "    psf_norm = (psf - np.min(psf)) / (np.max(psf) - np.min(psf))\n",
    "    \n",
    "    # Find the center of the PSF using the maximum intensity\n",
    "    center = np.unravel_index(np.argmax(psf_norm), psf_norm.shape)\n",
    "    # Extract a 1D slice of the PSF along the z-axis passing through the center\n",
    "    z_slice = psf_norm[:, center[0]]\n",
    "    \n",
    "    # Estimate the initial parameters of the Gaussian fit\n",
    "    amplitude = np.max(z_slice) - np.min(z_slice)\n",
    "    mean = center[0]\n",
    "    stddev = 2\n",
    "    \n",
    "    # Fit the Gaussian to the 1D slice using least squares optimization\n",
    "    try:\n",
    "        popt, _ = opt.curve_fit(gaussian, np.arange(z_slice.size), z_slice, p0=[amplitude, mean, stddev])\n",
    "    except RuntimeError:\n",
    "        return np.inf\n",
    "    # Compute the FWHM of the Gaussian fit\n",
    "    fwhm = 2 * np.sqrt(2 * np.log(2)) * popt[2]\n",
    "    \n",
    "    return fwhm\n",
    "\n",
    "def determine_best_focus_slice(psf):\n",
    "    # Measure the FWHM of the PSF for each z-slice\n",
    "    fwhm_values = []\n",
    "    for i in range(psf.shape[0]):\n",
    "        fwhm = measure_psf_fwhm(psf[i])\n",
    "        fwhm_values.append(fwhm)\n",
    "    \n",
    "    # Find the index of the z-slice with the minimum FWHM value\n",
    "    best_slice_idx = np.argmin(fwhm_values)\n",
    "    plt.rcParams['figure.figsize'] = [2, 2]\n",
    "    print(f'best slice: {best_slice_idx}')\n",
    "    slices = psf[[best_slice_idx-5, best_slice_idx, best_slice_idx], :, :]\n",
    "    show_psf_axial(slices, '', 1)\n",
    "    return best_slice_idx\n",
    "\n",
    "def fwhm_offsets(psfs):\n",
    "    idxs = np.array([determine_best_focus_slice(psf) for psf in psfs]).astype(float)\n",
    "    idxs -= np.mean(idxs)\n",
    "    return idxs\n",
    "\n",
    "# measure_psf_fwhm(psfs[0][0])\n",
    "new_offsets = fwhm_offsets(psfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb4bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Alignment error (frames, 50nm z-step)')\n",
    "sns.scatterplot(data=df, x='x', y='y', hue=abs(df['roll']-new_offsets))\n",
    "plt.show()\n",
    "plt.scatter(df['roll'], abs(df['roll']+classic_offsets))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84433e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.utils.graph_shortest_path import graph_shortest_path\n",
    "import networkx as nx\n",
    "\n",
    "def get_center_point(df):\n",
    "    center_point = [[0, 0]]\n",
    "    df['dists'] = euclidean_distances(df[['x', 'y']], center_point)\n",
    "    idx = df['dists'].idxmin()\n",
    "    return df.iloc[idx]\n",
    "\n",
    "dists = euclidean_distances(df[['x', 'y']])\n",
    "\n",
    "G = nx.from_numpy_matrix(dists)\n",
    "G = nx.minimum_spanning_tree(G)\n",
    "center_point = get_center_point(df)\n",
    "\n",
    "from itertools import combinations\n",
    "for src, target in G.edges:\n",
    "    G[src][target]['weight'] = dists[src, target]\n",
    "\n",
    "nx.draw(G, pos=df[['x', 'y']].values, with_labels=True, node_size=100, node_color='lightgreen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc2ee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.utils.graph_shortest_path import graph_shortest_path\n",
    "from tqdm import trange\n",
    "import networkx as nx\n",
    "\n",
    "UPSCALE_RATIO = 10\n",
    "\n",
    "def pad_and_fit_spline(coords, psf, z, z_ups):\n",
    "    x, y = coords\n",
    "    zs = psf[:, x, y]\n",
    "    cs = UnivariateSpline(z, zs, k=1, s=1e-6)\n",
    "    if False:\n",
    "        plt.scatter(z, zs, label='raw')\n",
    "        plt.plot(z_ups, cs(z_ups), label='smooth')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return x, y, cs(z_ups)\n",
    "    \n",
    "def upsample_psf(psf, ratio=UPSCALE_RATIO):\n",
    "    pad_width = 10\n",
    "    z = np.arange(-pad_width, psf.shape[0] + pad_width)\n",
    "    z_ups = np.arange(0, psf.shape[0], 1/ratio)\n",
    "    upsampled_psf = np.zeros((z_ups.shape[0], *psf.shape[1:]))\n",
    "    \n",
    "    psf = np.pad(psf, ((pad_width, pad_width), (0, 0), (0, 0)), mode='edge')\n",
    "    xys = list(product(np.arange(psf.shape[1]), np.arange(psf.shape[2])))\n",
    "    func = partial(pad_and_fit_spline, psf=psf, z=z, z_ups=z_ups)\n",
    "    res = list(map(func, xys))\n",
    "    # with Pool(8) as p:\n",
    "    #     res = list(p.imap(func, xys))\n",
    "    for x, y, z_col in res:\n",
    "        upsampled_psf[:, x, y] = z_col\n",
    "\n",
    "    return upsampled_psf\n",
    "\n",
    "\n",
    "def plot_correction(target, img, psf_corrected, errors):\n",
    "    if True:\n",
    "        plt.plot(target.max(axis=(1,2)), label='target')\n",
    "        plt.plot(img.max(axis=(1,2)),  label='original')\n",
    "        plt.plot(psf_corrected.max(axis=(1,2)), label='corrected', )\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "mse = MeanSquaredError(reduction='sum')\n",
    "def loss_func(true_m, pred_m):\n",
    "    m = tf.math.abs(true_m-pred_m)\n",
    "    m = tf.math.square(m*true_m)\n",
    "    return tf.math.reduce_mean(m)\n",
    "\n",
    "def tf_find_optimal_roll(target, img, upscale_ratio=UPSCALE_RATIO):\n",
    "    ref_tf = tf.convert_to_tensor(target)\n",
    "    img_tf = tf.convert_to_tensor(img)\n",
    "    errors = []\n",
    "\n",
    "    for i in range(img.shape[0]):\n",
    "#         error = loss_func(ref_tf, img_tf)\n",
    "        error = mse(ref_tf, img_tf)\n",
    "        errors.append(error)\n",
    "        img_tf = tf.roll(img_tf, 1, axis=0)\n",
    "\n",
    "    best_i = tf.argmin(errors).numpy()\n",
    "    # Prefer small backwards roll to large forwards roll\n",
    "    if abs(best_i - img.shape[0]) < best_i:\n",
    "        best_i = best_i - img.shape[0]\n",
    "\n",
    "    psf_corrected = np.roll(img, int(best_i), axis=0)\n",
    "    plot_correction(target, img, psf_corrected, errors)\n",
    "\n",
    "    return best_i/upscale_ratio\n",
    "\n",
    "\n",
    "def prepare_psf(psf):\n",
    "#     psf = gaussian(psf, sigma=1)\n",
    "    psf = psf.copy()\n",
    "    psf = psf * psf\n",
    "    psf = norm_zero_one(psf.copy())\n",
    "    psf = upsample_psf(psf)\n",
    "    # psf = mask_img_stack(psf, 12)\n",
    "    return psf\n",
    "\n",
    "\n",
    "def align_psfs(psf, psf2):\n",
    "    psf = prepare_psf(psf)\n",
    "    psf2 = prepare_psf(psf2)\n",
    "    psf = match_histograms(psf, psf2)\n",
    "    offset = tf_find_optimal_roll(psf, psf2)\n",
    "    return offset * z_step\n",
    "\n",
    "offsets = np.zeros((df.shape[0], df.shape[0]))\n",
    "offsets[:] = None\n",
    "\n",
    "src_node = 0\n",
    "target_node = center_point.name\n",
    "all_offsets = []\n",
    "def get_path_offset(G, src_node, target_node):\n",
    "    spath = nx.shortest_path(G, src_node, target_node)\n",
    "    if not np.isnan(offsets[src, target]):\n",
    "        cumul = offsets[src, target]\n",
    "    else:\n",
    "        cumul = 0\n",
    "        for i in range(0, len(spath)-1):\n",
    "            a, b = spath[i], spath[i+1]\n",
    "            if not np.isnan(offsets[a, b]):\n",
    "                offset = offsets[a, b]\n",
    "            else:\n",
    "                offset = align_psfs(psfs[a], psfs[b])\n",
    "                offsets[a, b] = offset\n",
    "                offsets[b, a] = -offset\n",
    "                diff = (df['roll'][a] - df['roll'][b])\n",
    "                print(f'{a} -> {b}: {offsets[a, b]}, {diff}')\n",
    "            \n",
    "            cumul += offset\n",
    "        offsets[src_node, target_node] = cumul\n",
    "        offsets[target_node, src_node] = -cumul\n",
    "    all_offsets.append(cumul)\n",
    "    \n",
    "for i in trange(0, df.shape[0]):\n",
    "    if i == target_node:\n",
    "        all_offsets.append(0)\n",
    "        continue\n",
    "    get_path_offset(G, i, target_node)\n",
    "all_offsets = np.array(all_offsets)\n",
    "\n",
    "# def roll_psf(psf, roll):\n",
    "#     rolled_psf = np.roll(psf, shift=roll, axis=0)\n",
    "# #     if roll < 0:\n",
    "# #         rolled_psf[roll:] = 0\n",
    "# #     else:\n",
    "# #         rolled_psf[:roll] = 0\n",
    "#     return rolled_psf\n",
    "# rolls = df['roll']\n",
    "# print(rolls[5], rolls[6])\n",
    "# psf = psfs[5]\n",
    "# rolled_psf = roll_psf(psf, 10)\n",
    "# print(align_psfs(psf, rolled_psf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aaad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classic_offsets.shape)\n",
    "print(all_offsets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1f8763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "plt.scatter(df['roll'], -classic_offsets-150, label='classic')\n",
    "plt.scatter(df['roll'], all_offsets-150, label='new')\n",
    "plt.plot(df['roll'], df['roll'], c='red', label='1')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(round(mean_absolute_error(df['roll'], -classic_offsets-150), 5))\n",
    "print(round(mean_absolute_error(df['roll'], all_offsets-150), 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f7245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "co = -classic_offsets*50\n",
    "sns.scatterplot(data=df, x='x', y='y', hue=co-co.min())\n",
    "plt.title('Offsets [nm]')\n",
    "plt.xlabel('x [nm]')\n",
    "plt.ylabel('y [nm]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dc6593",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = df[['x', 'y']].to_numpy()\n",
    "target_node_coords = coords[69]\n",
    "dists = euclidean_distances(coords, [[0, 0]])\n",
    "plt.scatter(df['x'], co-co.min())\n",
    "plt.ylabel('offset [nm]')\n",
    "plt.xlabel('x [nm]')\n",
    "plt.show()\n",
    "plt.scatter(df['y'], co-co.min())\n",
    "plt.ylabel('offset [nm]')\n",
    "plt.xlabel('y [nm]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09bc225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "for path in tri.simplices:\n",
    "    nx.add_path(G, path)\n",
    "\n",
    "from itertools import combinations\n",
    "for src, target in G.edges:\n",
    "    G[src][target]['weight'] = dists[src, target]\n",
    "\n",
    "edge_weights = []\n",
    "for edge in G.edges():\n",
    "    src, target = edge\n",
    "    edge_weights.append(offsets[src-1, target-1])\n",
    "    \n",
    "nx.draw(G, pos=df[['x', 'y']].values, edge_color=edge_weights, width=edge_weights, with_labels=True, node_size=100, node_color='lightgreen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7199eb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_step = 50\n",
    "psf_z = np.arange(0, psfs.shape[1]) * z_step\n",
    "\n",
    "psf2_z = (np.arange(0, psfs.shape[1]) * z_step) + overall_align\n",
    "\n",
    "all_z = np.concatenate((psf_z, psf2_z))\n",
    "all_psfs = np.concatenate((psfs[src_node], psfs[target_node]))\n",
    "idx = np.argsort(all_z)\n",
    "all_psfs = all_psfs[idx]\n",
    "\n",
    "def norm_zero_one(psf):\n",
    "    return (psf - psf.min()) / (psf.max() - psf.min())\n",
    "\n",
    "all_psfs = np.stack([norm_zero_one(p) for p in all_psfs])\n",
    "\n",
    "from data.visualise import grid_psfs\n",
    "\n",
    "plt.imshow(grid_psfs(norm_zero_one(psfs[src_node])))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.imshow(grid_psfs(norm_zero_one(psfs[target_node])))\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(grid_psfs(all_psfs))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8034e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyotf\n",
    "from tifffile import imread, imwrite\n",
    "\n",
    "psf = imread('/home/miguel/Projects/uni/phd/smlm_z/test/psfs/20220506_Miguel_beads_zeiss_training_3_beads.tif')[0]\n",
    "\n",
    "from pyotf.otf import HanserPSF\n",
    "from pyotf.phaseretrieval import retrieve_phase\n",
    "from pyotf.zernike import zernike\n",
    "\n",
    "model_kwargs = dict(\n",
    "    wl=0.647,\n",
    "    na=1.3,\n",
    "    ni=1.33,\n",
    "    res=0.106,\n",
    "    size=psf.shape[1],\n",
    "    zsize=psf.shape[0],\n",
    "    zres=0.01,\n",
    "    vec_corr=\"none\",\n",
    "    condition=\"none\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f64dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(psf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130c5fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PR_result = retrieve_phase(\n",
    "    psf, model_kwargs, max_iters=200, pupil_tol=0, mse_tol=0, phase_only=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb87c264",
   "metadata": {},
   "outputs": [],
   "source": [
    "PR_result.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32c964a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PR_result.plot()\n",
    "PR_result.plot_convergence()\n",
    "PR_result.fit_to_zernikes(64)\n",
    "PR_result.zd_result.plot()\n",
    "PR_result.zd_result.plot_named_coefs()\n",
    "PR_result.zd_result.plot_coefs()\n",
    "\n",
    "from pyotf.otf import apply_aberration\n",
    "result_psf = HanserPSF(**model_kwargs)\n",
    "result_psf = apply_aberration(result_psf, PR_result.zd_result.mcoefs, PR_result.zd_result.pcoefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8bb569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data.visualise import show_psf_axial\n",
    "psfs = np.concatenate((psf, result_psf.PSFi), axis=2)\n",
    "show_psf_axial(psfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513fa384",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from data.datasets import TrainingPicassoDataset\n",
    "from config.datasets import dataset_configs\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "\n",
    "cfg = dataset_configs['paired_bead_stacks']['training_1']\n",
    "\n",
    "locs = cfg['bpath'] / cfg['locs']\n",
    "df = pd.read_hdf(locs, 'locs')\n",
    "center = df[['x', 'y']].mean(axis=0).to_numpy()\n",
    "coords = df[['x', 'y']].to_numpy()\n",
    "dists = euclidean_distances([center], coords).squeeze()\n",
    "df['dist_to_center'] = dists\n",
    "\n",
    "first_point = np.argmin(dists)\n",
    "df['source'] = False\n",
    "df.loc[first_point, 'source'] = True\n",
    "\n",
    "dists = euclidean_distances(coords, coords)\n",
    "m = csr_matrix(dists)\n",
    "tree = minimum_spanning_tree(m).toarray()\n",
    "tree[tree>0] = 1\n",
    "edges = np.where(tree>0)\n",
    "\n",
    "abs_offsets = np.zeros((df.shape[0]))\n",
    "abs_offsets[first_point] = 10\n",
    "\n",
    "tree += tree.T\n",
    "print(tree)\n",
    "print(first_point)\n",
    "alignable_psfs = set(np.argwhere(tree[first_point, :] > 0).squeeze())\n",
    "while len(alignable_psfs):\n",
    "    unaligned_psf = alignable_psfs.pop()\n",
    "    known_offsets = set(np.argwhere(abs_offsets>0).flatten())\n",
    "    connected_points = set(np.argwhere(tree[unaligned_psf, :] > 0).flatten())\n",
    "    \n",
    "    target_psf = known_offsets.intersection(connected_points).pop()\n",
    "    abs_offsets[unaligned_psf] += abs_offsets[target_psf] + 1\n",
    "    alignable_psfs = alignable_psfs.union(set(np.argwhere(tree[:, unaligned_psf]).flatten()))\n",
    "    alignable_psfs = alignable_psfs.difference(np.argwhere(abs_offsets>0).flatten())\n",
    "\n",
    "print(abs_offsets)\n",
    "\n",
    "edges = np.where(tree>0)\n",
    "for i in range(edges[0].shape[0]):\n",
    "    src, target = edges[0][i], edges[1][i]\n",
    "    x = [coords[src][0], coords[target][0]]\n",
    "    y = [coords[src][1], coords[target][1]]\n",
    "    plt.plot(x, y, color='0')\n",
    "sns.scatterplot(data=df, x='x', y='y', hue=abs_offsets.astype(int))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437d4cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pyotf.otf import HanserPSF, apply_aberration, apply_named_aberration\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "\n",
    "from data.visualise import show_psf_axial, grid_psfs\n",
    "from data.align_psfs import align_psfs, tf_find_optimal_roll, mask_img_stack, norm_zero_one\n",
    "\n",
    "align_psfs.debug = False\n",
    "\n",
    "kwargs = dict(\n",
    "    wl=647,\n",
    "    na=1.3,\n",
    "    ni=1.51,\n",
    "    res=106,\n",
    "    zres=50,\n",
    "    size=32,\n",
    "    zsize=200,\n",
    "    vec_corr=\"none\",\n",
    "    condition=\"none\",\n",
    ")\n",
    "psf = HanserPSF(**kwargs)\n",
    "psf = apply_aberration(psf, np.array([0, 0, 0, 0, 0]), np.array([0, 0, 0, 0, 1]))\n",
    "\n",
    "blank_psf = psf.PSFi\n",
    "\n",
    "blank_psf = norm_zero_one(blank_psf) * 255\n",
    "\n",
    "from experiments.noise.noise_psf import EMCCD\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "e = EMCCD(noise_background=125)\n",
    "e.add_noise(np.random.randint(0, 255, size=(32,32)))\n",
    "\n",
    "psf = blank_psf[blank_psf.shape[0]//2]\n",
    "plt.imshow(psf)\n",
    "plt.show()\n",
    "plt.imshow(e.add_noise(psf))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57185e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from experiments.noise.noise_psf import generate_noisy_psf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "\n",
    "offset = 5\n",
    "rolled_psf = np.roll(blank_psf, offset, axis=0)\n",
    "\n",
    "rolled_psf = generate_noisy_psf(rolled_psf)\n",
    "from skimage.exposure import match_histograms\n",
    "\n",
    "psfs = np.stack((blank_psf, rolled_psf))\n",
    "plt.plot(psfs[0].max(axis=(1,2)), label='target')\n",
    "plt.plot(psfs[1].max(axis=(1,2)), label='psf')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "res = tf_find_optimal_roll(rolled_psf, blank_psf, 1)\n",
    "\n",
    "offsets = [200, 200-res]\n",
    "\n",
    "print('offsets', offsets)\n",
    "assert res == -offset\n",
    "\n",
    "imgs = np.concatenate(psfs)\n",
    "zs = []\n",
    "labels = ['target', 'psf']\n",
    "\n",
    "for i, o in enumerate(offsets):\n",
    "    vals = (np.arange(0, psfs[0].shape[0])*10) - (o*10)\n",
    "    print(vals[0:5])\n",
    "    plt.plot(vals, np.max(psfs[i], axis=(1,2)), label=labels[i])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a7951b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.estimate_offset import get_peak_sharpness\n",
    "\n",
    "\n",
    "fake_stacks = []\n",
    "offsets = []\n",
    "for offset in [0, 5, 10]:\n",
    "    offsets.append(offset)\n",
    "    rolled_psf = np.roll(blank_psf, offset, axis=0)\n",
    "#     rolled_psf = generate_noisy_psf(rolled_psf)\n",
    "    fake_stacks.append(rolled_psf)\n",
    "\n",
    "fake_stacks = np.array(fake_stacks)\n",
    "fake_stacks[fake_stacks<0] = 0\n",
    "\n",
    "pos0 = (get_peak_sharpness(fake_stacks[0]) * 50) * 2\n",
    "print(pos0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d0919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_offsets = align_psfs(fake_stacks)\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "\n",
    "print(corr_offsets)\n",
    "voxel_size = 50\n",
    "zs = []\n",
    "for offset, psf in zip(corr_offsets, fake_stacks):\n",
    "    z = ((np.arange(0, psf.shape[0]) - offset)  * voxel_size) -  pos0\n",
    "    print(z.min(), z.max())\n",
    "    zs.append(z)\n",
    "    plt.plot(z, psf.max(axis=(1,2)))\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be016c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.datasets import stack_offset_to_z\n",
    "zs = []\n",
    "for psf, offset in zip(fake_stacks, corr_offsets):\n",
    "    z = stack_offset_to_z(offset, psf, 10)\n",
    "    print(z.min(), z.max())\n",
    "    zs.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cc76a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect training dataset\n",
    "from data.visualise import grid_psfs\n",
    "plt.rcParams['figure.figsize'] = [50, 50]\n",
    "\n",
    "zs = np.concatenate(zs)\n",
    "imgs = np.concatenate(fake_stacks)\n",
    "idx = np.argsort(zs)\n",
    "imgs = imgs[idx]\n",
    "\n",
    "plt.imshow(grid_psfs(imgs.squeeze()))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383557ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def read_spots(dirpath):\n",
    "    f = h5py.File(dirpath, 'r')\n",
    "    spots = np.array(f['spots'])[:, :, :, np.newaxis]\n",
    "    f.close()\n",
    "    return spots\n",
    "\n",
    "dirpath = '/home/miguel/Projects/uni/data/smlm_3d/picasso_sim/grid_pairs/grid_pairs_spots.hdf5'\n",
    "test_spots = read_spots(dirpath)\n",
    "\n",
    "dirpath = '/home/miguel/Projects/uni/data/smlm_3d/picasso_test/training/NPC-A647-3D-BEADS/0021_spots.hdf5'\n",
    "\n",
    "train_spots = read_spots(dirpath)\n",
    "\n",
    "print(train_spots.shape)\n",
    "print(test_spots.shape)\n",
    "for spots in [train_spots, test_spots]:\n",
    "    print(spots.min(), spots.mean(), spots.std(), spots.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da715eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skimage.exposure import match_histograms\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [3, 3]\n",
    "train_spot = train_spots[0]\n",
    "\n",
    "mean_img = np.mean(train_spots, axis=0)\n",
    "\n",
    "plt.imshow(train_spot)\n",
    "plt.show()\n",
    "plt.imshow(mean_img)\n",
    "plt.show()\n",
    "plt.imshow(match_histograms(train_spot, mean_img))\n",
    "plt.show()\n",
    "\n",
    "print(train_spot.min(), train_spot.max())\n",
    "\n",
    "print(mean_img.min(), mean_img.max())\n",
    "\n",
    "print(match_histograms(train_spot, mean_img).min(), match_histograms(train_spot, mean_img).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a99aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.exposure import equalize_hist\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "\n",
    "mean_img = np.mean(train_spots, axis=0)\n",
    "\n",
    "for i in range(0, 10):\n",
    "    show_imgs(train_spots[i], match_histograms(train_spots[i], mean_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04767c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from data.datasets import TrainingPicassoDataset\n",
    "from config.datasets import dataset_configs\n",
    "\n",
    "cfg = dataset_configs['paired_bead_stacks']['training_1']\n",
    "print(cfg)\n",
    "\n",
    "ds = TrainingPicassoDataset(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867c8874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "radius = 107\n",
    "n_elements = 8\n",
    "z_depths = [0, 200]\n",
    "offset = (0, 0)\n",
    "\n",
    "angles = [i*((np.pi*2)/n_elements) for i in range(n_elements)]\n",
    "x = np.array([np.cos(a)*radius for a in angles])\n",
    "y = np.array([np.sin(a)*radius for a in angles])\n",
    "\n",
    "x += offset[0]\n",
    "y += offset[0]\n",
    "\n",
    "xs = [round(n, 3) for n in np.concatenate((x, x))]\n",
    "ys = [round(n, 3) for n in np.concatenate((y, y))]\n",
    "zs = [round(n, 3) for n in (sorted(z_depths*n_elements))]\n",
    "\n",
    "print(len(xs), len(ys), len(zs))\n",
    "\n",
    "plt.scatter(xs, ys)\n",
    "plt.show()\n",
    "\n",
    "print(xs)\n",
    "print(ys)\n",
    "print(zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3463a9e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset ='picasso_test'\n",
    "BOUND = 31\n",
    "\n",
    "from model.model import load_trained_model\n",
    "\n",
    "\n",
    "model = load_trained_model(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860c14d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fea54b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "--------\n",
    "|    x |   <- 50nm deeper than other\n",
    "|      |\n",
    "| x    |\n",
    "--------\n",
    "\n",
    "5 structures, frame 16 px,\n",
    "structureX: 1000,3000\n",
    "structureY: 1000,3000\n",
    "\n",
    "structure3D: 0,50\n",
    "ExchangeLabels:1,1\n",
    "'''\n",
    "dirpath = '/home/miguel/Projects/uni/data/smlm_3d/picasso_sim/50nm/50nm'\n",
    "\n",
    "# dirpath = '/home/miguel/Projects/uni/data/smlm_3d/picasso_sim/grid_single/grid_single'\n",
    "# dirpath = '/home/miguel/Projects/uni/data/smlm_3d/picasso_sim/grid_pairs/grid_pairs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ada6b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "yaml_file = f'{dirpath}.yaml'\n",
    "\n",
    "class SafeLoaderIgnoreUnknown(yaml.SafeLoader):\n",
    "    def ignore_unknown(self, node):\n",
    "        return None \n",
    "\n",
    "SafeLoaderIgnoreUnknown.add_constructor(None, SafeLoaderIgnoreUnknown.ignore_unknown)\n",
    "\n",
    "with open(yaml_file, \"r\") as stream:\n",
    "    root = yaml.load(stream, Loader=SafeLoaderIgnoreUnknown)\n",
    "with open(yaml_file, \"w\") as stream:\n",
    "    yaml.dump(root, stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd55e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "\n",
    "def norm_zero_one(img):\n",
    "    img_max = img.max()\n",
    "    img_min = img.min()\n",
    "    return (img - img_min) / (img_max - img_min)\n",
    "\n",
    "def norm_one_one(img):\n",
    "    return (2 * norm_zero_one(img)) - 1 \n",
    "\n",
    "locs = f'{dirpath}_locs.hdf5'\n",
    "spots = f'{dirpath}_spots.hdf5'\n",
    "print(locs)\n",
    "print(spots)\n",
    "df = pd.read_hdf(locs, 'locs')\n",
    "\n",
    "f = h5py.File(spots, 'r')\n",
    "spots = np.array(f['spots'])[:, :, :, np.newaxis]\n",
    "\n",
    "df['id'] = np.arange(0, df.shape[0])\n",
    "print(spots.shape)\n",
    "print(df.shape, spots.shape)\n",
    "f.close()\n",
    "\n",
    "from data.visualise import grid_psfs\n",
    "# plt.imshow(grid_psfs(spots.squeeze()))\n",
    "# plt.show()\n",
    "\n",
    "assert df.shape[0] == spots.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "from data.datasets import norm_dataset_from_config, standardise, load_ref_img_and_norm\n",
    "\n",
    "print(spots.shape)\n",
    "print(spots.min(), spots.max())\n",
    "# spots = standardise(spots)\n",
    "# spots = load_ref_img_and_norm(spots)\n",
    "for s in spots[0:5]:\n",
    "    print(s.min(), s.max())\n",
    "    \n",
    "spots = np.stack([norm_one_one(img) for img in spots])\n",
    "\n",
    "print(spots.min(), spots.max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377d472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 20]\n",
    "plt.imshow(grid_psfs(spots[0:50].squeeze()).squeeze())\n",
    "plt.show()\n",
    "\n",
    "mean_img = spots[1]\n",
    "\n",
    "spots_matched_hist = np.stack([match_histograms(img, mean_img) for img in spots])\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "plt.imshow(mean_img)\n",
    "plt.show()\n",
    "plt.rcParams['figure.figsize'] = [20, 20]\n",
    "plt.imshow(grid_psfs(spots_matched_hist[0:50].squeeze()).squeeze())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec02f556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rescale to [-1, 1]\n",
    "# spots = np.stack([norm_one_one(img) for img in spots])\n",
    "\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "df['index'] = np.arange(0, df.shape[0])\n",
    "print(spots.shape)\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "sns.scatterplot(data=df, x='x', y='y', marker='+')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "coords = np.zeros((spots.shape[0], 2))\n",
    "\n",
    "pred = model.predict((spots_matched_hist, coords)).squeeze()\n",
    "print(np.std(pred))\n",
    "df['pred'] = pred\n",
    "print(pred.min(), pred.max())\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "sns.histplot(pred, bins=50)\n",
    "plt.xlabel('Z position (nm)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sub_imgs = spots[0:100]\n",
    "sub_preds = pred[0:100]\n",
    "plt.rcParams['figure.figsize'] = [100, 100]\n",
    "from data.visualise import grid_psfs\n",
    "print(np.sort(sub_preds.squeeze()))\n",
    "\n",
    "plt.imshow(grid_psfs(sub_imgs[np.argsort(sub_preds.squeeze())].squeeze()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3abc15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.std(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03026e87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from config.datasets import dataset_configs\n",
    "\n",
    "from data.datasets import TrainingDataSet, ExperimentalDataSet, GenericDataSet, MultiTrainingDataset, TrainingPicassoDataset\n",
    "\n",
    "dataset = 'picasso_test'\n",
    "version = ''\n",
    "cfg = dataset_configs[dataset]['training']\n",
    "print(cfg)\n",
    "\n",
    "train_dataset = TrainingPicassoDataset(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc0966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.data['train'][0][0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98e91fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "\n",
    "test_imgs = spots.flatten()\n",
    "\n",
    "plt.hist(test_imgs, label='exp', alpha=0.5)\n",
    "\n",
    "for k in train_dataset.data.keys():\n",
    "    imgs = train_dataset.data[k][0][0].flatten()\n",
    "    print(imgs.min(), imgs.max())\n",
    "    plt.hist(imgs.flatten(), label=k, alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a281fe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pixel_vals = np.mean(spots, axis=(1,2))\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "sns.kdeplot(mean_pixel_vals, pred)\n",
    "plt.xlabel('Mean pixel value')\n",
    "plt.ylabel('Pred')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9340bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "# df['emitter'] = df['x'] < 60\n",
    "df['pred'] = pred.squeeze()\n",
    "# df['pred2'] = df['pred']>-250\n",
    "df['snr'] = [np.max(s)/np.median(s) for s in spots]\n",
    "df['error'] = df['pred']+130\n",
    "sns.scatterplot(data=df, x='snr', y='pred')\n",
    "plt.show()\n",
    "sns.scatterplot(data=df, x='snr', y='error')\n",
    "plt.show()\n",
    "# sns.histplot(daata=df, x='pred', hue='emitter')\n",
    "# plt.show()\n",
    "\n",
    "# sns.scatterplot(data=df, x='x', y='y', hue='pred2')\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a59394",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Checks influence of polar coords on Z localisation\n",
    "\"\"\"\n",
    "\n",
    "# from itertools import product\n",
    "\n",
    "# plt.rcParams['figure.figsize'] = [5, 5]\n",
    "\n",
    "# spot = spots[0:1][:, :, :, np.newaxis]\n",
    "# thetas = np.linspace(0, 1, 20)\n",
    "# rhos = np.linspace(0, 1, 20)\n",
    "\n",
    "# coords = np.array(list(product(thetas, rhos))).squeeze()\n",
    "# spot = np.repeat(spot, coords.shape[0], axis=0)\n",
    "\n",
    "# preds = model.predict((spot, coords))\n",
    "\n",
    "# tmp_df = pd.DataFrame.from_dict({'theta': coords[:, 0], 'rho': coords[:, 1], 'z': preds.squeeze()})\n",
    "# sns.scatterplot(data=tmp_df, x='theta', y='z')\n",
    "# plt.show()\n",
    "# sns.scatterplot(data=tmp_df, x='rho', y='z')\n",
    "# plt.show()\n",
    "# sns.scatterplot(data=tmp_df, x='theta', y='rho', hue='z')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedead60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# from config.datasets import dataset_configs\n",
    "# from data.datasets import StormDataset, ExperimentalDataSet\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# dataset ='picasso_test'\n",
    "\n",
    "# cfg = dataset_configs[dataset]['nucleopore']\n",
    "# ds = StormDataset(cfg, normalize_psf=True, lazy=True, apply_clustering=False)\n",
    "# # ds.neighbour_radius = 15\n",
    "# # ds.max_off_frames = 1000\n",
    "# ds.csv_data = ds.csv_data[(ds.csv_data['x [nm]'] > 8250) \n",
    "#                           & (ds.csv_data['x [nm]'] < 8425) \n",
    "#                           & (ds.csv_data['y [nm]'] > 9425) \n",
    "#                           & (ds.csv_data['y [nm]'] < 9575) \n",
    "#                          ]\n",
    "\n",
    "# plt.rcParams['figure.figsize'] = [5, 5]\n",
    "# sns.scatterplot(data=ds.csv_data, x='x [nm]', y='y [nm]', marker='.')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# ds.prepare_data()\n",
    "\n",
    "# from scipy.ndimage import median_filter\n",
    "# ds.data[0] = np.stack([median_filter(d, size=2) for d in ds.data[0]])\n",
    "\n",
    "# df = ds.csv_data\n",
    "# print(df.shape)\n",
    "# print(ds.data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8a65a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5,5]\n",
    "\n",
    "coords = df[['x', 'y']].to_numpy() * 100\n",
    "distance_matrix = euclidean_distances(coords, coords)\n",
    "eps = 15\n",
    "min_count = 50\n",
    "\n",
    "distance_matrix = (distance_matrix < eps).astype(int).sum(axis=0)\n",
    "idx = np.argwhere(distance_matrix > min_count).squeeze()\n",
    "sub_coords = coords[idx]\n",
    "print(coords.shape)\n",
    "print(sub_coords.shape)\n",
    "\n",
    "cluster_ids = DBSCAN(eps=eps, min_samples=min_count).fit_predict(sub_coords)\n",
    "sns.scatterplot(x=sub_coords[:, 0], y=sub_coords[:, 1], hue=cluster_ids.astype(str))\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "sub_df = df.iloc[idx]\n",
    "sub_df['cluster_id'] = cluster_ids.squeeze().astype(str)\n",
    "sub_spots = spots[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c989dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(model.predict((spots, np.zeros((spots.shape[0], 2)))).squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297b85b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN, OPTICS, KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import norm\n",
    "from data.visualise import grid_psfs\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20, 5]\n",
    "# df['cluster_id'] = OPTICS().fit_predict(df[['x', 'y']].to_numpy()).squeeze().astype(str)\n",
    "plt.axis('equal')\n",
    "plt.title('Units: pixels (90nm each)')\n",
    "sns.scatterplot(data=sub_df, x='x', y='y', marker='.', hue='cluster_id', legend=False)\n",
    "plt.show()\n",
    "\n",
    "n_components = 2\n",
    "\n",
    "gm_mean_diffs = {}\n",
    "mean_img_diffs = {}\n",
    "\n",
    "\n",
    "def gm_min_bic(data, imgs):\n",
    "    gm_df = pd.DataFrame({'pred': data.squeeze()}, index=np.arange(0, data.squeeze().shape[0]))\n",
    "\n",
    "    best_gm = None\n",
    "    min_bic = np.inf\n",
    "    bics = []\n",
    "    cov_type = 'tied'\n",
    "    stdevs = []\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 6)\n",
    "    for n in range(1, 7):\n",
    "        gm = GaussianMixture(n_components=n, n_init=20, covariance_type=cov_type).fit(data)\n",
    "        bic = gm.bic(data)\n",
    "        \n",
    "        bics.append(round(bic, 3))\n",
    "        stdevs.append(round(np.std(gm.weights_), 3))\n",
    "        if bic < min_bic:\n",
    "            min_bic = bic\n",
    "            best_gm = gm\n",
    "        \n",
    "        ax = axes[n-1]\n",
    "        labels = gm.predict(data).squeeze()\n",
    "\n",
    "        gm_df['cluster_id'] = labels.astype(str)\n",
    "\n",
    "        weights = gm.weights_\n",
    "\n",
    "        sns.histplot(data=gm_df, x='pred', hue='cluster_id', stat='density', alpha=0.2, bins=20, ax=ax)\n",
    "\n",
    "        # create necessary things to plot\n",
    "        x_axis = np.linspace(data.min(), data.max(), 50)\n",
    "        ys = []\n",
    "        sub_df2 = pd.DataFrame.from_dict({'x': x_axis})\n",
    "        for i in range(0, best_gm.n_components):\n",
    "            if cov_type == 'tied':\n",
    "                cov = gm.covariances_.squeeze()\n",
    "            elif cov_type == 'full' or cov_type == None:\n",
    "                cov = gm.covariances_[i][0][0]\n",
    "            elif cov_type == 'spherical':\n",
    "                cov = gm.covariances_[i]\n",
    "            elif cov_type == 'diag':\n",
    "                cov_type = gm.covariances_[i]\n",
    "\n",
    "            sub_df2[f'y_{i}'] = norm.pdf(x_axis, float(gm.means_[i][0]), np.sqrt(cov))*gm.weights_[i]\n",
    "            sns.lineplot(data=sub_df2, x='x', y=f'y_{i}', ax=ax)\n",
    "    plt.show()        \n",
    "        \n",
    "    print(bics)\n",
    "    print(stdevs)\n",
    "\n",
    "    return best_gm.means_[:, 0]\n",
    "\n",
    "def apply_gm(data, imgs, cid):\n",
    "    data = data.reshape(-1, 1)\n",
    "    gm_df = pd.DataFrame({'pred': list(data)})\n",
    "    \n",
    "    return gm_min_bic(data, imgs)\n",
    "    \n",
    "\n",
    "all_coords = []\n",
    "for cid in set(sub_df['cluster_id']):\n",
    "    if cid == '-1':\n",
    "        continue\n",
    "    idx = np.argwhere(sub_df['cluster_id'].to_numpy()==cid).squeeze()\n",
    "    imgs = sub_spots[idx][:, :, :, np.newaxis]\n",
    "    coords = np.zeros((imgs.shape[0], 2))\n",
    "    preds = model.predict((imgs, coords)).squeeze()\n",
    "    preds -= preds.min()\n",
    "    preds += 0.00000001\n",
    "    preds = np.sqrt(preds)\n",
    "    z_pos = apply_gm(preds, imgs, cid)\n",
    "    x, y = sub_df.iloc[idx][['x', 'y']].mean(axis=0).to_numpy() * 106\n",
    "    coords = [[x, y, z, int(cid)] for z in z_pos]\n",
    "    all_coords.extend(coords)\n",
    "\n",
    "all_coords = np.array(all_coords)\n",
    "res = pd.DataFrame.from_dict({\n",
    "    k: all_coords[:, i] for k, i in zip(['x', 'y', 'z', 'cluster_id'], [0, 1, 2, 3])\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b656523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = gm_mean_diffs\n",
    "tops = [np.max(v) for k, v in d.items()]\n",
    "bottoms = [np.min(v) for k, v in d.items()]\n",
    "print(np.mean(tops))\n",
    "print(np.mean(bottoms))\n",
    "print(np.mean(tops) - np.mean(bottoms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713b6dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "res['cluster_id'] = res['cluster_id'].astype(int)\n",
    "fig = plt.figure()\n",
    "fig.tight_layout() \n",
    "\n",
    "plt.subplots_adjust(wspace = 0.4) \n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "\n",
    "ax.scatter(res['x'], res['y'], res['z'], marker='o', c=res['cluster_id'])\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "ax2.scatter(res['cluster_id'], res['z'], c=res['cluster_id'])\n",
    "plt.xlabel('Cluster ID')\n",
    "plt.title('All units in nm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78083955",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spots.shape)\n",
    "print(coords.shape)\n",
    "all_coords = np.zeros((spots.shape[0], 2))\n",
    "all_spots = spots[:, :, :, np.newaxis]\n",
    "\n",
    "pred = model.predict((all_spots, all_coords))\n",
    "df['z'] = pred.squeeze()\n",
    "# for c in list(df):\n",
    "#     sns.scatterplot(data=df, x=c, y='z')\n",
    "#     plt.show()\n",
    "sns.scatterplot(data=df.iloc[0:1000], x='frame', y='z')\n",
    "plt.show()\n",
    "sns.histplot(data=df, x='z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1bec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_img = spots[0]\n",
    "\n",
    "recs = []\n",
    "\n",
    "imgs = []\n",
    "coords = []\n",
    "\n",
    "for _ in range(1000):\n",
    "    noise_loc = np.random.uniform(0, 0.8)\n",
    "    noise_scale = np.random.uniform(0, 0.7)\n",
    "    new_img = np.random.normal(loc=noise_loc, scale=noise_scale, size=best_img.shape)\n",
    "    new_img += best_img\n",
    "    new_img = norm_zero_one(new_img)\n",
    "    \n",
    "    new_img = norm_zero_one(new_img)\n",
    "    \n",
    "    new_img = np.array([new_img])\n",
    "    imgs.append(new_img)\n",
    "    coords.append(np.zeros((1, 2)))\n",
    "    \n",
    "    recs.append({\n",
    "        'img_mean': np.mean(new_img),\n",
    "        'img_median': np.median(new_img),\n",
    "        'noise_loc': noise_loc,\n",
    "        'noise_scale': noise_scale,\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame.from_dict(recs)\n",
    "\n",
    "imgs = np.concatenate(imgs)\n",
    "print(imgs.shape)\n",
    "coords = np.concatenate(coords)\n",
    "print(coords.shape)\n",
    "pred = model.predict((imgs, coords)).squeeze()\n",
    "err = pred\n",
    "df['err'] = err\n",
    "df['pred'] = pred\n",
    "for col in list(df):\n",
    "    sns.scatterplot(data=df, x=col, y='err')\n",
    "    plt.show()\n",
    "sns.scatterplot(data=df, x='noise_loc', y='noise_scale', hue='err')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e093e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45bde13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352086ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
