{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed72486e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from data.visualise import grid_psfs, show_psf_axial\n",
    "from tifffile import imread\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_pickle_file(dpath):\n",
    "    with open(dpath, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "    \n",
    "psfs = imread('/home/miguel/Projects/uni/data/smlm_3d/20230601_MQ_celltype/beads_box15/combined/stacks.ome.tif')\n",
    "locs = pd.read_hdf('/home/miguel/Projects/uni/data/smlm_3d/20230601_MQ_celltype/beads_box15/combined/locs.hdf', key='locs')\n",
    "\n",
    "\n",
    "psfs = psfs[:, :, :, :, np.newaxis]\n",
    "\n",
    "# for i, psf in enumerate(psfs.sum(axis=-1)):\n",
    "#     plt.title(str(i))\n",
    "#     show_psf_axial(psf)\n",
    "\n",
    "exclude_idx = [5, 7, 11, 14, 22, 24, 26, 27, 28, 31, 32, 35, 37, 38, 40, 45, 50, 51, 54, 68, 69, 71, 72, 82, 87, 89, 91, 98, 102, 108, 109, 112, 113, 115, 116, 121, 122, 123, 127, 129, 131, 132, 133, 138, 141, 144, 150, 151, 154, 161, 167, 169, 170, 172, 178, 179, 181, 182, 184, 185, 186, 187, 190, 200, 201, 205, 206, 210, 214, 219, 221, 224, 226, 230, 233, 234, 235, 236, 237, 243]\n",
    "\n",
    "# exclude_idx = [0, 5, 7, 12, 22, 26, 32, 35, 38, 40, 45, 50, 51, 54, 68, 69, 71, 72, 82, 87, 89, 91, 98, 102, 108, 109, 112, 113, 115, 116, 121, 122, 123, 124, 127, 129, 131, 132, 133, 138, 141, 144, 150, 151, 154, 161, 167, 169, 170, 172, 178, 179, 181, 182, 184, 185, 186, 187, 190, 200, 201, 205, 206, 210, 214, 219, 221, 224, 226, 230, 233, 234, 235, 236, 237, 243]\n",
    "\n",
    "# print('Excluded PSFs \\n\\n\\n\\n\\n')\n",
    "# for i in exclude_idx:\n",
    "#     show_psf_axial(psfs[i].mean(axis=-1), str(i))\n",
    "#     plt.plot(psfs[i].max(axis=(1,2)))\n",
    "#     plt.show()\n",
    "# print('End of excluded PSFs \\n\\n\\n\\n\\n')\n",
    "\n",
    "# for i in range(psfs.shape[0]):\n",
    "#     if i in exclude_idx:\n",
    "#         continue\n",
    "#     plt.title(str(i))\n",
    "#     show_psf_axial(psfs[i].mean(axis=-1))\n",
    "#     plt.plot(psfs[i].max(axis=(1,2,3)), label='max')\n",
    "#     plt.legend()\n",
    "#     plt.title(str(i))\n",
    "#     plt.show()\n",
    "Z_STEP = 20\n",
    "idx = [i for i in range(psfs.shape[0]) if i not in exclude_idx]\n",
    "\n",
    "orig_idx = np.arange(psfs.shape[0])\n",
    "orig_idx = orig_idx[idx]\n",
    "psfs = psfs[idx]\n",
    "locs = locs.iloc[idx]\n",
    "ys = []\n",
    "for i in range(psfs.shape[0]):\n",
    "    y = np.arange(psfs.shape[1]) * Z_STEP\n",
    "    y = y - 1000\n",
    "    ys.append(y)\n",
    "ys = np.stack(ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e91bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data.estimate_offset import estimate_offset\n",
    "\n",
    "# for i, psf in enumerate(psfs):\n",
    "#     ys[i] = estimate_offset(psf.squeeze(), (Z_STEP,), disable_boundary_check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80e0106",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ROTATION MATCHING ALIGNMENT\n",
    "\n",
    "# from skimage.filters import gaussian\n",
    "# from data.align_psfs import norm_zero_one\n",
    "# from sklearn.metrics import euclidean_distances, mean_squared_error\n",
    "# import scipy.ndimage as ndi\n",
    "\n",
    "\n",
    "# Z_STEP = 20\n",
    "# UPSCALE_RATIO = 5\n",
    "        \n",
    "# def find_seed_psf(df):\n",
    "#     n_points = 5\n",
    "#     # Seed PSF - most centered PSF in FOV\n",
    "#     center = df[['x', 'y']].mean(axis=0).to_numpy()\n",
    "#     coords = df[['x', 'y']].to_numpy()\n",
    "#     dists = euclidean_distances([center], coords).squeeze()\n",
    "#     return np.argsort(dists)[:n_points]\n",
    "# #     first_point = np.argmin(dists)\n",
    "# #     return first_point\n",
    "\n",
    "\n",
    "# def norm_sum_imgs(psf):\n",
    "#     psf_sums = psf.sum(axis=(1,2, 3))\n",
    "#     psf = psf / psf_sums[:, np.newaxis, np.newaxis, np.newaxis]\n",
    "#     return psf\n",
    "\n",
    "\n",
    "# def eval_roll(fixed, moving, roll, debug=False):\n",
    "#     fixed_section = fixed\n",
    "#     moving_section = moving    \n",
    "#     if roll < 0:\n",
    "#         moving_section = moving[-roll:]\n",
    "#         fixed_section = fixed_section[:moving_section.shape[0]]\n",
    "#     elif roll > 0:\n",
    "#         fixed_section = fixed[roll:]\n",
    "#         moving_section = moving_section[:fixed_section.shape[0]]\n",
    "\n",
    "#     score = mean_squared_error(fixed_section.flatten(), moving_section.flatten())\n",
    "# #     score = ((fixed_section-moving_section)**2).sum()\n",
    "#     if debug:\n",
    "#         plt.rcParams['figure.figsize'] = [10, 3]\n",
    "#         x = np.arange(fixed.shape[0])\n",
    "#         x_moving = np.arange(moving.shape[0]) + roll\n",
    "#         plt.title(f'full roll {roll}')\n",
    "        \n",
    "#         plt.plot(x, moving.max(axis=(1,2)), label='original')\n",
    "#         plt.plot(x_moving, moving.max(axis=(1,2)), label='shifted')\n",
    "#         plt.plot(x, fixed.max(axis=(1,2)), label='seed')\n",
    "#         plt.xlim((-5, max((x.max(), x_moving.max()))))\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "        \n",
    "# #         plt.title(f'{round(score, 2)} length {moving_section.shape[0]} roll {roll}')\n",
    "# #         plt.plot(moving_section.max(axis=(1,2)), label='moving')\n",
    "# #         plt.plot(fixed_section.max(axis=(1,2)), label='fixed')\n",
    "# #         plt.legend()\n",
    "# #         plt.show()   \n",
    "#         show_psf_axial(fixed.mean(axis=-1), 'fixed', 3*UPSCALE_RATIO)\n",
    "#         show_psf_axial(moving.mean(axis=-1), 'moving', 3*UPSCALE_RATIO)\n",
    "#         show_psf_axial(np.roll(moving.mean(axis=-1), roll, 0), f'shifted {roll}', 3*UPSCALE_RATIO)\n",
    "        \n",
    "\n",
    "#     return score\n",
    "\n",
    "# def find_best_roll(fixed, moving):\n",
    "#     roll_min = -fixed.shape[0] // 2\n",
    "#     roll_max = fixed.shape[0] // 2\n",
    "#     best_roll = None\n",
    "#     min_score = np.inf\n",
    "    \n",
    "#     rolls = []\n",
    "#     scores = []\n",
    "#     for roll in range(roll_min, roll_max, 1):\n",
    "#         score = eval_roll(fixed, moving, roll, debug=False)\n",
    "#         if score < min_score:\n",
    "#             best_roll = roll\n",
    "#             min_score = score\n",
    "#         rolls.append(roll)\n",
    "#         scores.append(score)\n",
    "        \n",
    "# #     plt.plot(rolls, scores)\n",
    "# #     plt.xlabel('roll')\n",
    "# #     plt.ylabel('MSE')\n",
    "# #     plt.show()\n",
    "#     eval_roll(fixed, moving, best_roll, debug=True)\n",
    "#     return best_roll * (Z_STEP/UPSCALE_RATIO)\n",
    "\n",
    "\n",
    "# def upscale_psf(psf):\n",
    "#     out = ndi.zoom(psf, (UPSCALE_RATIO, 1, 1, 1), order=2)\n",
    "#     return out\n",
    "    \n",
    "# def register_psfs(psfs, df):\n",
    "#     GAUSS_SIGMA = 1\n",
    "#     offsets = []\n",
    "#     seed_idx = find_seed_psf(df)\n",
    "# #     seed_idx = -1\n",
    "# #     seed_psf = norm_zero_one(psfs.mean(axis=0))\n",
    "#     seed_psf = norm_zero_one(gaussian(psfs[seed_idx].mean(axis=0), sigma=GAUSS_SIGMA))\n",
    "#     seed_psf = upscale_psf(seed_psf)\n",
    "#     seed_idx = -1\n",
    "    \n",
    "#     for i in range(psfs.shape[0]):\n",
    "#         if i == seed_idx:\n",
    "#             offset = 0\n",
    "#         else:\n",
    "#             psf = norm_zero_one(gaussian(psfs[i], sigma=GAUSS_SIGMA))\n",
    "#             psf = upscale_psf(psf)\n",
    "# #             plt.plot(psf.max(axis=(1,2,3)))\n",
    "# #             plt.plot(seed_psf.max(axis=(1,2,3)))\n",
    "# #             plt.show()\n",
    "#             offset = find_best_roll(seed_psf, psf)\n",
    "#         offsets.append(offset)\n",
    "#     return np.array(offsets)\n",
    "\n",
    "# offsets = register_psfs(psfs, locs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a36667",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SIMPLE ITK alignment\n",
    "\n",
    "# import SimpleITK as sitk\n",
    "# import numpy as np\n",
    "# from data.align_psfs import norm_zero_one\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics import euclidean_distances\n",
    "\n",
    "# Z_STEP = 20 \n",
    "# def command_iteration(method):\n",
    "#     print(\n",
    "#         f\"{method.GetOptimizerIteration():3} \"\n",
    "#         + f\" = {method.GetMetricValue():7.5f} \"\n",
    "#         + f\" : {method.GetOptimizerPosition()}\"\n",
    "#     )        \n",
    "\n",
    "# def arr_to_img(arr):\n",
    "#     arr = norm_zero_one(arr.squeeze()).astype(np.float32)\n",
    "#     img = sitk.GetImageFromArray(arr)\n",
    "#     img = sitk.DiscreteGaussian(img)\n",
    "#     return img\n",
    "\n",
    "\n",
    "# def register_psf(psf, fixed):\n",
    "#     moving = arr_to_img(psf)\n",
    "\n",
    "#     R = sitk.ImageRegistrationMethod()\n",
    "\n",
    "#     R.SetMetricAsMeanSquares()\n",
    "\n",
    "#     R.SetOptimizerAsRegularStepGradientDescent(\n",
    "#         learningRate=0.05,\n",
    "#         minStep=1e-12,\n",
    "#         numberOfIterations=10000,\n",
    "#         gradientMagnitudeTolerance=1e-12,\n",
    "#     )\n",
    "#     R.SetOptimizerScalesFromIndexShift()\n",
    "\n",
    "#     R.SetInitialTransform(sitk.TranslationTransform(fixed.GetDimension()))\n",
    "\n",
    "#     R.SetInterpolator(sitk.sitkLinear)\n",
    "\n",
    "# #     R.AddCommand(sitk.sitkIterationEvent, lambda: command_iteration(R))\n",
    "\n",
    "#     outTx = R.Execute(fixed, moving)\n",
    "#     print(\"-------\")\n",
    "#     print(outTx)\n",
    "#     print(f\"Optimizer stop condition: {R.GetOptimizerStopConditionDescription()}\")\n",
    "#     print(f\" Iteration: {R.GetOptimizerIteration()}\")\n",
    "#     print(f\" Metric value: {R.GetMetricValue()}\")\n",
    "\n",
    "#     return outTx.GetOffset()[2] * Z_STEP\n",
    "\n",
    "\n",
    "# def find_seed_psf(df):\n",
    "#     # Seed PSF - most centered PSF in FOV\n",
    "#     center = df[['x', 'y']].mean(axis=0).to_numpy()\n",
    "#     coords = df[['x', 'y']].to_numpy()\n",
    "#     dists = euclidean_distances([center], coords).squeeze()\n",
    "#     first_point = np.argmin(dists)\n",
    "#     return first_point\n",
    "\n",
    "# def register_psfs(psfs, df):\n",
    "#     offsets = []\n",
    "#     seed_idx = find_seed_psf(df)\n",
    "    \n",
    "#     ref = arr_to_img(psfs[seed_idx])\n",
    "\n",
    "#     for i in range(psfs.shape[0]):\n",
    "#         if i == seed_idx:\n",
    "#             offset = 0\n",
    "#         else:\n",
    "#             offset = register_psf(psfs[i], ref)\n",
    "#         offsets.append(offset)\n",
    "#     return offsets\n",
    "\n",
    "\n",
    "# # offsets = register_psfs(psfs, locs)\n",
    "# # offsets = np.array(offsets)\n",
    "# # offset_backup = offsets.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93f3deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import UnivariateSpline\n",
    "from data.align_psfs import norm_zero_one\n",
    "\n",
    "UPSCALE_RATIO = 10\n",
    "Z_STEP = 20\n",
    "\n",
    "def find_peak(psf):\n",
    "    x = np.arange(psf.shape[0]) * Z_STEP\n",
    "    inten = norm_zero_one(psf.max(axis=(1,2)))\n",
    "#   prev 0.8\n",
    "    cs = UnivariateSpline(x, inten, k=3, s=1.25)\n",
    "\n",
    "    x_ups = np.linspace(0, psf.shape[0], len(x) * UPSCALE_RATIO) * Z_STEP\n",
    "    \n",
    "\n",
    "\n",
    "    peak = x_ups[np.argmax(cs(x_ups))] \n",
    "    \n",
    "    plt.plot(x-peak, inten, label='raw')\n",
    "    plt.plot(x_ups-peak, cs(x_ups), label='fit')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return peak\n",
    "\n",
    "offsets = np.array([find_peak(psf) for psf in psfs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a397af57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# offsets = [0] * psfs.shape[0]\n",
    "for i in range(len(offsets)):\n",
    "    ys[i] = ys[i] - offsets[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616ddac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ys = []\n",
    "# for i, offset in zip(list(range(psfs.shape[0])), offsets):\n",
    "#     y = np.arange(psfs.shape[1]) * Z_STEP\n",
    "#     ys.append(y-offset)\n",
    "# ys = np.stack(ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cb0c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_idx = np.argwhere(abs(offsets)<750).squeeze()\n",
    "# psfs = psfs[valid_idx]\n",
    "# coords = coords[valid_idx]\n",
    "# ys = ys[valid_idx]\n",
    "\n",
    "# offsets = offsets[valid_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5a2656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-center z positions using average peak\n",
    "\n",
    "peak_zs = []\n",
    "for psf, z in zip(psfs, ys):\n",
    "    peak = np.argmax(psf.max(axis=(1,2,3)))\n",
    "    peak_zs.append(z[peak])\n",
    "    \n",
    "mean_peak = np.mean(peak_zs)\n",
    "print(mean_peak)\n",
    "ys = ys - mean_peak\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede8422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.align_psfs import norm_zero_one\n",
    "from skimage.filters import gaussian\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 3]\n",
    "for psf, z in list(zip(psfs, ys))[0:10]:\n",
    "    y = norm_zero_one(gaussian(psf).max(axis=(1,2,3))).squeeze()\n",
    "#     show_psf_axial(psf.squeeze(), '', 1)\n",
    "    plt.plot(z, y)\n",
    "plt.xlim((-1000, 1000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ef6eac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def concat_training_datasets(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    X_train = list(X_train)\n",
    "    X_val = list(X_val)\n",
    "    X_test = list(X_test)\n",
    "    for i in range(2):\n",
    "        X_train[i] = np.concatenate(X_train[i]).astype(float)\n",
    "        X_val[i] = np.concatenate(X_val[i]).astype(float)\n",
    "        X_test[i] = np.concatenate(X_test[i]).astype(float)\n",
    "\n",
    "    y_train = np.concatenate(y_train).astype(float)\n",
    "    y_val = np.concatenate(y_val).astype(float)\n",
    "    y_test = np.concatenate(y_test).astype(float)\n",
    "\n",
    "#     print(X_train[0].shape, X_train[1].shape)\n",
    "#     print(X_val[0].shape, X_val[1].shape)\n",
    "#     print(X_test[0].shape, X_test[1].shape)\n",
    "\n",
    "\n",
    "#     print(y_train.shape)\n",
    "#     print(y_val.shape)\n",
    "#     print(y_test.shape)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4166c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim stacks\n",
    "\n",
    "Z_RANGE = 1000\n",
    "def filter_z_datasets(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    def filter_z_range(X, zs):\n",
    "        psfs, coords = X\n",
    "        print(psfs.shape, coords.shape, zs.shape)\n",
    "        valid_ids = np.argwhere(abs(zs.squeeze()) < Z_RANGE).squeeze()\n",
    "        return [psfs[valid_ids], coords[valid_ids]], zs[valid_ids]\n",
    "\n",
    "    X_train, y_train = filter_z_range(X_train, y_train)\n",
    "    X_val, y_val = filter_z_range(X_val, y_val)\n",
    "    X_test, y_test = filter_z_range(X_test, y_test)\n",
    "\n",
    "#     print(X_train[0].min(), X_train[0].mean(), X_train[0].max())\n",
    "#     print(X_val[0].min(), X_val[0].mean(), X_val[0].max())\n",
    "#     print(X_test[0].min(), X_test[0].mean(), X_test[0].max())\n",
    "\n",
    "#     print(y_train.min(), y_train.max())\n",
    "#     print(y_val.min(), y_val.max())\n",
    "#     print(y_test.min(), y_test.max())\n",
    "\n",
    "#     print(X_train[0].shape, X_train[1].shape)\n",
    "#     print(X_val[0].shape, X_val[1].shape)\n",
    "#     print(X_test[0].shape, X_test[1].shape)\n",
    "\n",
    "\n",
    "#     print(y_train.shape)\n",
    "#     print(y_val.shape)\n",
    "#     print(y_test.shape)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa21febb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from data.visualise import grid_psfs\n",
    "\n",
    "def aug_dataset(X_train, y_train):\n",
    "    AUG_RATIO = 0.5\n",
    "    MAX_TRANSLATION_PX = 2\n",
    "    MAX_GAUSS_NOISE = 0.001\n",
    "    img_size = X_train[0].shape[1]\n",
    "\n",
    "    aug_pipeline = Sequential([\n",
    "        layers.GaussianNoise(stddev=MAX_GAUSS_NOISE*X_train[0].max(), seed=42),\n",
    "        layers.RandomTranslation(MAX_TRANSLATION_PX/img_size, MAX_TRANSLATION_PX/img_size, seed=42),\n",
    "        layers.RandomBrightness(0.2, [X_train[0].min(), X_train[0].max()], seed=42)\n",
    "    ])\n",
    "\n",
    "    idx = np.random.randint(0, X_train[0].shape[0], size=int(AUG_RATIO*X_train[0].shape[0]))\n",
    "\n",
    "    aug_psfs = aug_pipeline(X_train[0][idx].copy(), training=True).numpy()\n",
    "    aug_coords = X_train[1][idx]\n",
    "\n",
    "    aug_z = y_train[idx]\n",
    "\n",
    "    subset_psfs = np.concatenate((aug_psfs[0:100], X_train[0][idx][0:100]))\n",
    "    plt.imshow(grid_psfs(subset_psfs.mean(axis=-1)))\n",
    "    plt.show()\n",
    "\n",
    "    train_psfs = np.concatenate([aug_psfs, X_train[0]])\n",
    "    train_coords = np.concatenate([aug_coords, X_train[1]])\n",
    "    train_zs = np.concatenate([aug_z, y_train])\n",
    "\n",
    "    X_train = [train_psfs, train_coords]\n",
    "    y_train = train_zs\n",
    "    return X_train, y_train\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c11638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skimage.transform import rescale\n",
    "# plt.rcParams['figure.figsize'] = [3, 3]\n",
    "# def bin_pixels(img):\n",
    "#     subsample_ratio = 2\n",
    "#     new_shape = [s//subsample_ratio for s in img.shape[0:2]] + [1]\n",
    "#     new_img = rescale(img, 1/subsample_ratio, anti_aliasing=False)\n",
    "    \n",
    "#     upscaled_img = rescale(new_img, (subsample_ratio, subsample_ratio, 1), anti_aliasing=False, order=0)\n",
    "# #     plt.imshow(img)\n",
    "# #     plt.show()\n",
    "# #     plt.imshow(new_img)\n",
    "# #     plt.title(new_img.shape)\n",
    "# #     plt.show()\n",
    "    \n",
    "# #     plt.imshow(upscaled_img)\n",
    "# #     plt.title(upscaled_img.shape)\n",
    "# #     plt.show()\n",
    "#     return upscaled_img\n",
    "\n",
    "# plt.imshow(X_train[0])\n",
    "# plt.show()\n",
    "# X_train = np.stack([bin_pixels(img) for img in X_train])\n",
    "# X_val = np.stack([bin_pixels(img) for img in X_val])\n",
    "# X_test = np.stack([bin_pixels(img) for img in X_test])\n",
    "# plt.imshow(X_train[0])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bba4ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def min_max_xy_coords(X_train, X_val, X_test):\n",
    "    coords_scaler = MinMaxScaler().fit(X_train[1])\n",
    "    X_train[1] = coords_scaler.transform(X_train[1])\n",
    "    X_val[1] = coords_scaler.transform(X_val[1])\n",
    "    X_test[1] = coords_scaler.transform(X_test[1])\n",
    "    return coords_scaler\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4913e270",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "def resize_psfs(X_train, X_val, X_test):\n",
    "    target_size = 128\n",
    "    imshape = (target_size, target_size, 3)\n",
    "\n",
    "    X_train[0] = np.stack([resize(psf, imshape) for psf in X_train[0]])\n",
    "    X_val[0] = np.stack([resize(psf, imshape) for psf in X_val[0]])\n",
    "    X_test[0] = np.stack([resize(psf, imshape) for psf in X_test[0]])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0beb51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def preprocess_imgs(X_train, X_val, X_test):\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1.0/65336.0,\n",
    "        samplewise_center=False,\n",
    "        samplewise_std_normalization=False,\n",
    "        featurewise_center=True,\n",
    "        featurewise_std_normalization=True,\n",
    "        horizontal_flip=False)\n",
    "\n",
    "    # compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied)\n",
    "    datagen.fit(X_train[0])\n",
    "\n",
    "    X_train_preproc = [X_train[0].copy(), X_train[1].copy()]\n",
    "    X_val_preproc = [X_val[0].copy(), X_val[1].copy()]\n",
    "    X_test_preproc = [X_test[0].copy(), X_test[1].copy()]\n",
    "\n",
    "    X_train_preproc[0] = datagen.standardize(X_train_preproc[0])\n",
    "    X_val_preproc[0] = datagen.standardize(X_val_preproc[0])\n",
    "    X_test_preproc[0] = datagen.standardize(X_test_preproc[0])\n",
    "    \n",
    "    return X_train_preproc, X_val_preproc, X_test_preproc, datagen\n",
    "\n",
    "\n",
    "# X_train_preproc, X_val_preproc, X_test_preproc, datagen = preprocess_imgs(X_train, X_val, X_test)\n",
    "\n",
    "# preprocessors = {\n",
    "#     'psfs': datagen,\n",
    "#     'coords': coords_scaler\n",
    "# }\n",
    "\n",
    "# import pickle\n",
    "# with open('./scalers.p', 'wb') as f:\n",
    "#     pickle.dump(preprocessors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eb9403",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tqdm.keras import TqdmCallback\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def train_model(X_train_preproc, y_train, X_val_preproc, y_val):\n",
    "    img_input = layers.Input((X_train_preproc[0][0].shape))\n",
    "    coords_input = layers.Input(X_train_preproc[1][0].shape)\n",
    "\n",
    "    x = img_input\n",
    "\n",
    "    x = keras.applications.MobileNet(\n",
    "        input_tensor=img_input,\n",
    "        include_top=False,\n",
    "        weights='./mobilenet_1_0_128_tf_no_top.h5',\n",
    "        pooling='avg',\n",
    "    )(x)\n",
    "\n",
    "    x = tf.concat([x, coords_input], axis=-1)\n",
    "\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    out = layers.Dense(1, activation=\"linear\",\n",
    "                       kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n",
    "                       bias_regularizer=regularizers.L2(1e-4),\n",
    "                       activity_regularizer=regularizers.L2(1e-5)\n",
    "                      )(x)\n",
    "\n",
    "    model = keras.Model(inputs=(img_input, coords_input), outputs=out)\n",
    "\n",
    "    model.summary(expand_nested=False)\n",
    "\n",
    "\n",
    "    batch_size = 32\n",
    "    epochs = 5000\n",
    "    lr = 0.001\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizers.AdamW(learning_rate=lr), metrics=['mean_absolute_error'])\n",
    "\n",
    "    callbacks = [\n",
    "        ReduceLROnPlateau(monitor='val_mean_absolute_error', factor=0.1,\n",
    "                          patience=25, verbose=True, mode='min', min_delta=5, min_lr=1e-6,),\n",
    "        EarlyStopping(monitor='val_mean_absolute_error', patience=75,\n",
    "                      verbose=False, min_delta=1, restore_best_weights=True),\n",
    "        TqdmCallback(verbose=1),\n",
    "    ]\n",
    "\n",
    "\n",
    "    history = model.fit(X_train_preproc, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val_preproc, y_val), callbacks=callbacks, shuffle=True, verbose=False)\n",
    "\n",
    "    model.save('./latest_model')\n",
    "    return model, history\n",
    "\n",
    "\n",
    "# Peak alignment\n",
    "# 5.424, 114, 95\n",
    "# w/ aug 5.19, 113, 96\n",
    "# w/ more data and brightness aug 5.5, 119.52, 89\n",
    "# Using smoothed peaks (s=1.25) 63, 74, 67\n",
    "# 9.396, 78, 64.52\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf04eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def measure_errors(X_train, y_train, X_val, y_val):\n",
    "    for k, (X, y) in [('train', (X_train, y_train)), ('val', (X_val, y_val))]:\n",
    "        res = model.predict(X, verbose=True)\n",
    "        error = mean_absolute_error(res, y)\n",
    "        print(k, round(error, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e57458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plt_history(history):\n",
    "    plt.rcParams['figure.figsize'] = [5, 5]\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(history.history['mean_absolute_error'], label='mse')\n",
    "    ax1.plot(history.history['val_mean_absolute_error'], label='val_mse')\n",
    "    ax1.set_ylim([0, 500])\n",
    "    ax1.legend(loc=1)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(history.history['lr'], label='lr', color='red')\n",
    "    ax2.legend(loc=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7796db5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "import scipy.optimize as opt\n",
    "\n",
    "# plt.rcParams['figure.figsize'] = [5, 3]\n",
    "\n",
    "def chunk_dataset(train, z):\n",
    "    splits = [0] + list(np.argwhere(np.diff(z.squeeze()) < 0).flatten()+1) + [z.shape[0]]\n",
    "    for i in range(len(splits)-1):\n",
    "        start = splits[i]\n",
    "        end = splits[i+1]\n",
    "        yield (train[0][start:end], train[1][start:end]), z[start:end]\n",
    "        \n",
    "def bestfit_error(z_true, z_pred):\n",
    "\n",
    "    def linfit(x, c):\n",
    "        return x + c\n",
    "\n",
    "    x = z_true.squeeze()\n",
    "    y = z_pred.squeeze()\n",
    "    popt, _ = opt.curve_fit(linfit, x, y, p0=[0])\n",
    "\n",
    "    x = np.linspace(z_true.min(), z_true.max(), len(y))\n",
    "    y_fit = linfit(x, popt[0])\n",
    "#     plt.plot(x, x, label=f'x=y')\n",
    "#     plt.plot(x, y_fit, label=f'best_fit c={popt[0]}')\n",
    "#     plt.show()\n",
    "    error = mean_absolute_error(y_fit, y)\n",
    "    return error, popt[0]\n",
    "\n",
    "def get_offsets(X, y):\n",
    "    offsets = []\n",
    "    for i, (inp, true_zs) in enumerate(chunk_dataset(X, y)):\n",
    "        pred_zs = model.predict(inp, verbose=False)\n",
    "        error, offset = bestfit_error(true_zs, pred_zs)\n",
    "        offsets.append(offset)\n",
    "    return offsets\n",
    "        \n",
    "    \n",
    "\n",
    "# res_offsets = []\n",
    "# for name, dataset in [\n",
    "# #     ('train', (X_train_preproc, y_train)), \n",
    "#     ('val', (X_val_preproc, y_val)), \n",
    "#     ('test', (X_test_preproc, y_test))]:\n",
    "#     inputs, z = dataset\n",
    "#     for i, (inp, true_zs) in enumerate(chunk_dataset(*dataset)):\n",
    "#         pred_zs = model.predict(inp)\n",
    "#         show_psf_axial(inp[0].mean(axis=-1))\n",
    "#         find_peak(inp[0].mean(axis=-1))\n",
    "        \n",
    "#         error, offset = bestfit_error(true_zs, pred_zs)\n",
    "#         res_offsets.append(abs(offset))\n",
    "# #         error = mean_absolute_error(true_zs, pred_zs)\n",
    "#         plt.title(f'variable error {round(error, 1)}nm, const error {round(offset, 1)}nm')\n",
    "#         plt.scatter(true_zs, pred_zs, label=f'{name}_{i}', marker='+', c='orange')\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "#         if name == 'val':\n",
    "#             idx = val_idx\n",
    "#         else:\n",
    "#             idx = test_idx\n",
    "#         psf_idx = idx[i]\n",
    "#         psf = psfs[psf_idx]\n",
    "#         plt.plot(psf.max(axis=(1,2,3)))\n",
    "#         plt.title(str(psf_idx))\n",
    "#         plt.show()\n",
    "\n",
    "# print('Mean offset', round(np.mean(res_offsets), 3))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4843f987",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "coords = locs[['x', 'y']].to_numpy()[:, np.newaxis]\n",
    "coords = np.repeat(coords, psfs.shape[1], 1)\n",
    "\n",
    "print(psfs.shape, ys.shape, coords.shape)\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n_folds = 5\n",
    "kfold = KFold(n_folds, shuffle=True, random_state=42)\n",
    "idx = np.arange(psfs.shape[0])\n",
    "\n",
    "results = np.zeros((n_folds, len(idx)), dtype=object)\n",
    "results[:] = np.nan\n",
    "for split, (train_idx, val_idx) in enumerate(kfold.split(idx)):\n",
    "    gc.collect()\n",
    "    X_train = psfs[train_idx], coords[train_idx]\n",
    "    y_train = ys[train_idx]\n",
    "\n",
    "    X_val = psfs[val_idx], coords[val_idx]\n",
    "    y_val = ys[val_idx]\n",
    "    \n",
    "    X_test = psfs[val_idx].copy(), coords[val_idx].copy()\n",
    "    y_test = ys[val_idx].copy()\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = concat_training_datasets(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = filter_z_datasets(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    X_train, y_train = aug_dataset(X_train, y_train)\n",
    "    coords_scaler = min_max_xy_coords(X_train, X_val, X_test)\n",
    "    resize_psfs(X_train, X_val, X_test)\n",
    "    X_train_preproc, X_val_preproc, X_test_preproc, datagen = preprocess_imgs(X_train, X_val, X_test)\n",
    "    \n",
    "    model, history = train_model(X_train_preproc, y_train, X_val_preproc, y_val)\n",
    "    plt_history(history)\n",
    "    \n",
    "    orig_train_idx = orig_idx[train_idx]\n",
    "    orig_val_idx = orig_idx[val_idx]\n",
    "    \n",
    "    results[split, train_idx] = 'train'\n",
    "\n",
    "    offsets = get_offsets(X_val, y_val)\n",
    "    results[split, val_idx] = offsets\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b170ef0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = results.T\n",
    "for i, row in enumerate(res):\n",
    "    val = [r for r in row if isinstance(r, np.float64)][0]\n",
    "    print(val)\n",
    "    if abs(val) < 25:\n",
    "        plt.plot(psfs[i].max(axis=(1,2,3)))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e87479",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df = pd.read_csv('/home/miguel/Projects/uni/data/smlm_3d/20230601_MQ_celltype/nup/fov2/storm_1/storm_1_MMStack_Default.ome_locs_40k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71f0e37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# sns.scatterplot(data=df, x='x [nm]', y='y [nm]', alpha=0.05)\n",
    "\n",
    "# xlims = [31000, 33000]\n",
    "# ylims = [24000, 26000]\n",
    "# plt.xlim(*xlims)\n",
    "# plt.ylim(*ylims)\n",
    "# plt.show()\n",
    "\n",
    "# pixel_size = 86\n",
    "# xlim_pixels = [x/pixel_size for x in xlims]\n",
    "# ylim_pixels= [y/pixel_size for y in ylims]\n",
    "\n",
    "# print(xlim_pixels)\n",
    "# print(ylim_pixels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e453d01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
