{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed72486e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from data.visualise import grid_psfs, show_psf_axial\n",
    "from tifffile import imread\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "def load_pickle_file(dpath):\n",
    "    with open(dpath, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# # MQ_data     \n",
    "# stacks = '/home/miguel/Projects/uni/data/smlm_3d/20230601_MQ_celltype/beads_box15/combined/stacks.ome.tif'\n",
    "# locs = '/home/miguel/Projects/uni/data/smlm_3d/20230601_MQ_celltype/beads_box15/combined/locs.hdf'\n",
    "# exclude_idx = [5, 7, 11, 14, 22, 24, 26, 27, 28, 31, 32, 35, 37, 38, 40, 45, 50, 51, 54, 68, 69, 71, 72, 82, 87, 89, 91, 98, 102, 108, 109, 112, 113, 115, 116, 121, 122, 123, 127, 129, 131, 132, 133, 138, 141, 144, 150, 151, 154, 161, 167, 169, 170, 172, 178, 179, 181, 182, 184, 185, 186, 187, 190, 200, 201, 205, 206, 210, 214, 219, 221, 224, 226, 230, 233, 234, 235, 236, 237, 243]\n",
    "# Z_STEP = 20\n",
    "\n",
    "# FD-deeploc data\n",
    "stacks = '/home/miguel/Projects/uni/data/smlm_3d/fd-deeploc-data/Astigmatism_beads_stacks_2um/combined/stacks.ome.tif'\n",
    "locs = '/home/miguel/Projects/uni/data/smlm_3d/fd-deeploc-data/Astigmatism_beads_stacks_2um/combined/locs.hdf'\n",
    "exclude_idx = []\n",
    "Z_STEP = 50\n",
    "\n",
    "all_psfs = imread(stacks)\n",
    "all_locs = pd.read_hdf(locs, key='locs')\n",
    "\n",
    "all_psfs = all_psfs[:, :, :, :, np.newaxis]\n",
    "\n",
    "print(all_psfs.shape, all_psfs.dtype)\n",
    "\n",
    "# # for i, psf in enumerate(psfs.sum(axis=-1)):\n",
    "# #     plt.title(str(i))\n",
    "# #     show_psf_axial(psf)\n",
    "\n",
    "\n",
    "# # exclude_idx = [0, 5, 7, 12, 22, 26, 32, 35, 38, 40, 45, 50, 51, 54, 68, 69, 71, 72, 82, 87, 89, 91, 98, 102, 108, 109, 112, 113, 115, 116, 121, 122, 123, 124, 127, 129, 131, 132, 133, 138, 141, 144, 150, 151, 154, 161, 167, 169, 170, 172, 178, 179, 181, 182, 184, 185, 186, 187, 190, 200, 201, 205, 206, 210, 214, 219, 221, 224, 226, 230, 233, 234, 235, 236, 237, 243]\n",
    "\n",
    "# # print('Excluded PSFs \\n\\n\\n\\n\\n')\n",
    "# # for i in exclude_idx:\n",
    "# #     show_psf_axial(psfs[i].mean(axis=-1), str(i))\n",
    "# #     plt.plot(psfs[i].max(axis=(1,2)))\n",
    "# #     plt.show()\n",
    "# # print('End of excluded PSFs \\n\\n\\n\\n\\n')\n",
    "\n",
    "# # print(psfs.shape[0])\n",
    "# # for i in range(psfs.shape[0]):\n",
    "# #     if i in exclude_idx:\n",
    "# #         continue\n",
    "# #     plt.title(str(i))\n",
    "# #     show_psf_axial(psfs[i].mean(axis=-1))\n",
    "# #     plt.plot(psfs[i].max(axis=(1,2,3)), label='max')\n",
    "# #     plt.legend()\n",
    "# #     plt.title(str(i))\n",
    "# #     plt.show()\n",
    "\n",
    "# idx = [i for i in range(psfs.shape[0]) if i not in exclude_idx]\n",
    "# psfs = psfs[idx]\n",
    "# locs = locs.iloc[idx]\n",
    "all_locs['idx'] = np.arange(all_locs.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ebbd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xlim = ((450, 750))\n",
    "# ylim = ((450, 750))\n",
    "\n",
    "\n",
    "xlim = ((810, 810+250))\n",
    "ylim = ((790, 790+250))\n",
    "\n",
    "\n",
    "idx = (xlim[0] < all_locs['x']) & (all_locs['x'] < xlim[1]) & (ylim[0] < all_locs['y']) & (all_locs['y'] < ylim[1])\n",
    "locs = all_locs[idx]\n",
    "psfs = all_psfs[locs['idx']]\n",
    "\n",
    "print(psfs.shape)\n",
    "\n",
    "ys = []\n",
    "for i in range(psfs.shape[0]):\n",
    "    y = np.arange(psfs.shape[1]) * Z_STEP\n",
    "    y = y - 1000\n",
    "    ys.append(y)\n",
    "ys = np.stack(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d341907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude_idx = [35, 55, 60, 96, 104, 113, 128, 132, 230, 234]\n",
    "exclude_idx = [0, 82, 109, 114, 138, 141, 149, 153]\n",
    "# exclude_idx = []\n",
    "idx = [i for i in range(psfs.shape[0]) if i not in exclude_idx]\n",
    "psfs = psfs[idx]\n",
    "locs = locs.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6c73e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spline peak finding\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "Z_STEP = 50\n",
    "\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from data.align_psfs import norm_zero_one\n",
    "from scipy import signal\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5, 3]\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "UPSCALE_RATIO = 10\n",
    "\n",
    "bad_psfs_idx = []\n",
    "\n",
    "def denoise(img):\n",
    "    from scipy.ndimage import gaussian_filter\n",
    "    \n",
    "    return gaussian_filter(img.copy(), sigma=(2, 1, 1))\n",
    "    \n",
    "\n",
    "def find_peak(i, psf):\n",
    "    if psf.ndim == 4:\n",
    "        psf = psf.mean(axis=-1)\n",
    "    x = np.arange(psf.shape[0]) * Z_STEP\n",
    "#     psf = denoise(psf)\n",
    "    \n",
    "    inten = norm_zero_one(psf.max(axis=(1,2)))\n",
    "\n",
    "    cs = UnivariateSpline(x, inten, k=3, s=1.1)\n",
    "\n",
    "    x_ups = np.linspace(0, psf.shape[0], len(x) * UPSCALE_RATIO) * Z_STEP\n",
    "\n",
    "    peak_xups = x_ups[np.argmax(cs(x_ups))] \n",
    "\n",
    "    fit = cs(x_ups)\n",
    "    \n",
    "    peak = max(fit)\n",
    "    low = min(fit)\n",
    "    half_max = (peak - low) / 2\n",
    "    \n",
    "    peak_idx = np.argmax(fit)\n",
    "    center_x = len(x_ups) / 2\n",
    "    \n",
    "    half_max_crossings = np.where(np.diff(np.sign(fit-half_max)))[0]\n",
    "    if len(half_max_crossings) < 2:\n",
    "        print(half_max_crossings)\n",
    "        bad_psfs_idx.append(i)\n",
    "        show_psf_axial(psf)\n",
    "        plt.plot(x-peak, inten, label='raw')\n",
    "        plt.plot(x_ups-peak, cs(x_ups), label='fit')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    if DEBUG:\n",
    "        show_psf_axial(psf)\n",
    "        plt.plot(x-peak, inten, label='raw')\n",
    "        plt.plot(x_ups-peak, cs(x_ups), label='fit')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return peak_xups\n",
    "\n",
    "offsets = np.array([find_peak(i, psf) for i, psf in tqdm(enumerate(psfs))])\n",
    "\n",
    "good_idx = [i for i in range(len(psfs)) if i not in bad_psfs_idx]\n",
    "\n",
    "offsets = offsets[good_idx]\n",
    "psfs = psfs[good_idx]\n",
    "locs = locs.iloc[good_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb55ce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ys = []\n",
    "for i, offset in enumerate(offsets):\n",
    "    zs = ((np.arange(psfs.shape[1])) * Z_STEP) -offset\n",
    "    ys.append(zs)\n",
    "\n",
    "ys = np.array(ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f012fd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(psfs.shape)\n",
    "print(ys.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e20b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratify according to area of FOV\n",
    "\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "def cart2pol(xy):\n",
    "    x, y = xy\n",
    "    rho = np.sqrt(x**2 + y**2)\n",
    "    phi = np.arctan2(y, x)\n",
    "    return(rho, phi)\n",
    "\n",
    "center = locs[['x', 'y']].mean().to_numpy()\n",
    "coords = locs[['x', 'y']].to_numpy() - center\n",
    "\n",
    "polar_coords = np.stack([cart2pol(xy) for xy in coords])\n",
    "\n",
    "discretizer = KBinsDiscretizer(n_bins=6, encode='ordinal')\n",
    "groups = discretizer.fit_transform(polar_coords[:, 1:2]).astype(str)\n",
    "\n",
    "center_radius = 50\n",
    "idx = np.argwhere(polar_coords[:, 0] <= center_radius).squeeze()\n",
    "groups[idx] = -1\n",
    "\n",
    "locs['group'] = groups\n",
    "\n",
    "sns.scatterplot(data=locs, x='x', y='y', hue='group')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb8a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Withold some PSFs for evaluation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "idx = np.arange(psfs.shape[0])\n",
    "\n",
    "train_idx, test_idx = train_test_split(idx, train_size=0.9, random_state=SEED, stratify=locs['group'])\n",
    "\n",
    "_train_val_psfs = psfs[train_idx]\n",
    "test_psfs = psfs[test_idx]\n",
    "\n",
    "_train_val_ys = ys[train_idx]\n",
    "test_ys = ys[test_idx]\n",
    "\n",
    "train_fov_groups = locs['group'].to_numpy()[train_idx]\n",
    "\n",
    "train_val_coords = locs[['x', 'y']].to_numpy()[train_idx]\n",
    "\n",
    "# ds_cls = np.zeros((psfs.shape[0]), dtype=object)\n",
    "# ds_cls[train_idx] = 'train/val'\n",
    "# ds_cls[test_idx] = 'test'\n",
    "# locs['ds'] = ds_cls\n",
    "# sns.scatterplot(data=locs, x='x', y='y', hue='ds')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ff33f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_val_labels = np.zeros(_train_val_psfs.shape[0:2])\n",
    "for i in range(_train_val_psfs.shape[0]):\n",
    "    _train_val_labels[i, :] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f6a150",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "groups = np.repeat(np.arange(len(train_idx))[:, np.newaxis], psfs.shape[1], axis=1).flatten()\n",
    "\n",
    "coords = np.repeat(train_val_coords[:, :, np.newaxis], psfs.shape[1], axis=0)\n",
    "\n",
    "train_val_psfs = np.concatenate(_train_val_psfs)\n",
    "train_val_labels = np.concatenate(_train_val_labels)\n",
    "\n",
    "train_val_ys = np.concatenate(_train_val_ys)\n",
    "split_idx = np.arange(train_val_psfs.shape[0])\n",
    "\n",
    "train_idx, val_idx = train_test_split(split_idx, train_size=0.9, random_state=SEED, stratify=groups)\n",
    "\n",
    "train_psfs = train_val_psfs[train_idx]\n",
    "train_ys = train_val_ys[train_idx][:, np.newaxis]\n",
    "_train_labels = train_val_labels[train_idx][:, np.newaxis]\n",
    "\n",
    "val_psfs = train_val_psfs[val_idx]\n",
    "val_ys = train_val_ys[val_idx][:, np.newaxis]\n",
    "_val_labels = train_val_labels[val_idx][:, np.newaxis]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2afa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder().fit(_train_labels)\n",
    "\n",
    "train_labels = encoder.transform(_train_labels).toarray()\n",
    "val_labels = encoder.transform(_val_labels).toarray()\n",
    "\n",
    "print(train_psfs.shape, train_ys.shape, train_labels.shape)\n",
    "print(val_psfs.shape, val_ys.shape, val_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4166c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim stacks\n",
    "\n",
    "def filter_z_range(X, zs):\n",
    "    psfs, groups = X\n",
    "    valid_ids = np.argwhere(abs(zs.squeeze()) < Z_RANGE).squeeze()\n",
    "    return [psfs[valid_ids], groups[valid_ids]], zs[valid_ids]\n",
    "    \n",
    "Z_RANGE = 1000\n",
    "X_train, y_train = filter_z_range((train_psfs, train_labels), train_ys)\n",
    "\n",
    "X_val, y_val = filter_z_range((val_psfs, val_labels), val_ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c7d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from data.visualise import grid_psfs\n",
    "\n",
    "def aug_dataset(X_train, y_train):\n",
    "    AUG_RATIO = 2\n",
    "    MAX_TRANSLATION_PX = 2\n",
    "    MAX_GAUSS_NOISE = 0.005\n",
    "    img_size = X_train[0].shape[1]\n",
    "\n",
    "    aug_pipeline = Sequential([\n",
    "        layers.GaussianNoise(stddev=MAX_GAUSS_NOISE*X_train[0].max(), seed=SEED),\n",
    "        layers.RandomTranslation(MAX_TRANSLATION_PX/img_size, MAX_TRANSLATION_PX/img_size, seed=SEED),\n",
    "        layers.RandomBrightness(0.2, [X_train[0].min(), X_train[0].max()], seed=SEED)\n",
    "    ])\n",
    "\n",
    "    idx = np.random.randint(0, X_train[0].shape[0], size=int(AUG_RATIO*X_train[0].shape[0]))\n",
    "\n",
    "    aug_psfs = aug_pipeline(X_train[0][idx].copy(), training=True).numpy()\n",
    "    aug_coords = X_train[1][idx]\n",
    "\n",
    "    aug_z = y_train[idx]\n",
    "\n",
    "    subset_psfs = np.concatenate((aug_psfs[0:100], X_train[0][idx][0:100]))\n",
    "    plt.imshow(grid_psfs(subset_psfs.mean(axis=-1)))\n",
    "    plt.show()\n",
    "\n",
    "    train_psfs = np.concatenate([aug_psfs, X_train[0]])\n",
    "    train_coords = np.concatenate([aug_coords, X_train[1]])\n",
    "    train_zs = np.concatenate([aug_z, y_train])\n",
    "\n",
    "    X_train = [train_psfs, train_coords]\n",
    "    y_train = train_zs\n",
    "    return X_train, y_train\n",
    "\n",
    "X_train[0] = X_train[0].astype(float)\n",
    "print(X_train[0].shape, X_train[0].min(), X_train[0].max(), X_train[0].mean())\n",
    "X_train, y_train = aug_dataset(X_train, y_train)\n",
    "print(X_train[0].shape, X_train[0].min(), X_train[0].max(), X_train[0].mean())\n",
    "\n",
    "X_train[0] = X_train[0].astype(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4913e270",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "def resize_psfs(X):\n",
    "    print('Resizing...')\n",
    "    target_size = 128\n",
    "    imshape = (target_size, target_size, 3)\n",
    "    X[0] = np.stack([resize(psf, imshape) for psf in X[0]])\n",
    "    print(X[0].shape)\n",
    "    print('Finished')\n",
    "\n",
    "resize_psfs(X_train)\n",
    "resize_psfs(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93efed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0].shape, X_train[1].shape, y_train.shape)\n",
    "print(X_val[0].shape, X_val[1].shape, y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c350189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0/65336.0,\n",
    "    samplewise_center=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    horizontal_flip=False)\n",
    "\n",
    "print('Fitting datagen...')\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "datagen.fit(X_train[0])\n",
    "print('Fitted')\n",
    "\n",
    "X_train_preproc = [X_train[0].copy(), X_train[1].copy()]\n",
    "X_val_preproc = [X_val[0].copy(), X_val[1].copy()]\n",
    "\n",
    "X_train_preproc[0] = datagen.standardize(X_train_preproc[0].astype(float))\n",
    "X_val_preproc[0] = datagen.standardize(X_val_preproc[0].astype(float))\n",
    "\n",
    "# preprocessors = {\n",
    "#     'psfs': datagen,\n",
    "#     'coords': coords_scaler\n",
    "# }\n",
    "\n",
    "# import pickle\n",
    "# with open('./scalers.p', 'wb') as f:\n",
    "#     pickle.dump(preprocessors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bcf881",
   "metadata": {},
   "outputs": [],
   "source": [
    "psfs[test_idx].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a03a9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_psfs = psfs[test_idx]\n",
    "\n",
    "test_ys = ys[test_idx]\n",
    "\n",
    "test_psfs = np.concatenate(test_psfs)\n",
    "\n",
    "test_labels = np.zeros((len(test_psfs), train_labels.shape[1]))\n",
    "test_labels_grouped = np.zeros((len(test_psfs), train_labels.shape[1]))\n",
    "\n",
    "n_psfs = psfs[test_idx].shape[1]\n",
    "for i in range(len(test_idx)):\n",
    "    test_labels_grouped[i*n_psfs:(i+1)*n_psfs, i] = 1\n",
    "\n",
    "test_ys = np.concatenate(test_ys)[:, np.newaxis]\n",
    "print(test_psfs.shape, test_labels.shape, test_ys.shape)\n",
    "\n",
    "X_test, y_test = filter_z_range((test_psfs, test_labels), test_ys)\n",
    "\n",
    "test_groups = X_test[1].copy()\n",
    "\n",
    "\n",
    "resize_psfs(X_test)\n",
    "X_test_preproc = [X_test[0].copy(), X_test[1].copy()]\n",
    "X_test_preproc[0] = datagen.standardize(X_test_preproc[0].astype(float))\n",
    "\n",
    "print(X_test_preproc[0].shape, X_test_preproc[1].shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd21c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train_preproc[1] = scaler.fit_transform(X_train_preproc[1])\n",
    "# X_val_preproc[1] = scaler.transform(X_val_preproc[1])\n",
    "# X_test_preproc[1] = scaler.transform(X_test_preproc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9502965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_preproc[0].min(), X_train_preproc[0].max())\n",
    "print(X_val_preproc[0].min(), X_val_preproc[0].max())\n",
    "print(X_test_preproc[0].min(), X_test_preproc[0].max())\n",
    "\n",
    "print(X_train_preproc[1].min(), X_train_preproc[1].max())\n",
    "print(X_val_preproc[1].min(), X_val_preproc[1].max())\n",
    "print(X_test_preproc[1].min(), X_test_preproc[1].max())\n",
    "\n",
    "\n",
    "print(X_train_preproc[0].shape, X_train_preproc[1].shape)\n",
    "print(X_val_preproc[0].shape, X_val_preproc[1].shape)\n",
    "print(X_test_preproc[0].shape, X_test_preproc[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382caf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train / 1000\n",
    "y_val = y_val / 1000\n",
    "y_test = y_test / 1000\n",
    "print(y_train.max(), y_train.min())\n",
    "print(y_val.max(), y_val.min())\n",
    "print(y_test.max(), y_test.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eb9403",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " # Vision transformer training\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tqdm.keras import TqdmCallback\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, LayerNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from vit_keras import vit\n",
    "\n",
    "# Assuming your input images have size (image_size, image_size, num_channels)\n",
    "image_size = X_train_preproc[0].shape[1]\n",
    "num_channels = X_train_preproc[0].shape[-1]\n",
    "num_classes = 1  # Regression task, predicting a single continuous value\n",
    "\n",
    "# Create the Vision Transformer model using the vit_keras library\n",
    "inputs = Input(shape=(image_size, image_size, num_channels))\n",
    "\n",
    "\n",
    "vit_model = vit.vit_b16(image_size=image_size, \n",
    "                        activation='sigmoid',\n",
    "                        pretrained=True,\n",
    "                        include_top=False,\n",
    "                        pretrained_top=False)\n",
    "\n",
    "x = vit_model(inputs)\n",
    "# Add additional layers for regression prediction\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "regression_output = Dense(num_classes, activation='linear')(x)  # Linear activation for regression\n",
    "\n",
    "labels_input = layers.Input(X_train_preproc[1].shape[1])\n",
    "labels_dense = layers.Dense(1, use_bias=False)\n",
    "\n",
    "regression_output += labels_dense(labels_input)\n",
    "\n",
    "# Combine the Vision Transformer backbone with the regression head\n",
    "model = Model(inputs=[inputs, labels_input], outputs=regression_output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 5000\n",
    "lr = 0.0001\n",
    "\n",
    "# # Model refining\n",
    "# model = keras.models.load_model('./latest_vit_model/')\n",
    "    \n",
    "# n_layers = len(model.layers)\n",
    "# for i in range(0, len(model.layers)-4):\n",
    "#     model.layers[i].trainable = False\n",
    "# assert model.trainable == True\n",
    "\n",
    "    \n",
    "# # Print a summary of the model architecture\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizers.AdamW(learning_rate=lr), metrics=['mean_absolute_error'])\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_mean_absolute_error', factor=0.1,\n",
    "                      patience=50, verbose=True, mode='min', min_delta=5, min_lr=1e-6,),\n",
    "    EarlyStopping(monitor='val_mean_absolute_error', patience=75,\n",
    "                  verbose=False, min_delta=1, restore_best_weights=True),\n",
    "    TqdmCallback(verbose=1),\n",
    "]\n",
    "\n",
    "\n",
    "history = model.fit(X_train_preproc, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val_preproc, y_val), callbacks=callbacks, shuffle=True, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c35004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fe42b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./latest_vit_model')\n",
    "print('Fitted model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e7e30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 227/227 [==============================] - 1s 4ms/step\n",
    "# train 15.435\n",
    "# 27/27 [==============================] - 0s 5ms/step\n",
    "# val 53.627\n",
    "# 29/29 [==============================] - 0s 5ms/step\n",
    "# test 94.618\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e57458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(history.history['mean_absolute_error'], label='mse')\n",
    "ax1.plot(history.history['val_mean_absolute_error'], label='val_mse')\n",
    "# ax1.set_ylim([0, 500])\n",
    "ax1.legend(loc=1)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(history.history['lr'], label='lr', color='red')\n",
    "ax2.legend(loc=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf04eb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "ds = [\n",
    "    ('train', (X_train_preproc, y_train)), \n",
    "    ('val', (X_val_preproc, y_val)),\n",
    "    ('test', (X_test_preproc, y_test))\n",
    "]\n",
    "for k, (X, y) in ds:\n",
    "    res = model.predict(X, verbose=True)\n",
    "    error = mean_absolute_error(res, y)\n",
    "    print(k, round(error, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e722cd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MAE without located error\n",
    "import scipy.optimize as opt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [3,3]\n",
    "\n",
    "def bestfit_error(z_true, z_pred):\n",
    "    def linfit(x, c):\n",
    "        return x + c\n",
    "\n",
    "    x = z_true\n",
    "    y = z_pred\n",
    "    popt, _ = opt.curve_fit(linfit, x, y, p0=[0])\n",
    "\n",
    "    x = np.linspace(z_true.min(), z_true.max(), len(y))\n",
    "    y_fit = linfit(x, popt[0])\n",
    "    error = mean_absolute_error(y_fit, y)\n",
    "    plt.plot(x, x, label=f'x=y')\n",
    "    plt.plot(x, y_fit, label=f'best_fit c={popt[0]}')\n",
    "    plt.scatter(z_true, z_pred, marker='x', c='orange')\n",
    "    plt.show()\n",
    "    return error, popt[0], y_fit-y\n",
    "\n",
    "ds = [\n",
    "#     ('train', (X_train_preproc, y_train)),\n",
    "    ('test', (X_test_preproc, y_test))\n",
    "]\n",
    "\n",
    "res = {}\n",
    "for k, (X, y) in ds:\n",
    "    pred_z = model.predict(X, verbose=False)\n",
    "    res[k] = []\n",
    "    if k == 'test':\n",
    "        labels = test_labels_grouped.astype(str)\n",
    "    else:\n",
    "        labels = X[1].astype(str)\n",
    "    labels = [','.join(list(arr)) for arr in labels]\n",
    "    label_ids = LabelEncoder().fit_transform(labels)\n",
    "    y = y.squeeze()\n",
    "    for g in set(label_ids):\n",
    "        idx = np.argwhere(label_ids==g)[:, 0]\n",
    "        group_psfs = X[0][idx]\n",
    "        show_psf_axial(group_psfs.mean(axis=-1), '', 1)\n",
    "        group_true_zs = y[idx]\n",
    "        group_pred_zs = pred_z[idx][:, 0]\n",
    "        if len(idx) == 1:\n",
    "            res[k].append([mean_absolute_error(group_true_zs, group_pred_zs)])\n",
    "        else:\n",
    "            error, offset, errors = bestfit_error(group_true_zs, group_pred_zs)\n",
    "            res[k].extend(errors)\n",
    "\n",
    "for k, v in res.items():\n",
    "    print(k, round(np.mean(np.abs(v)), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66cbd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in res.items():\n",
    "    print(k, round(np.mean(np.abs(v)), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7495f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error with xy coords\n",
    "import scipy.optimize as opt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def bestfit_error(z_true, z_pred):\n",
    "    def linfit(x, c):\n",
    "        return x + c\n",
    "\n",
    "    x = z_true\n",
    "    y = z_pred\n",
    "    popt, _ = opt.curve_fit(linfit, x, y, p0=[0])\n",
    "\n",
    "    x = np.linspace(z_true.min(), z_true.max(), len(y))\n",
    "    y_fit = linfit(x, popt[0])\n",
    "    error = mean_absolute_error(y_fit, y)\n",
    "    plt.plot(x, x, label=f'x=y')\n",
    "    plt.plot(x, y_fit, label=f'best_fit c={popt[0]}')\n",
    "    plt.scatter(z_true, z_pred, marker='x', c='orange')\n",
    "    return error, popt[0], y_fit-y\n",
    "\n",
    "ds = [\n",
    "    ('val', (X_val_preproc, y_val)),\n",
    "    ('test', (X_test_preproc, y_test))\n",
    "]\n",
    "\n",
    "res = {}\n",
    "for k, (X, y) in ds:\n",
    "    pred_z = model.predict(X, verbose=False)\n",
    "    res[k] = []\n",
    "    labels = X[1].astype(str)\n",
    "    labels = [','.join(list(arr)) for arr in labels]\n",
    "    label_ids = LabelEncoder().fit_transform(labels)\n",
    "    \n",
    "    X2 = X[0].copy(), X[1].copy()\n",
    "    pred_z_no_coords = model.predict(X2, verbose=False)\n",
    "    \n",
    "    y = y.squeeze()\n",
    "    plt.scatter(y, pred_z, label='w/ coords', marker='.')\n",
    "    plt.scatter(y, pred_z_no_coords, label='w/o coords', marker='.')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(mean_absolute_error(y, pred_z))\n",
    "    print(mean_absolute_error(y, pred_z_no_coords))\n",
    "\n",
    "#     for g in set(label_ids):\n",
    "#         idx = np.argwhere(label_ids==g)[:, 0]\n",
    "#         group_psfs = X[0][idx]\n",
    "#         show_psf_axial(group_psfs.mean(axis=-1), '', 2)\n",
    "#         group_true_zs = y[idx]\n",
    "#         group_pred_zs = pred_z[idx][:, 0]\n",
    "#         if len(idx) == 1:\n",
    "#             res[k].append([mean_absolute_error(group_true_zs, group_pred_zs)])\n",
    "#         else:\n",
    "#             error, offset, errors = bestfit_error(group_true_zs, group_pred_zs)\n",
    "#             error, offset, errors = bestfit_error(group_true_zs, pred_z_no_coords[idx][:, 0])\n",
    "\n",
    "#         plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defee40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "# w/               groups    no groups   no groups larger FOV\n",
    "# train            18.952    11.403      12\n",
    "# val              55.52     53.929      68\n",
    "# test             102.356   99.47       74\n",
    "# test_wo_offsets  48.838    48.318      42\n",
    "\n",
    "# w/ No reg        groups    no groups   no groups larger FOV\n",
    "# train            ______    7.9___      ______\n",
    "# val              ______    54____      ______\n",
    "# test             ______    126___      ______\n",
    "# test_wo_offsets  ______    84____      ______\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52d63b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# MQ_DATA\n",
    "dirname = '/home/miguel/Projects/uni/data/smlm_3d/20230601_MQ_celltype/nup/fov2/storm_1/'\n",
    "locs = 'storm_1_MMStack_Default.ome_locs.hdf5'\n",
    "spots = 'storm_1_MMStack_Default.ome_spots.hdf5'\n",
    "\n",
    "# FD-DEEPLOC-data\n",
    "\n",
    "dirname = '/home/miguel/Projects/uni/data/smlm_3d/fd-deeploc-data/demo2_FD_astig_NPC/'\n",
    "locs = 'roi_startpos_810_790_split.ome_locs.hdf5'\n",
    "spots = 'roi_startpos_810_790_split.ome_spots.hdf5'\n",
    "\n",
    "\n",
    "all_locs = pd.read_hdf(dirname+locs, key='locs')\n",
    "picked_locs = pd.read_hdf(dirname+locs.replace('_locs', '_locs_picked'), key='locs')\n",
    "\n",
    "with h5py.File(dirname+spots, 'r') as f:\n",
    "    spots = np.array(f['spots']).astype(np.uint16)\n",
    "\n",
    "print(all_locs.shape)\n",
    "print(picked_locs.shape)\n",
    "print(spots.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452ac75c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Check values in base image vs spots\n",
    "# from PIL import Image\n",
    "\n",
    "# d = Image.open('/home/miguel/Projects/uni/data/smlm_3d/20230601_MQ_celltype/nup/fov2/storm_1/storm_1_MMStack_Default.ome.tif')\n",
    "# print(d.n_frames)\n",
    "# d.seek(200)\n",
    "# np.array(d).max()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9390c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_locs['x'].min(), all_locs['x'].max())\n",
    "print(picked_locs['x'].min(), picked_locs['x'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02810217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MQ_data_only\n",
    "if 'demo2_FD_astig_NPC' in dirname:\n",
    "    \n",
    "#     xlim = ((450, 750))\n",
    "#     ylim = ((450, 750))\n",
    "    xlim = 100, 125\n",
    "    ylim = 50, 75\n",
    "    \n",
    "#     xlim = 105, 110\n",
    "#     ylim = 60, 65\n",
    "    l2 = picked_locs[(xlim[0]<picked_locs['x']) & (xlim[1]>picked_locs['x']) & (ylim[0]<picked_locs['y']) & (ylim[1]>picked_locs['y'])]\n",
    "    all_locs = all_locs.iloc[l2.index]\n",
    "    picked_locs = all_locs\n",
    "    spots = spots[l2.index]\n",
    "\n",
    "\n",
    "print(all_locs.shape)\n",
    "print(picked_locs.shape)\n",
    "print(spots.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c515db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if all_locs.shape[0] == picked_locs.shape[0]:\n",
    "    idx = np.arange(all_locs.shape[0])\n",
    "else:\n",
    "    all_keys = list(all_locs[['bg', 'photons']].astype(str).agg('-'.join, axis=1))\n",
    "    picked_keys = picked_locs[['bg', 'photons']].astype(str).agg('-'.join, axis=1)\n",
    "    idx = [all_keys.index(k) for k in picked_keys]\n",
    "\n",
    "exp_psfs = spots[idx]\n",
    "print(exp_psfs.shape, picked_locs.shape)\n",
    "print(exp_psfs.min(), exp_psfs.max())\n",
    "try:\n",
    "    print(psfs.min(), psfs.max())\n",
    "    print(psfs.dtype, exp_psfs.dtype)\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1212c88c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from data.visualise import grid_psfs\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "plt.imshow(grid_psfs(exp_psfs[0:100]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3590f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [3, 3]\n",
    "sns.scatterplot(data=picked_locs, x='x', y='y', alpha=0.01)\n",
    "plt.xlim((100, 125))\n",
    "plt.ylim((50, 75))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae47bb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import keras\n",
    "\n",
    "# with open('./scalers.p', 'rb') as f:\n",
    "#     preprocessors = pickle.load(f)\n",
    "\n",
    "# model = keras.models.load_model('./latest_model/')\n",
    "\n",
    "# datagen = preprocessors['psfs']\n",
    "# coords_scaler = preprocessors['coords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471c0f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(exp_psfs_preproc.min(), exp_psfs_preproc.max())\n",
    "# print(X_train_preproc[0].min(), X_train_preproc[0].max())\n",
    "# print(exp_coords_preproc.min(), exp_coords_preproc.max())\n",
    "# print(exp_psfs_preproc.shape, exp_coords_preproc.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4539512",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exp_psfs.dtype, psfs.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535238e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = [10, 3]\n",
    "\n",
    "exp_coords = picked_locs[['x', 'y']].to_numpy()\n",
    "exp_coords_preproc = scaler.transform(exp_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3aa681",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_exp = [exp_psfs, exp_coords_preproc]\n",
    "resize_psfs(X_exp)\n",
    "X_exp[0] = datagen.standardize(X_exp[0].astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0acf1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X in (X_exp, X_train_preproc):\n",
    "    print(X[0].min(), X[0].mean(), X[0].max())\n",
    "    print(X[1].min(), X[1].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1090af38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_z = model.predict(X_exp)\n",
    "plt.rcParams['figure.figsize'] = [3,3]\n",
    "sns.histplot(pred_z)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bd8698",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 20]\n",
    "\n",
    "idx = np.argsort(pred_z.squeeze())\n",
    "sorted_psfs = exp_psfs[idx]\n",
    "plt.imshow(grid_psfs(sorted_psfs[::10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2f3046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb82b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
