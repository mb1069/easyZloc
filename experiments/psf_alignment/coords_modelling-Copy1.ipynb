{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed72486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from data.visualise import grid_psfs, show_psf_axial\n",
    "from tifffile import imread\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "def load_pickle_file(dpath):\n",
    "    with open(dpath, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# # MQ_data     \n",
    "# stacks = '/home/miguel/Projects/uni/data/smlm_3d/20230601_MQ_celltype/beads_box15/combined/stacks.ome.tif'\n",
    "# locs = '/home/miguel/Projects/uni/data/smlm_3d/20230601_MQ_celltype/beads_box15/combined/locs.hdf'\n",
    "# exclude_idx = [5, 7, 11, 14, 22, 24, 26, 27, 28, 31, 32, 35, 37, 38, 40, 45, 50, 51, 54, 68, 69, 71, 72, 82, 87, 89, 91, 98, 102, 108, 109, 112, 113, 115, 116, 121, 122, 123, 127, 129, 131, 132, 133, 138, 141, 144, 150, 151, 154, 161, 167, 169, 170, 172, 178, 179, 181, 182, 184, 185, 186, 187, 190, 200, 201, 205, 206, 210, 214, 219, 221, 224, 226, 230, 233, 234, 235, 236, 237, 243]\n",
    "# Z_STEP = 20\n",
    "\n",
    "# FD-deeploc data\n",
    "stacks = '/home/miguel/Projects/uni/data/smlm_3d/fd-deeploc-data/Astigmatism_beads_stacks_2um/combined/stacks.ome.tif'\n",
    "locs = '/home/miguel/Projects/uni/data/smlm_3d/fd-deeploc-data/Astigmatism_beads_stacks_2um/combined/locs.hdf'\n",
    "exclude_idx = []\n",
    "Z_STEP = 50\n",
    "\n",
    "all_psfs = imread(stacks)\n",
    "all_locs = pd.read_hdf(locs, key='locs')\n",
    "\n",
    "all_psfs = all_psfs[:, :, :, :, np.newaxis]\n",
    "\n",
    "print(all_psfs.shape, all_psfs.dtype)\n",
    "\n",
    "# # for i, psf in enumerate(psfs.sum(axis=-1)):\n",
    "# #     plt.title(str(i))\n",
    "# #     show_psf_axial(psf)\n",
    "\n",
    "\n",
    "# # exclude_idx = [0, 5, 7, 12, 22, 26, 32, 35, 38, 40, 45, 50, 51, 54, 68, 69, 71, 72, 82, 87, 89, 91, 98, 102, 108, 109, 112, 113, 115, 116, 121, 122, 123, 124, 127, 129, 131, 132, 133, 138, 141, 144, 150, 151, 154, 161, 167, 169, 170, 172, 178, 179, 181, 182, 184, 185, 186, 187, 190, 200, 201, 205, 206, 210, 214, 219, 221, 224, 226, 230, 233, 234, 235, 236, 237, 243]\n",
    "\n",
    "# # print('Excluded PSFs \\n\\n\\n\\n\\n')\n",
    "# # for i in exclude_idx:\n",
    "# #     show_psf_axial(psfs[i].mean(axis=-1), str(i))\n",
    "# #     plt.plot(psfs[i].max(axis=(1,2)))\n",
    "# #     plt.show()\n",
    "# # print('End of excluded PSFs \\n\\n\\n\\n\\n')\n",
    "\n",
    "# # print(psfs.shape[0])\n",
    "# # for i in range(psfs.shape[0]):\n",
    "# #     if i in exclude_idx:\n",
    "# #         continue\n",
    "# #     plt.title(str(i))\n",
    "# #     show_psf_axial(psfs[i].mean(axis=-1))\n",
    "# #     plt.plot(psfs[i].max(axis=(1,2,3)), label='max')\n",
    "# #     plt.legend()\n",
    "# #     plt.title(str(i))\n",
    "# #     plt.show()\n",
    "\n",
    "# idx = [i for i in range(psfs.shape[0]) if i not in exclude_idx]\n",
    "# psfs = psfs[idx]\n",
    "# locs = locs.iloc[idx]\n",
    "all_locs['idx'] = np.arange(all_locs.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c0875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xlim = ((450, 750))\n",
    "# ylim = ((450, 750))\n",
    "\n",
    "\n",
    "xlim = ((810, 810+250))\n",
    "ylim = ((790, 790+250))\n",
    "\n",
    "\n",
    "idx = (xlim[0] < all_locs['x']) & (all_locs['x'] < xlim[1]) & (ylim[0] < all_locs['y']) & (all_locs['y'] < ylim[1])\n",
    "locs = all_locs[idx]\n",
    "psfs = all_psfs[locs['idx']]\n",
    "\n",
    "print(psfs.shape)\n",
    "\n",
    "ys = []\n",
    "for i in range(psfs.shape[0]):\n",
    "    y = np.arange(psfs.shape[1]) * Z_STEP\n",
    "    y = y - 1000\n",
    "    ys.append(y)\n",
    "ys = np.stack(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e5a6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(psfs.shape, psfs.min(), psfs.max())\n",
    "print(ys.shape)\n",
    "print(locs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f734cad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tifffile import imwrite, imread\n",
    "fname = './tmp.tif'\n",
    "# imwrite(fname, psfs)\n",
    "\n",
    "# psfs = imread(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f6120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.visualise import show_psf_axial\n",
    "plt.rcParams['figure.figsize'] = [5, 3]\n",
    "for i, psf in enumerate(psfs[0:200]):\n",
    "    show_psf_axial(psf.mean(axis=-1), str(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d341907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude_idx = [35, 55, 60, 96, 104, 113, 128, 132, 230, 234]\n",
    "exclude_idx = [82, 109, 114, 138, 141, 149, 153]\n",
    "# exclude_idx = []\n",
    "idx = [i for i in range(psfs.shape[0]) if i not in exclude_idx]\n",
    "psfs = psfs[idx]\n",
    "locs = locs.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6c73e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spline peak finding\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "Z_STEP = 50\n",
    "\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from data.align_psfs import norm_zero_one\n",
    "\n",
    "UPSCALE_RATIO = 10\n",
    "\n",
    "bad_psfs_idx = []\n",
    "\n",
    "def find_peak(i, psf):\n",
    "    if psf.ndim == 4:\n",
    "        psf = psf.mean(axis=-1)\n",
    "    x = np.arange(psf.shape[0]) * Z_STEP\n",
    "    inten = norm_zero_one(psf.max(axis=(1,2)))\n",
    "#   prev 0.8\n",
    "\n",
    "#     cs = UnivariateSpline(x, inten, k=3, s=1.25)\n",
    "\n",
    "#     x_ups = np.linspace(0, psf.shape[0], len(x) * UPSCALE_RATIO) * Z_STEP\n",
    "    \n",
    "#     peak = x_ups[np.argmax(cs(x_ups))] \n",
    "    \n",
    "    \n",
    "#     peak_idx = np.argmax(cs(x_ups))\n",
    "#     center_x = len(x_ups) / 2\n",
    "#     if abs(center_x - peak_idx) > 250:\n",
    "#         bad_psfs_idx.append(i)\n",
    "        \n",
    "# #         show_psf_axial(psf)\n",
    "# #         plt.plot(x-peak, inten, label='raw')\n",
    "# #         plt.plot(x_ups-peak, cs(x_ups), label='fit')\n",
    "# #         plt.legend()\n",
    "# #         plt.show()\n",
    "\n",
    "    cs = UnivariateSpline(x, inten, k=3, s=1.25)\n",
    "\n",
    "    x_ups = np.linspace(0, psf.shape[0], len(x) * UPSCALE_RATIO) * Z_STEP\n",
    "    \n",
    "    fit = cs(x_ups)\n",
    "    peak_x = np.argmax(fit)\n",
    "    \n",
    "    peak = max(fit)\n",
    "    low = min(fit)\n",
    "    half_max = (peak - low) / 2\n",
    "    \n",
    "    peak_idx = np.argmax(fit)\n",
    "    center_x = len(x_ups) / 2\n",
    "    \n",
    "    half_max_crossings = np.where(np.diff(np.sign(fit-half_max)))[0]\n",
    "    if len(half_max_crossings) < 2:\n",
    "        print(half_max_crossings)\n",
    "        bad_psfs_idx.append(i)\n",
    "        show_psf_axial(psf)\n",
    "        plt.plot(x-peak, inten, label='raw')\n",
    "        plt.plot(x_ups-peak, cs(x_ups), label='fit')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    min_x = min(x_ups-peak)\n",
    "    max_x = max(x_ups-peak)\n",
    "    \n",
    "#     raise EnvironmentError\n",
    "    return peak\n",
    "\n",
    "offsets = np.array([find_peak(i, psf) for i, psf in tqdm(enumerate(psfs))])\n",
    "\n",
    "good_idx = [i for i in range(len(psfs)) if i not in bad_psfs_idx]\n",
    "\n",
    "offsets = offsets[good_idx]\n",
    "psfs = psfs[good_idx]\n",
    "locs = locs.iloc[good_idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb55ce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ys = []\n",
    "for i, offset in enumerate(offsets):\n",
    "    zs = ((np.arange(psfs.shape[1])) * Z_STEP) -offset\n",
    "    ys.append(zs)\n",
    "\n",
    "ys = np.array(ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d2e7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for psf, y in zip(psfs[0:10], ys):\n",
    "    plt.plot(y, psf.max(axis=(1,2,3)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e20b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratify according to area of FOV\n",
    "\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "def cart2pol(xy):\n",
    "    x, y = xy\n",
    "    rho = np.sqrt(x**2 + y**2)\n",
    "    phi = np.arctan2(y, x)\n",
    "    return(rho, phi)\n",
    "\n",
    "center = locs[['x', 'y']].mean().to_numpy()\n",
    "coords = locs[['x', 'y']].to_numpy() - center\n",
    "\n",
    "polar_coords = np.stack([cart2pol(xy) for xy in coords])\n",
    "\n",
    "discretizer = KBinsDiscretizer(n_bins=6, encode='ordinal')\n",
    "groups = discretizer.fit_transform(polar_coords[:, 1:2]).astype(str)\n",
    "\n",
    "center_radius = 50\n",
    "idx = np.argwhere(polar_coords[:, 0] <= center_radius).squeeze()\n",
    "groups[idx] = -1\n",
    "\n",
    "locs['group'] = groups\n",
    "\n",
    "sns.scatterplot(data=locs, x='x', y='y', hue='group')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb8a756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Withold some PSFs for evaluation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "idx = np.arange(psfs.shape[0])\n",
    "\n",
    "train_idx, test_idx = train_test_split(idx, train_size=0.9, random_state=SEED, stratify=locs['group'])\n",
    "\n",
    "_train_val_psfs = psfs[train_idx]\n",
    "test_psfs = psfs[test_idx]\n",
    "\n",
    "_train_val_ys = ys[train_idx]\n",
    "test_ys = ys[test_idx]\n",
    "\n",
    "train_fov_groups = locs['group'].to_numpy()[train_idx]\n",
    "\n",
    "train_val_coords = locs[['x', 'y']].to_numpy()[train_idx]\n",
    "test_coords = locs[['x', 'y']].to_numpy()[test_idx]\n",
    "\n",
    "# ds_cls = np.zeros((psfs.shape[0]), dtype=object)\n",
    "# ds_cls[train_idx] = 'train/val'\n",
    "# ds_cls[test_idx] = 'test'\n",
    "# locs['ds'] = ds_cls\n",
    "# sns.scatterplot(data=locs, x='x', y='y', hue='ds')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f6a150",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "groups = np.repeat(np.arange(len(train_idx))[:, np.newaxis], psfs.shape[1], axis=1).flatten()\n",
    "\n",
    "coords = np.repeat(train_val_coords[:, :, np.newaxis], psfs.shape[1], axis=0)\n",
    "\n",
    "train_val_psfs = np.concatenate(_train_val_psfs)\n",
    "train_val_ys = np.concatenate(_train_val_ys)\n",
    "split_idx = np.arange(train_val_psfs.shape[0])\n",
    "\n",
    "train_idx, val_idx = train_test_split(split_idx, train_size=0.9, random_state=SEED, stratify=groups)\n",
    "\n",
    "train_psfs = train_val_psfs[train_idx]\n",
    "train_ys = train_val_ys[train_idx][:, np.newaxis]\n",
    "\n",
    "val_psfs = train_val_psfs[val_idx]\n",
    "val_ys = train_val_ys[val_idx][:, np.newaxis]\n",
    "\n",
    "val_coords = coords[val_idx].squeeze()\n",
    "train_coords = coords[train_idx].squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e08059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_psfs.shape, train_ys.shape, _train_groups.shape)\n",
    "# print(val_psfs.shape, val_ys.shape, _val_groups.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2afa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# encoder = OneHotEncoder().fit(_train_groups)\n",
    "\n",
    "# train_groups = encoder.transform(_train_groups).toarray()\n",
    "# val_groups = encoder.transform(_val_groups).toarray()\n",
    "\n",
    "# print(train_psfs.shape, train_ys.shape, train_groups.shape)\n",
    "# print(val_psfs.shape, val_ys.shape, val_groups.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4166c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim stacks\n",
    "\n",
    "def filter_z_range(X, zs):\n",
    "    psfs, groups = X\n",
    "    valid_ids = np.argwhere(abs(zs.squeeze()) < Z_RANGE).squeeze()\n",
    "    return [psfs[valid_ids], groups[valid_ids]], zs[valid_ids]\n",
    "    \n",
    "Z_RANGE = 1000\n",
    "X_train, y_train = filter_z_range((train_psfs, train_coords), train_ys)\n",
    "\n",
    "X_val, y_val = filter_z_range((val_psfs, val_coords), val_ys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c7d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from data.visualise import grid_psfs\n",
    "\n",
    "def aug_dataset(X_train, y_train):\n",
    "    AUG_RATIO = 1\n",
    "    MAX_TRANSLATION_PX = 2\n",
    "    MAX_GAUSS_NOISE = 0.001\n",
    "    img_size = X_train[0].shape[1]\n",
    "\n",
    "    aug_pipeline = Sequential([\n",
    "        layers.GaussianNoise(stddev=MAX_GAUSS_NOISE*X_train[0].max(), seed=SEED),\n",
    "        layers.RandomTranslation(MAX_TRANSLATION_PX/img_size, MAX_TRANSLATION_PX/img_size, seed=SEED),\n",
    "        layers.RandomBrightness(0.2, [X_train[0].min(), X_train[0].max()], seed=SEED)\n",
    "    ])\n",
    "\n",
    "    idx = np.random.randint(0, X_train[0].shape[0], size=int(AUG_RATIO*X_train[0].shape[0]))\n",
    "\n",
    "    aug_psfs = aug_pipeline(X_train[0][idx].copy(), training=True).numpy()\n",
    "    aug_coords = X_train[1][idx]\n",
    "\n",
    "    aug_z = y_train[idx]\n",
    "\n",
    "    subset_psfs = np.concatenate((aug_psfs[0:100], X_train[0][idx][0:100]))\n",
    "    plt.imshow(grid_psfs(subset_psfs.mean(axis=-1)))\n",
    "    plt.show()\n",
    "\n",
    "    train_psfs = np.concatenate([aug_psfs, X_train[0]])\n",
    "    train_coords = np.concatenate([aug_coords, X_train[1]])\n",
    "    train_zs = np.concatenate([aug_z, y_train])\n",
    "\n",
    "    X_train = [train_psfs, train_coords]\n",
    "    y_train = train_zs\n",
    "    return X_train, y_train\n",
    "\n",
    "X_train[0] = X_train[0].astype(float)\n",
    "print(X_train[0].shape, X_train[0].min(), X_train[0].max(), X_train[0].mean())\n",
    "X_train, y_train = aug_dataset(X_train, y_train)\n",
    "print(X_train[0].shape, X_train[0].min(), X_train[0].max(), X_train[0].mean())\n",
    "\n",
    "X_train[0] = X_train[0].astype(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4913e270",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "def resize_psfs(X):\n",
    "    print('Resizing...')\n",
    "    target_size = 128\n",
    "    imshape = (target_size, target_size, 3)\n",
    "    X[0] = np.stack([resize(psf, imshape) for psf in X[0]])\n",
    "    print(X[0].shape)\n",
    "    print('Finished')\n",
    "\n",
    "resize_psfs(X_train)\n",
    "resize_psfs(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93efed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0].shape, X_train[1].shape, y_train.shape)\n",
    "print(X_val[0].shape, X_val[1].shape, y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c350189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0/65336.0,\n",
    "    samplewise_center=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    horizontal_flip=False)\n",
    "\n",
    "print('Fitting datagen...')\n",
    "# compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied)\n",
    "datagen.fit(X_train[0])\n",
    "print('Fitted')\n",
    "\n",
    "X_train_preproc = [X_train[0].copy(), X_train[1].copy()]\n",
    "X_val_preproc = [X_val[0].copy(), X_val[1].copy()]\n",
    "\n",
    "X_train_preproc[0] = datagen.standardize(X_train_preproc[0].astype(float))\n",
    "X_val_preproc[0] = datagen.standardize(X_val_preproc[0].astype(float))\n",
    "\n",
    "# preprocessors = {\n",
    "#     'psfs': datagen,\n",
    "#     'coords': coords_scaler\n",
    "# }\n",
    "\n",
    "# import pickle\n",
    "# with open('./scalers.p', 'wb') as f:\n",
    "#     pickle.dump(preprocessors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a03a9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_psfs = psfs[test_idx]\n",
    "\n",
    "test_ys = ys[test_idx]\n",
    "\n",
    "\n",
    "test_groups = np.repeat(np.arange(len(test_psfs))[:, np.newaxis], test_psfs.shape[1], axis=1)\n",
    "test_groups = np.concatenate(test_groups)\n",
    "\n",
    "test_coords = np.repeat(test_coords[:, :, np.newaxis], test_psfs.shape[1], axis=0).squeeze()\n",
    "\n",
    "test_psfs = np.concatenate(test_psfs)\n",
    "\n",
    "print(test_psfs.shape, test_coords.shape)\n",
    "\n",
    "test_ys = np.concatenate(test_ys)[:, np.newaxis]\n",
    "\n",
    "X_test, y_test = filter_z_range((test_psfs, test_coords), test_ys)\n",
    "\n",
    "test_groups = X_test[1].copy()\n",
    "\n",
    "\n",
    "resize_psfs(X_test)\n",
    "X_test_preproc = [X_test[0].copy(), X_test[1].copy()]\n",
    "X_test_preproc[0] = datagen.standardize(X_test_preproc[0].astype(float))\n",
    "\n",
    "print(X_test_preproc[0].shape, X_test_preproc[1].shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd21c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_preproc[1] = scaler.fit_transform(X_train_preproc[1])\n",
    "X_val_preproc[1] = scaler.transform(X_val_preproc[1])\n",
    "X_test_preproc[1] = scaler.transform(X_test_preproc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9502965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_preproc[0].min(), X_train_preproc[0].max())\n",
    "print(X_val_preproc[0].min(), X_val_preproc[0].max())\n",
    "\n",
    "print(X_train_preproc[1].min(), X_train_preproc[1].max())\n",
    "print(X_val_preproc[1].min(), X_val_preproc[1].max())\n",
    "\n",
    "print(X_train_preproc[0].shape, X_train_preproc[1].shape)\n",
    "print(X_val_preproc[0].shape, X_val_preproc[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a96ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = np.argwhere(abs(y_train.squeeze()) < 50).squeeze()\n",
    "tmp_psfs = X_train_preproc[0][train_idx].mean(axis=-1)\n",
    "print(tmp_psfs.shape)\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "plt.imshow(grid_psfs(tmp_psfs))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eb9403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tqdm.keras import TqdmCallback\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def train_model(X_train_preproc, y_train, X_val_preproc, y_val):\n",
    "    img_input = layers.Input((X_train_preproc[0][0].shape))\n",
    "    x = img_input\n",
    "    \n",
    "    coords_input = layers.Input(X_train_preproc[1][0].shape)\n",
    "    x_coords = layers.Dense(64)(coords_input)\n",
    "    \n",
    "    x_coords = layers.Dense(64)(x_coords)\n",
    "\n",
    "    x = keras.applications.ResNet101V2(\n",
    "        input_tensor = img_input,\n",
    "        include_top=False,\n",
    "        pooling='max',\n",
    "        weights='imagenet',\n",
    "    )(x)\n",
    "\n",
    "#     x = keras.applications.MobileNetV3Small(\n",
    "#         input_tensor=img_input,\n",
    "#         include_top=False,\n",
    "#         pooling='avg',\n",
    "#     )(x)\n",
    "#     x = keras.applications.MobileNet(\n",
    "#         input_tensor=img_input,\n",
    "#         include_top=False,\n",
    "#         weights='../mobilenet_1_0_128_tf_no_top.h5',\n",
    "#         pooling='max',\n",
    "#     )(x)\n",
    "\n",
    "    x = tf.concat([x, x_coords], axis=-1)\n",
    "    \n",
    "#     x = layers.Dense(64, activation='relu')(x)\n",
    "#     x = layers.Dropout(0.5)(x)\n",
    "#     x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    out = layers.Dense(1, activation=\"linear\",\n",
    "                       kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n",
    "                       bias_regularizer=regularizers.L2(1e-4),\n",
    "                       activity_regularizer=regularizers.L2(1e-5)\n",
    "                      )(x)\n",
    "\n",
    "    model = keras.Model(inputs=(img_input, coords_input), outputs=out)\n",
    "\n",
    "    model.summary(expand_nested=False)\n",
    "\n",
    "\n",
    "    batch_size = 256\n",
    "    epochs = 5000\n",
    "    lr = 0.0001\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizers.AdamW(learning_rate=lr), metrics=['mean_absolute_error'])\n",
    "\n",
    "    callbacks = [\n",
    "        ReduceLROnPlateau(monitor='val_mean_absolute_error', factor=0.1,\n",
    "                          patience=50, verbose=True, mode='min', min_delta=5, min_lr=1e-6,),\n",
    "        EarlyStopping(monitor='val_mean_absolute_error', patience=75,\n",
    "                      verbose=False, min_delta=1, restore_best_weights=True),\n",
    "        TqdmCallback(verbose=1),\n",
    "    ]\n",
    "\n",
    "\n",
    "    history = model.fit(X_train_preproc, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_val_preproc, y_val), callbacks=callbacks, shuffle=True, verbose=True)\n",
    "\n",
    "#     model.save('./latest_model')\n",
    "    return model, history\n",
    "\n",
    "# X_train_preproc[1][:] = 0\n",
    "# X_val_preproc[1][:] = 0\n",
    "model, history = train_model(X_train_preproc, y_train, X_val_preproc, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a07f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./latest_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023a41bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e7e30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     AUG_RATIO = 1\n",
    "#     MAX_TRANSLATION_PX = 2\n",
    "#     MAX_GAUSS_NOISE = 0.001\n",
    "#     img_size = X_train[0].shape[1]\n",
    "\n",
    "#     aug_pipeline = Sequential([\n",
    "#         layers.GaussianNoise(stddev=MAX_GAUSS_NOISE*X_train[0].max(), seed=SEED),\n",
    "#         layers.RandomTranslation(MAX_TRANSLATION_PX/img_size, MAX_TRANSLATION_PX/img_size, seed=SEED),\n",
    "#         layers.RandomBrightness(0.2, [X_train[0].min(), X_train[0].max()], seed=SEED)\n",
    "#     ])\n",
    "\n",
    "# train 41.962\n",
    "# 16/16 [==============================] - 0s 18ms/step\n",
    "# val 59.407\n",
    "# 19/19 [==============================] - 0s 16ms/step\n",
    "# test 42.7\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e57458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(history.history['mean_absolute_error'], label='mse')\n",
    "ax1.plot(history.history['val_mean_absolute_error'], label='val_mse')\n",
    "ax1.set_ylim([0, 500])\n",
    "ax1.legend(loc=1)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(history.history['lr'], label='lr', color='red')\n",
    "ax2.legend(loc=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf04eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "ds = [\n",
    "    ('train', (X_train_preproc, y_train)), \n",
    "    ('val', (X_val_preproc, y_val)),\n",
    "    ('test', (X_test_preproc, y_test))\n",
    "]\n",
    "for k, (X, y) in ds:\n",
    "    res = model.predict(X, verbose=True)\n",
    "    error = mean_absolute_error(res, y)\n",
    "    print(k, round(error, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e722cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE without located error\n",
    "import scipy.optimize as opt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "plt.rcParams['figure.figsize'] = [3,3]\n",
    "def bestfit_error(z_true, z_pred):\n",
    "    def linfit(x, c):\n",
    "        return x + c\n",
    "\n",
    "    x = z_true\n",
    "    y = z_pred\n",
    "    popt, _ = opt.curve_fit(linfit, x, y, p0=[0])\n",
    "\n",
    "    x = np.linspace(z_true.min(), z_true.max(), len(y))\n",
    "    y_fit = linfit(x, popt[0])\n",
    "    error = mean_absolute_error(y_fit, y)\n",
    "    plt.plot(x, x, label=f'x=y')\n",
    "    plt.plot(x, y_fit, label=f'best_fit c={popt[0]}')\n",
    "    plt.scatter(z_true, z_pred, marker='x', c='orange')\n",
    "    plt.show()\n",
    "    return error, popt[0], y_fit-y\n",
    "\n",
    "ds = [\n",
    "#     ('train', (X_train_preproc, y_train)),\n",
    "    ('test', (X_test_preproc, y_test))\n",
    "]\n",
    "\n",
    "res = {}\n",
    "for k, (X, y) in ds:\n",
    "    pred_z = model.predict(X, verbose=False)\n",
    "    res[k] = []\n",
    "    labels = X[1].astype(str)\n",
    "    labels = [','.join(list(arr)) for arr in labels]\n",
    "    label_ids = LabelEncoder().fit_transform(labels)\n",
    "    \n",
    "    y = y.squeeze()\n",
    "    for g in set(label_ids):\n",
    "        idx = np.argwhere(label_ids==g)[:, 0]\n",
    "        group_psfs = X[0][idx]\n",
    "        show_psf_axial(group_psfs.mean(axis=-1), '', 2)\n",
    "        group_true_zs = y[idx]\n",
    "        group_pred_zs = pred_z[idx][:, 0]\n",
    "        if len(idx) == 1:\n",
    "            res[k].append([mean_absolute_error(group_true_zs, group_pred_zs)])\n",
    "        else:\n",
    "            error, offset, errors = bestfit_error(group_true_zs, group_pred_zs)\n",
    "            res[k].extend(errors)\n",
    "\n",
    "for k, v in res.items():\n",
    "    print(k, round(np.mean(np.abs(v)), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66cbd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in res.items():\n",
    "    print(k, round(np.mean(np.abs(v)), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7495f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error with xy coords\n",
    "import scipy.optimize as opt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def bestfit_error(z_true, z_pred):\n",
    "    def linfit(x, c):\n",
    "        return x + c\n",
    "\n",
    "    x = z_true\n",
    "    y = z_pred\n",
    "    popt, _ = opt.curve_fit(linfit, x, y, p0=[0])\n",
    "\n",
    "    x = np.linspace(z_true.min(), z_true.max(), len(y))\n",
    "    y_fit = linfit(x, popt[0])\n",
    "    error = mean_absolute_error(y_fit, y)\n",
    "    plt.plot(x, x, label=f'x=y')\n",
    "    plt.plot(x, y_fit, label=f'best_fit c={popt[0]}')\n",
    "    plt.scatter(z_true, z_pred, marker='x', c='orange')\n",
    "    return error, popt[0], y_fit-y\n",
    "\n",
    "ds = [\n",
    "    ('val', (X_val_preproc, y_val)),\n",
    "    ('test', (X_test_preproc, y_test))\n",
    "]\n",
    "\n",
    "res = {}\n",
    "for k, (X, y) in ds:\n",
    "    pred_z = model.predict(X, verbose=False)\n",
    "    res[k] = []\n",
    "    labels = X[1].astype(str)\n",
    "    labels = [','.join(list(arr)) for arr in labels]\n",
    "    label_ids = LabelEncoder().fit_transform(labels)\n",
    "    \n",
    "    X2 = X[0].copy(), X[1].copy()\n",
    "    X2[1][:] = 0\n",
    "    pred_z_no_coords = model.predict(X2, verbose=False)\n",
    "    \n",
    "    y = y.squeeze()\n",
    "    plt.scatter(y, pred_z, label='w/ coords', marker='.')\n",
    "    plt.scatter(y, pred_z_no_coords, label='w/o coords', marker='.')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(mean_absolute_error(y, pred_z))\n",
    "    print(mean_absolute_error(y, pred_z_no_coords))\n",
    "\n",
    "#     for g in set(label_ids):\n",
    "#         idx = np.argwhere(label_ids==g)[:, 0]\n",
    "#         group_psfs = X[0][idx]\n",
    "#         show_psf_axial(group_psfs.mean(axis=-1), '', 2)\n",
    "#         group_true_zs = y[idx]\n",
    "#         group_pred_zs = pred_z[idx][:, 0]\n",
    "#         if len(idx) == 1:\n",
    "#             res[k].append([mean_absolute_error(group_true_zs, group_pred_zs)])\n",
    "#         else:\n",
    "#             error, offset, errors = bestfit_error(group_true_zs, group_pred_zs)\n",
    "#             error, offset, errors = bestfit_error(group_true_zs, pred_z_no_coords[idx][:, 0])\n",
    "\n",
    "#         plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defee40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "# w/               groups    no groups   no groups larger FOV\n",
    "# train            18.952    11.403      12\n",
    "# val              55.52     53.929      68\n",
    "# test             102.356   99.47       74\n",
    "# test_wo_offsets  48.838    48.318      42\n",
    "\n",
    "# w/ No reg        groups    no groups   no groups larger FOV\n",
    "# train            ______    7.9___      ______\n",
    "# val              ______    54____      ______\n",
    "# test             ______    126___      ______\n",
    "# test_wo_offsets  ______    84____      ______\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52d63b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# MQ_DATA\n",
    "dirname = '/home/miguel/Projects/uni/data/smlm_3d/20230601_MQ_celltype/nup/fov2/storm_1/'\n",
    "locs = 'storm_1_MMStack_Default.ome_locs.hdf5'\n",
    "spots = 'storm_1_MMStack_Default.ome_spots.hdf5'\n",
    "\n",
    "# FD-DEEPLOC-data\n",
    "\n",
    "dirname = '/home/miguel/Projects/uni/data/smlm_3d/fd-deeploc-data/demo2_FD_astig_NPC/'\n",
    "locs = 'roi_startpos_810_790_split.ome_locs.hdf5'\n",
    "spots = 'roi_startpos_810_790_split.ome_spots.hdf5'\n",
    "\n",
    "\n",
    "all_locs = pd.read_hdf(dirname+locs, key='locs')\n",
    "picked_locs = pd.read_hdf(dirname+locs.replace('_locs', '_locs_picked'), key='locs')\n",
    "\n",
    "with h5py.File(dirname+spots, 'r') as f:\n",
    "    spots = np.array(f['spots']).astype(np.uint16)\n",
    "\n",
    "print(all_locs.shape)\n",
    "print(picked_locs.shape)\n",
    "print(spots.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1478e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=all_locs, x='x', y='y', marker='.')\n",
    "plt.rcParams['figure.figsize'] = [20, 20]\n",
    "\n",
    "xlim = 150, 170\n",
    "ylim = 130, 150\n",
    "plt.xlim(*xlim)\n",
    "plt.ylim(*ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452ac75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check values in base image vs spots\n",
    "# from PIL import Image\n",
    "\n",
    "# d = Image.open('/home/miguel/Projects/uni/data/smlm_3d/20230601_MQ_celltype/nup/fov2/storm_1/storm_1_MMStack_Default.ome.tif')\n",
    "# print(d.n_frames)\n",
    "# d.seek(200)\n",
    "# np.array(d).max()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b36a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_locs['x'].min(), all_locs['x'].max())\n",
    "print(picked_locs['x'].min(), picked_locs['x'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02810217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MQ_data_only\n",
    "if 'demo2_FD_astig_NPC' in dirname:\n",
    "    \n",
    "#     xlim = ((450, 750))\n",
    "#     ylim = ((450, 750))\n",
    "\n",
    "    \n",
    "#     xlim = 105, 110\n",
    "#     ylim = 60, 65\n",
    "    l2 = picked_locs[(xlim[0]<picked_locs['x']) & (xlim[1]>picked_locs['x']) & (ylim[0]<picked_locs['y']) & (ylim[1]>picked_locs['y'])]\n",
    "    all_locs = all_locs.iloc[l2.index]\n",
    "    picked_locs = all_locs\n",
    "    spots = spots[l2.index]\n",
    "\n",
    "all_locs['x'] += 810\n",
    "all_locs['y'] += 790\n",
    "print(all_locs.shape)\n",
    "print(picked_locs.shape)\n",
    "print(spots.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c515db",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_locs.shape[0] == picked_locs.shape[0]:\n",
    "    idx = np.arange(all_locs.shape[0])\n",
    "else:\n",
    "    all_keys = list(all_locs[['bg', 'photons']].astype(str).agg('-'.join, axis=1))\n",
    "    picked_keys = picked_locs[['bg', 'photons']].astype(str).agg('-'.join, axis=1)\n",
    "    idx = [all_keys.index(k) for k in picked_keys]\n",
    "\n",
    "exp_psfs = spots[idx]\n",
    "print(exp_psfs.shape, picked_locs.shape)\n",
    "print(exp_psfs.min(), exp_psfs.max())\n",
    "try:\n",
    "    print(psfs.min(), psfs.max())\n",
    "    print(psfs.dtype, exp_psfs.dtype)\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1212c88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from data.visualise import grid_psfs\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "plt.imshow(grid_psfs(exp_psfs[0:100]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3590f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [3, 3]\n",
    "sns.scatterplot(data=picked_locs, x='x', y='y', alpha=0.01)\n",
    "# plt.xlim((100, 125))\n",
    "# plt.ylim((50, 75))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae47bb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import keras\n",
    "\n",
    "# with open('./scalers.p', 'rb') as f:\n",
    "#     preprocessors = pickle.load(f)\n",
    "\n",
    "model = keras.models.load_model('./latest_model/')\n",
    "\n",
    "# datagen = preprocessors['psfs']\n",
    "# coords_scaler = preprocessors['coords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471c0f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(exp_psfs_preproc.min(), exp_psfs_preproc.max())\n",
    "# print(X_train_preproc[0].min(), X_train_preproc[0].max())\n",
    "# print(exp_coords_preproc.min(), exp_coords_preproc.max())\n",
    "# print(exp_psfs_preproc.shape, exp_coords_preproc.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4539512",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exp_psfs.dtype, psfs.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535238e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = [10, 3]\n",
    "\n",
    "exp_coords = picked_locs[['x', 'y']].to_numpy()\n",
    "exp_coords_preproc = scaler.transform(exp_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3aa681",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_exp = [exp_psfs, exp_coords_preproc]\n",
    "resize_psfs(X_exp)\n",
    "X_exp[0] = datagen.standardize(X_exp[0].astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0acf1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X in (X_exp, X_train_preproc):\n",
    "    print(X[0].min(), X[0].mean(), X[0].max())\n",
    "    print(X[1].min(), X[1].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1090af38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_z = model.predict(X_exp)\n",
    "plt.rcParams['figure.figsize'] = [3,3]\n",
    "sns.histplot(pred_z)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a8d9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "def gmm_bic_score(estimator, X):\n",
    "    return estimator.bic(X)\n",
    "\n",
    "param_grid = {\n",
    "    \"n_components\": range(1, 3),\n",
    "    \"covariance_type\": [\"full\"],\n",
    "}\n",
    "\n",
    "\n",
    "def get_n_components(data):\n",
    "    \n",
    "    res = []\n",
    "    estimators = []\n",
    "    for param in ParameterGrid(param_grid):\n",
    "        gm = GaussianMixture(**param).fit(data)\n",
    "        param['score'] = gm.bic(data)\n",
    "        res.append(param)\n",
    "        estimators.append(gm)\n",
    "    \n",
    "    df = pd.DataFrame.from_records(res)\n",
    "    best_params = np.argmin(df['score'].to_numpy())\n",
    "    return estimators[best_params]\n",
    "\n",
    "\n",
    "\n",
    "def get_cov(gm, i):\n",
    "    cov_type = gm.covariance_type\n",
    "    if cov_type == 'tied':\n",
    "        cov = gm.covariances_.squeeze()\n",
    "    elif cov_type == 'full' or cov_type == None:\n",
    "        cov = gm.covariances_[i][0][0]\n",
    "    elif cov_type == 'spherical':\n",
    "        cov = gm.covariances_[i]\n",
    "    elif cov_type == 'diag':\n",
    "        cov = gm.covariances_[i]\n",
    "    \n",
    "    return cov\n",
    "\n",
    "def fit_gmm(ax, data):\n",
    "\n",
    "    if len(data) < 5:\n",
    "        sns.histplot(data, stat='density', ax=ax)\n",
    "        return\n",
    "\n",
    "\n",
    "    gm = get_n_components(data)\n",
    "    df = pd.DataFrame.from_dict({\n",
    "        'z': data.squeeze(),\n",
    "        'labels': gm.predict(data).squeeze().astype(str)\n",
    "    })\n",
    "    \n",
    "    sns.histplot(data=df, x='z', hue='labels', stat='density', bins=20, ax=ax)\n",
    "    \n",
    "    x = np.linspace(data.min(), data.max(), 100)\n",
    "\n",
    "    print(gm.means_)\n",
    "    for i in range(gm.n_components):\n",
    "        cov = get_cov(gm, i)\n",
    "        mean = float(gm.means_[i][0])\n",
    "        weight = gm.weights_[i]\n",
    "        ax.set_title(f'{gm.n_components}')\n",
    "        ax.plot(x, norm.pdf(x, m, np.sqrt(cov))*weight, label=str(i))\n",
    "\n",
    "    \n",
    "picked_locs['z'] = pred_z\n",
    "xyz = picked_locs[['x', 'y', 'z']].to_numpy()\n",
    "xy = xyz[:, [0, 1]]\n",
    "\n",
    "cls = DBSCAN(eps=0.3, min_samples=15).fit_predict(xy).astype(str)\n",
    "\n",
    "picked_locs['clusterID'] = cls\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [5, 5]\n",
    "sns.scatterplot(data=picked_locs, x='x', y='y', hue='clusterID', legend=False)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 3]\n",
    "for cluster_id in set(cls):\n",
    "    cluster_locs = picked_locs[picked_locs['clusterID'] == cluster_id]\n",
    "    cluster_coords = cluster_locs[['x', 'y']].to_numpy()\n",
    "    \n",
    "    cluster_locs['sub_clusterID'] = KMeans(n_init=8, n_clusters=8).fit_predict(cluster_coords).astype(str)\n",
    "    \n",
    "    if cluster_locs.shape[0] < 50:\n",
    "        continue\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    gs = fig.add_gridspec(1, 4)\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax3 = fig.add_subplot(gs[0, 2:4])\n",
    "\n",
    "    sns.scatterplot(data=cluster_locs, x='x', y='y', marker='.', hue='sub_clusterID', ax=ax1, legend=False)\n",
    "    sns.violinplot(data=cluster_locs, y='z', marker='.', ax=ax2)\n",
    "    sns.kdeplot(data=cluster_locs, x='z', ax=ax3)\n",
    "    plt.show()\n",
    "\n",
    "    fig2 = plt.figure()\n",
    "    gs2 = fig2.add_gridspec(1, 8)\n",
    "    axes = []\n",
    "    for i in sorted(set(cluster_locs['sub_clusterID'])):\n",
    "        if int(i) > 0:\n",
    "            ax = fig2.add_subplot(gs2[0, int(i)], sharey=axes[0])\n",
    "            plt.setp(ax.get_yticklabels(), visible=False)\n",
    "        else:\n",
    "            ax = fig2.add_subplot(gs2[0, int(i)])\n",
    "        axes.append(ax)\n",
    "        \n",
    "        z_data = cluster_locs[cluster_locs['sub_clusterID']==i]['z'].to_numpy()[:, np.newaxis]\n",
    "        fit_gmm(ax, z_data)\n",
    "        \n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24a228b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = picked_locs['z'].to_numpy()[:, np.newaxis]\n",
    "sns.histplot(data, stat='density', bins=100)\n",
    "\n",
    "\n",
    "fit = get_n_components(data)\n",
    "means = fit.means_\n",
    "print(means)\n",
    "print(fit.__dict__)\n",
    "x = np.linspace(data.min(), data.max(), 100)\n",
    "\n",
    "cov = fit.covariances_.squeeze()\n",
    "for i in range(fit.n_components):\n",
    "    mean = float(fit.means_[i][0])\n",
    "    weight = fit.weights_[i]\n",
    "    plt.plot(x, norm.pdf(x, m, np.sqrt(cov))*weight)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bd8698",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 20]\n",
    "\n",
    "idx = np.argsort(pred_z.squeeze())\n",
    "sorted_psfs = exp_psfs[idx]\n",
    "plt.imshow(grid_psfs(sorted_psfs[::10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2f3046",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [5, 3]\n",
    "for sd in [40]:\n",
    "    p1 = np.random.normal(0, sd, size=10000)\n",
    "    p2 = np.random.normal(50, sd, size=10000)\n",
    "    data = np.concatenate((p1, p2))\n",
    "    plt.title(f'Stdev: {sd}')\n",
    "    sns.histplot(data)\n",
    "    plt.xlabel('Z (nm)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b33ab2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
